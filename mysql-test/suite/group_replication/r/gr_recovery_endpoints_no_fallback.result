include/group_replication.inc
Warnings:
Note	####	Sending passwords in plain text without SSL/TLS is extremely insecure.
Note	####	Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the 'START SLAVE Syntax' in the MySQL Manual for more information.
[connection server1]

# 1. Install clone plugin on server1.
[connection server1]
INSTALL PLUGIN clone SONAME 'CLONE_PLUGIN';

# 2. Configure a recovery endpoint with a port that do not have
#    mysql server listening
SET @@GLOBAL.group_replication_advertise_recovery_endpoints = "127.0.0.1:SERVER1_ADMIN_PORT";

# 3. Start server 1 and bootstrap group
include/start_and_bootstrap_group_replication.inc

# 4. Create some transactions to send to joiner
CREATE TABLE t1 (a INT NOT NULL AUTO_INCREMENT PRIMARY KEY , b INT);
INSERT INTO test.t1 (b) VALUES (1);
INSERT INTO test.t1 (b) VALUES (2);

# 5. Change settings on recovery to speed up failure
[connection server2]
include/spawn_monitoring_process.inc
SET GLOBAL group_replication_recovery_reconnect_interval= 1;
SET GLOBAL group_replication_recovery_retry_count= 2;

# 6. Setup the server so group replication starts on boot
#    Install the Clone plugin
INSTALL PLUGIN clone SONAME 'CLONE_PLUGIN';

# 7. Ensure clone is used on recovery
SET GLOBAL group_replication_clone_threshold= 1;

# 8. Recovery will fail and server state change to ERROR
include/start_group_replication.inc

# 9. Clone will fail connecting to donor
include/assert_grep.inc [recovery channel used recovery endpoints configuration]

# 10. Distributed recovery will fail connecting to donor
include/assert_grep.inc [recovery channel used recovery endpoints configuration]

# 11. Set recovery endpoints to DEFAULT and server 2 will be able to join
[connection server1]
SET @@GLOBAL.group_replication_advertise_recovery_endpoints = "DEFAULT";

# 12. Stop server 2 and start it again, now it will receive donor default
#     configuration for recovery
[connection server2]
include/stop_group_replication.inc
START GROUP_REPLICATION;
include/rpl_reconnect.inc
include/gr_wait_for_member_state.inc

# 13. Cleanup
DROP TABLE test.t1;
include/rpl_sync.inc
set session sql_log_bin=0;
call mtr.add_suppression("There was an error when connecting to the donor*");
call mtr.add_suppression("For details please check performance_schema.replication_connection_status table and error log messages of Slave I/O for channel group_replication_recovery.");
call mtr.add_suppression("Maximum number of retries when*");
call mtr.add_suppression("Fatal error during the incremental recovery process of Group Replication.*");
call mtr.add_suppression("Skipping leave operation: concurrent attempt to leave the group is on-going.");
call mtr.add_suppression("The server was automatically set into read only mode after an error was detected.");
call mtr.add_suppression("Due to the number of missing transactions being higher than the configured threshold of*");
call mtr.add_suppression("Clone removing all user data for provisioning: Started");
call mtr.add_suppression("Clone removing all user data for provisioning: Finished");
call mtr.add_suppression("Internal query: CLONE INSTANCE FROM 'root'@'127.0.0.1':.*");
call mtr.add_suppression("There was an issue when cloning from another server: Error number: 3862*");
call mtr.add_suppression("Due to some issue on the previous step distributed recovery is now executing: Incremental Recovery.");
call mtr.add_suppression("Failed to shutdown components infrastructure.");
set session sql_log_bin=1;
SET GLOBAL group_replication_clone_threshold= 9223372036854775807;
RESET PERSIST IF EXISTS group_replication_group_name;
RESET PERSIST IF EXISTS group_replication_local_address;
RESET PERSIST IF EXISTS group_replication_group_seeds;
RESET PERSIST IF EXISTS group_replication_start_on_boot;
RESET PERSIST IF EXISTS group_replication_communication_stack;
SET GLOBAL group_replication_start_on_boot= START_ON_BOOT_VALUE;
include/clean_monitoring_process.inc
UNINSTALL PLUGIN clone;
[connection server1]
UNINSTALL PLUGIN clone;
include/group_replication_end.inc
