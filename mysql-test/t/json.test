--echo # Test of the JSON data type.
--echo # ----------------------------------------------------------------------

SET NAMES utf8;

CREATE TABLE t1 (i INT, j JSON) ENGINE=InnoDB;
SHOW CREATE TABLE t1;
INSERT INTO t1 VALUES (0, NULL);
INSERT INTO t1 VALUES (1, '{"a": 2}');
INSERT INTO t1 VALUES (2, '[1,2]');
INSERT INTO t1 VALUES (3, '{"a":"b", "c":"d","ab":"abc", "bc": ["x", "y"]}');
INSERT INTO t1 VALUES (4, '["here", ["I", "am"], "!!!"]');
INSERT INTO t1 VALUES (5, '"scalar string"');
INSERT INTO t1 VALUES (6, 'true');
INSERT INTO t1 VALUES (7, 'false');
INSERT INTO t1 VALUES (8, 'null');
INSERT INTO t1 VALUES (9, '-1');
INSERT INTO t1 VALUES (10, CAST(CAST(1 AS UNSIGNED) AS JSON));
INSERT INTO t1 VALUES (11, '32767');
INSERT INTO t1 VALUES (12, '32768');
INSERT INTO t1 VALUES (13, '-32768');
INSERT INTO t1 VALUES (14, '-32769');
INSERT INTO t1 VALUES (15, '2147483647');
INSERT INTO t1 VALUES (16, '2147483648');
INSERT INTO t1 VALUES (17, '-2147483648');
INSERT INTO t1 VALUES (18, '-2147483649');
INSERT INTO t1 VALUES (19, '18446744073709551615');
INSERT INTO t1 VALUES (20, '18446744073709551616');
INSERT INTO t1 VALUES (21, '3.14');
INSERT INTO t1 VALUES (22, '{}');
INSERT INTO t1 VALUES (23, '[]');
INSERT INTO t1 VALUES (24, CAST(CAST('2015-01-15 23:24:25' AS DATETIME) AS JSON));
INSERT INTO t1 VALUES (25, CAST(CAST('23:24:25' AS TIME) AS JSON));
INSERT INTO t1 VALUES (26, CAST(CAST('2015-01-15' AS DATE) AS JSON));
INSERT INTO t1 VALUES (27, CAST(TIMESTAMP'2015-01-15 23:24:25' AS JSON));
INSERT INTO t1 VALUES (28, CAST(ST_GeomFromText('POINT(1 1)') AS JSON));
# auto-convert to utf8mb4
INSERT INTO t1 VALUES (29, CAST('[]' AS CHAR CHARACTER SET 'ascii'));
INSERT INTO t1 VALUES (30, CAST(x'cafe' AS JSON));
INSERT INTO t1 VALUES (31, CAST(x'cafebabe' AS JSON));

# Maximum allowed key length is 64k-1
INSERT INTO t1 VALUES (100, CONCAT('{"', REPEAT('a', 64 * 1024 - 1), '":123}'));
--error ER_JSON_KEY_TOO_BIG
INSERT INTO t1 VALUES (101, CONCAT('{"', REPEAT('a', 64 * 1024), '":123}'));
--error ER_JSON_KEY_TOO_BIG
INSERT INTO t1 VALUES (102, CAST(CONCAT('{"', REPEAT('a', 64 * 1024), '":123}') AS JSON));

SELECT i, LENGTH(j), j FROM t1 ORDER BY i;
CREATE TABLE auxtbl(ts TIMESTAMP);
INSERT INTO auxtbl VALUES ('2015-02-24 18:52:00');

--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) VALUES ('');
--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) VALUES ('[');
--error ER_INVALID_JSON_CHARSET
INSERT INTO t1(j) VALUES (x'cafebabe');
--error ER_INVALID_JSON_CHARSET
INSERT INTO t1(j) VALUES (ST_GeomFromText('POINT(1 1)'));
--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) VALUES (-1);
--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) VALUES (CAST(1 AS UNSIGNED));
--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) VALUES (3.14);
--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) VALUES (3.14E30);
--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) VALUES (PI());
--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) select ts from auxtbl;
DROP TABLE auxtbl;
--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) VALUES (CAST('15:52:00' as TIME));
--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) VALUES (CAST('2015-02-24' as DATE));
--error ER_INVALID_JSON_TEXT
INSERT INTO t1(j) VALUES (TIMESTAMP '2015-02-24 15:52:00');

--echo # ----------------------------------------------------------------------
--echo # INSERT using prepared statement (PS)
--echo # ----------------------------------------------------------------------
create table pt1(j json);
create table pt2(j json);
prepare ps1 from 'insert into pt1 values (cast(? as json))';
prepare ps2 from 'insert into pt2 values (?)';

set @a=123;
set @b='123';
set @c=3.14;
set @d=3.14E1;
set @e='"123"';
set @f='2010-12-01 18:44:25';
set @g=null;

execute ps1 using @a;
execute ps1 using @b;
execute ps1 using @c;
execute ps1 using @d;
execute ps1 using @e;
--error ER_INVALID_JSON_TEXT_IN_PARAM
execute ps1 using @f;
execute ps1 using @g;

--error ER_INVALID_JSON_TEXT
execute ps2 using @a;
execute ps2 using @b;
--error ER_INVALID_JSON_TEXT
execute ps2 using @c;
--error ER_INVALID_JSON_TEXT
execute ps2 using @d;
execute ps2 using @e;
--error ER_INVALID_JSON_TEXT
execute ps2 using @f;
execute ps2 using @g;

select json_type(j), j from pt1;
select json_type(j), j from pt2;

drop prepare ps1;
drop prepare ps2;
drop table pt1;
drop table pt2;


# A JSON column cannot be used as a key.
--error ER_JSON_USED_AS_KEY
CREATE INDEX t1_idx_j ON t1(j);
--error ER_JSON_USED_AS_KEY
CREATE INDEX t1_idx_i_j ON t1(i, j);
--error ER_JSON_USED_AS_KEY
CREATE INDEX t1_idx_j_i ON t1(j, i);
CREATE INDEX t1_idx_i ON t1(i);
DROP INDEX t1_idx_i ON t1;

CREATE TABLE t2(i INT, j JSON);
INSERT INTO t2 SELECT i, j FROM t1;
SELECT i, LENGTH(j) FROM t2 ORDER BY i;
DROP TABLE t2;

# JSON is a non-reserved keyword, so it should be possible to use it
# as an identifier.
CREATE TABLE json(json int);
INSERT INTO json(json) VALUES (1);
SELECT json FROM json;
DROP TABLE json;

# And it should be possible to use it as a label in a stored procedure.
DELIMITER |;
CREATE PROCEDURE p()
BEGIN
  json: LOOP
    LEAVE json;
  END LOOP json;
END|
DELIMITER ;|
CALL p();
DROP PROCEDURE p;

--echo #
--echo # Test of JSON comparator.
--echo #

SELECT i,
       (j = '"scalar string"') AS c1,
       (j = 'scalar string') AS c2,
       (j = CAST('"scalar string"' AS JSON)) AS c3,
       (j = CAST(CAST(j AS CHAR CHARACTER SET 'utf8mb4') AS JSON)) AS c4,
       (j = CAST(NULL AS JSON)) AS c5,
       (j = NULL) AS c6,
       (j <=> NULL) AS c7,
       (j <=> CAST(NULL AS JSON)) AS c8,
       (j IN (-1, 2, 32768, 3.14)) AS c9,
       (j IN (CAST('[1, 2]' AS JSON), CAST('{}' AS JSON), CAST(3.14 AS JSON)))
         AS c10,
       (j = (SELECT j FROM t1 WHERE j = CAST('null' AS JSON))) AS c11,
       (j = (SELECT j FROM t1 WHERE j IS NULL)) AS c12,
       (j = (SELECT j FROM t1 WHERE 1<>1)) AS c13,
       (j = DATE'2015-01-15') AS c14,
       (j = TIME'23:24:25') AS c15,
       (j = TIMESTAMP'2015-01-15 23:24:25') AS c16,
       (j = CURRENT_TIMESTAMP) AS c17,
       (j = ST_GeomFromText('POINT(1 1)')) AS c18,
       (JSON_EXTRACT(j, '$.a') = 2) AS c19
FROM t1
ORDER BY i;

SELECT i FROM t1
  WHERE j = CAST(CAST(j AS CHAR CHARACTER SET 'utf8') AS JSON)
  ORDER BY i;
SELECT CAST(NULL AS UNSIGNED) = CAST(NULL AS JSON);
SELECT CAST(NULL AS JSON) = CAST(NULL AS JSON);
SELECT CAST(NULL AS JSON) = NULL;
SELECT CAST(1 AS JSON) = NULL;
SELECT CAST('true' AS JSON) = 1;
SELECT CAST('true' AS JSON) = true;

SELECT a.i, b.i, a.j < b.j, a.j = b.j, a.j > b.j, a.j <> b.j, a.j <=> b.j
FROM t1 a, t1 b
ORDER BY a.i, b.i;

--echo # Verify that the index on the int column is not used when
--echo # comparing the int column to a JSON column. The two columns
--echo # should be compared using the JSON comparator.
CREATE TABLE t2(i int, j json);
CREATE INDEX t2_i ON t2(i);
INSERT INTO t2 values (1, CAST(1 AS JSON));
INSERT INTO t2 values (1, CAST('"1"' AS JSON));
ANALYZE TABLE t2;
let $query=SELECT * FROM t2 where i = j;
eval EXPLAIN $query;
eval $query;
DROP TABLE t2;

# Create a table full of JSON numeric scalars to verify that the JSON
# comparator returns the expected result when comparing all
# combinations of those values.
#
# The values should be inserted in ascending order. The table has a
# rank column that tells how the comparator is expected to order the
# JSON values. If two rows have the same rank, the comparator is
# expected to say that the JSON values on the two rows are equal. If a
# row has a lower rank than another, the JSON value in that row is
# expected to be smaller than the JSON value in the other row.
CREATE TABLE numbers(id INT NOT NULL AUTO_INCREMENT,
                     rank INT,
                     j JSON,
                     PRIMARY KEY(id));
INSERT INTO numbers(rank, j) VALUES
(1, '-1e100'),
(2, '-1e65'),
# smallest DECIMAL (negative with 65 digits)
(3, CAST(-99999999999999999999999999999999999999999999999999999999999999999 AS JSON)),
(4, CAST(-9223372036854776001 AS JSON)),
(5, CAST(-9223372036854776000 AS JSON)),
# closest DOUBLE approximation of the smallest SIGNED BIGINT
(5 /* same rank as previous */, '-9.223372036854776e18'),
(6, CAST(-9223372036854775999 AS JSON)),
(7, CAST(-9223372036854775809 AS JSON)),   # smallest SIGNED BIGINT - 1
(8, CAST(-9223372036854775808 AS JSON)),   # smallest SIGNED BIGINT
(9, CAST(-9223372036854775807 AS JSON)),   # smallest SIGNED BIGINT + 1
(10, '-1e-50'),                  # close to zero, fits in a DECIMAL
(11, '-1.2345678901234e-71'),    # has to be truncated to fit in a DECIMAL
(12, CAST(-0.000000000000000000000000000000000000000000000000000000000000000000000012 AS JSON)),
(12 /* same rank as previous */, '-1.2e-71'),
(13, '-1.0345678901234e-71'),    # has to be truncated to fit in a DECIMAL
(14, '-1e-100'),                 # too close to zero to fit in a DECIMAL
(15, '0'),
(15 /* same rank as previous */, '0.0'),
(15 /* same rank as previous */, '-0.0'),
(15 /* same rank as previous */, CAST(0.0 AS JSON)),
(15 /* same rank as previous */, CAST(CAST(-0.0e0 AS DECIMAL) AS JSON)),
(16, '1e-100'),                  # too close to zero to fit in a DECIMAL
(17, '1.0345678901234e-71'),     # has to be truncated to fit in a DECIMAL
(18, CAST(0.000000000000000000000000000000000000000000000000000000000000000000000012 AS JSON)),
(18 /* same rank as previous */, '1.2e-71'),
(19, '1.2345678901234e-71'),     # has to be truncated to fit in a DECIMAL
(20, '1e-50'),                   # close to zero, fits in a DECIMAL
(21, CAST(9223372036854775806 AS JSON)),    # largest SIGNED BIGINT - 1
(22, CAST(9223372036854775807 AS JSON)),    # largest SIGNED BIGINT
(23, CAST(9223372036854775808 AS JSON)),    # largest SIGNED BIGINT + 1
(24, CAST(9223372036854775999 AS JSON)),
# closest DOUBLE approximation of the largest SIGNED BIGINT
(25, '9.223372036854776e18'),
(25 /* same rank as previous */, CAST(9223372036854776000 AS JSON)),
(26, CAST(9223372036854776001 AS JSON)),
(27, CAST(18446744073709551614 AS JSON)),   # largest UNSIGNED BIGINT - 1
(28, CAST(18446744073709551615 AS JSON)),   # largest UNSIGNED BIGINT
(29, CAST(18446744073709551616 AS JSON)),   # largest UNSIGNED BIGINT + 1
# Gets converted to the closest DOUBLE approximation of UNSIGNED BIGINT + 1
# by the JSON parser
(30, '18446744073709551616'),
# biggest DECIMAL (65 digits)
(31, CAST(99999999999999999999999999999999999999999999999999999999999999999 AS JSON)),
(32, CAST('1e65' AS JSON)),
(33, CAST('1e100' AS JSON));
SELECT *, JSON_TYPE(j) FROM numbers ORDER BY id;

# Now compare every combination of scalars in the table using <, =, >,
# <> and <=>, and cross-check the results against the ranks. The query
# returns the rows where the comparison returned an unexpected result.
# If all is well, the query returns no rows.
SELECT a.j, b.j, a.j < b.j, a.j = b.j, a.j > b.j, a.j <=> b.j
FROM numbers a, numbers b
WHERE ((a.j < b.j) <> (a.rank < b.rank)) OR
      ((a.j = b.j) <> (a.rank = b.rank)) OR
      ((a.j > b.j) <> (a.rank > b.rank)) OR
      ((a.j <=> b.j) <> (a.rank <=> b.rank));

DROP TABLE numbers;

# Some extra checks for comparisons between positive and negative zero.
# All should be equal.
SELECT CAST(0.0e0 AS JSON) = -0.0e0;
SELECT CAST(CAST(0 AS DECIMAL) AS JSON) = CAST(-0.0e0 AS DECIMAL);
SELECT CAST(0.0e0 AS JSON) = CAST(-0.0e0 AS DECIMAL);
SELECT CAST(CAST(0 AS DECIMAL) AS JSON) = -0.0e0;
SELECT CAST(CAST(0 AS SIGNED) AS JSON) = -0.0e0;
SELECT CAST(CAST(0 AS SIGNED) AS JSON) = CAST(-0.0e0 AS DECIMAL);
SELECT CAST(CAST(0 AS UNSIGNED) AS JSON) = -0.0e0;
SELECT CAST(CAST(0 AS UNSIGNED) AS JSON) = CAST(-0.0e0 AS DECIMAL);

# Verify handling of errors during evaluation of the arguments to the
# comparator, both in the left argument and in the right argument.
CREATE TABLE t(txt TEXT);
INSERT INTO t VALUES ('');
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT COUNT(*) FROM t WHERE JSON_EXTRACT(txt, '$') = 5;
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT COUNT(*) FROM t WHERE 5 = JSON_EXTRACT(txt, '$');
DROP TABLE t;

--echo #
--echo # WL#8539 - Ordering of scalar JSON values
--echo #

# Create some timestamps.
CREATE TABLE timestamps (ts TIMESTAMP(6));
INSERT INTO timestamps VALUES
('2000-01-01 00:00:00'),
('2000-01-01 00:00:00.01'),
('2000-01-01 00:00:00.001'),
('2000-01-01 00:00:00.002'),
('2000-01-01 00:00:00.02'),
('2000-01-01 23:59:59.999999'),
('2000-01-02 00:00:00'),
('2000-02-01 00:00:00'),
('2010-12-02 01:00:00'),
('2010-12-02 01:02:00'),
('2010-12-02 01:02:03'),
('2010-12-02 02:01:00'),
('1970-01-02 00:00:01'),
('1970-01-02 00:00:01.000001');
SELECT * FROM timestamps ORDER BY CAST(ts AS JSON);

# Create datetimes that correspond to the above timestamps, and add some values
# that are outside the accepted range of the timestamp data type.
CREATE TABLE datetimes (dt DATETIME(6));
INSERT INTO datetimes SELECT ts FROM timestamps;
INSERT INTO datetimes VALUES
('1960-01-02 03:04:05'),
('1960-01-02 03:04:06'),
('1000-01-01 00:00:00'),
('9999-12-31 23:59:59.999999');
SELECT * FROM datetimes ORDER BY CAST(dt AS JSON);

# Create some times using the time component of the above datetimes. Also add
# some times that go outside of the 0-24 range of the time component of
# datetime.
CREATE TABLE times (t TIME(6));
INSERT INTO times SELECT DISTINCT TIME(dt) FROM datetimes;
INSERT INTO times VALUES
('-838:59:59'),
('838:59:59'),
('-00:00:00.000001'),
('-00:00:00'),
('24:00:00'),
('-12:00:00'),
('-24:00:00');
SELECT * FROM times ORDER BY CAST(t AS JSON);

# Create dates using the date component of the above datetimes.
CREATE TABLE dates(d DATE);
INSERT INTO dates SELECT DISTINCT DATE(dt) FROM datetimes;

# Create some signed integers.
CREATE TABLE signed_integers(i BIGINT);
INSERT INTO signed_integers VALUES
(0), (1), (2), (3), (4), (5), (10), (11), (12), (20), (21), (22),
(99), (100), (101), (999), (1000), (1001),
(9223372036854775806), (9223372036854775807);
INSERT INTO signed_integers SELECT -i FROM signed_integers;
INSERT INTO signed_integers VALUES (-9223372036854775808);
SELECT * FROM signed_integers ORDER BY CAST(i AS JSON);

# Create some unsigned integers.
CREATE TABLE unsigned_integers(i BIGINT UNSIGNED);
INSERT INTO unsigned_integers SELECT i FROM signed_integers where i >= 0;
INSERT INTO unsigned_integers VALUES
(9223372036854775808), (18446744073709551614), (18446744073709551615);
SELECT * FROM unsigned_integers ORDER BY CAST(i AS JSON);

# Create some decimals.
CREATE TABLE decimals (d DECIMAL(25,3));
INSERT INTO decimals SELECT i FROM signed_integers;
INSERT INTO decimals SELECT i FROM unsigned_integers;
INSERT INTO decimals VALUES
(9223372036854776000), (-9223372036854776000),
(9223372036854776001), (-9223372036854776001),
(3.13), (3.14), (3.15), (-3.13), (-3.14), (-3.15),
(3.131), (3.141), (3.151), (-3.131), (-3.141), (-3.151),
(3.129), (3.139), (3.149), (-3.129), (-3.139), (-3.149),
(0.1), (0.01), (0.001), (-0.1), (-0.01), (-0.001);
SELECT * FROM decimals ORDER BY CAST(d AS JSON);

# Create some doubles.
CREATE TABLE doubles (d DOUBLE);
INSERT INTO doubles SELECT d FROM decimals;
INSERT INTO doubles VALUES
(1.5E-200), (1.5E200), (-1.5E-200), (-1.5E200),
(-1E-323), (-1E-322), (-1E-321), (1E-323), (1E-322), (1E-321),
(-1E308), (-1E307), (-1E306), (1E308), (1E307), (1E306);
SELECT * FROM doubles ORDER BY CAST(d AS JSON);

# Now convert all of the above values to JSON.
CREATE TABLE t(id INT PRIMARY KEY AUTO_INCREMENT, j JSON);
INSERT INTO t(j) SELECT CAST(ts AS JSON) FROM timestamps ORDER BY ts;
INSERT INTO t(j) SELECT CAST(dt AS JSON) FROM datetimes ORDER BY dt;
INSERT INTO t(j) SELECT CAST(t AS JSON) FROM times ORDER BY t;
INSERT INTO t(j) SELECT CAST(d AS JSON) FROM dates ORDER BY d;
INSERT INTO t(j) SELECT CAST(i AS JSON) FROM signed_integers ORDER BY i;
INSERT INTO t(j) SELECT CAST(i AS JSON) FROM unsigned_integers ORDER BY i;
INSERT INTO t(j) SELECT CAST(d AS JSON) FROM decimals ORDER BY d;
INSERT INTO t(j) SELECT CAST(d AS JSON) FROM doubles ORDER BY d;

# Insert some more JSON values.
INSERT INTO t(j) VALUES
(NULL), (NULL), ('true'), ('false'), ('null'),
('"abc"'), ('""'), ('"abcd"'), ('"bc"'),
('"abc\\u0000\\u0000"'), ('"abc\\u0000"'),
('0.0'), ('-0.0'), ('9223372036854776000'),
('1e-15'), ('1.1e-15'), ('1e-16'), ('1.1e-16'),
(CAST(0.000000000000001 AS JSON)),
(CAST(0.0000000000000011 AS JSON)),
(CAST(0.0000000000000001 AS JSON)),
(CAST(0.00000000000000011 AS JSON)),
(CAST(0.0 AS JSON)),
(CAST(-999999999999999999999999999999999999999999999999999999999999999999999999999999999 AS JSON)),
(CAST(-999999999999999999999999999999999999999999999999999999999999999999999999999999998 AS JSON)),
(CAST(-999999999999999999999999999999999999999999999999999999999999999999999999999999997 AS JSON)),
(CAST(999999999999999999999999999999999999999999999999999999999999999999999999999999997 AS JSON)),
(CAST(999999999999999999999999999999999999999999999999999999999999999999999999999999998 AS JSON)),
(CAST(999999999999999999999999999999999999999999999999999999999999999999999999999999999 AS JSON)),
(CAST(-1E81 AS JSON)),
(CAST(-9.99E80 AS JSON)),
(CAST(9.99E80 AS JSON)),
(CAST(1E81 AS JSON)),
(JSON_ARRAY('an array')),
(JSON_ARRAY('another array')),
(JSON_OBJECT('an', 'object')),
(JSON_OBJECT('another', 'object')),
(CAST(ST_GeomFromText('POINT(0 0)') AS JSON)),
(CAST(ST_GeomFromText('POINT(0 1)') AS JSON)),
(CAST(CAST('1234abcd' AS BINARY) AS JSON));

# Now order the table on the JSON column.
SELECT j, JSON_TYPE(j) AS tp FROM t ORDER BY j, id;
SELECT j, JSON_TYPE(j) AS tp FROM t ORDER BY j DESC, id;

# Ordering on a JSON expression should give the same result.
SELECT JSON_EXTRACT(j, '$') AS je, JSON_TYPE(j) AS tp FROM t ORDER BY je, id;

# Set max_sort_length as small as possible and order again. Since we'll now just
# look at a prefix of the values, distinct values with a common prefix may order
# as equal.
SET @@max_sort_length=4;
SELECT j, JSON_TYPE(j) AS tp FROM t ORDER BY j, id;
SET @@max_sort_length=default;

# GROUP BY uses a temporary for grouping, GROUP BY WITH ROLLUP uses filesort to
# do the grouping.
ANALYZE TABLE t;
EXPLAIN SELECT j, COUNT(*) FROM t GROUP BY j ORDER BY j;
EXPLAIN SELECT j, COUNT(*) FROM t GROUP BY j WITH ROLLUP;

SELECT j, COUNT(*) FROM t GROUP BY j ORDER BY j;
SELECT JSON_EXTRACT(j, '$') AS je, COUNT(*) FROM t GROUP BY je ORDER BY je;

SELECT j, COUNT(*) FROM t GROUP BY j WITH ROLLUP;
SELECT JSON_EXTRACT(j, '$') AS je, COUNT(*) FROM t GROUP BY je WITH ROLLUP;

DROP TABLE t, timestamps, datetimes, times, dates, signed_integers,
           unsigned_integers, decimals, doubles;

# Test ordering of a not nullable column.
CREATE TABLE t(j JSON NOT NULL);
INSERT INTO t VALUES ('1'), ('2'), ('10'), ('"1"'), ('"2"'), ('"10"'),
                     ('true'), ('false'), ('null');
SELECT j FROM t ORDER BY j;
SELECT j FROM t ORDER BY JSON_EXTRACT(j, '$');
SELECT JSON_EXTRACT(j, '$') FROM t ORDER BY 1;

# Ordering on (j+1) will convert to a numeric type.
SELECT j FROM t ORDER BY j+1, JSON_TYPE(j);
DROP TABLE t;

# Order on stored and virtual generated columns with type JSON.
CREATE TABLE t(
  j JSON,
  stored_gc JSON GENERATED ALWAYS AS (JSON_EXTRACT(j, '$[0]')) STORED,
  virtual_gc JSON GENERATED ALWAYS AS (JSON_EXTRACT(j, '$[1]')) VIRTUAL);
INSERT INTO t(j) VALUES
(JSON_ARRAY(1, 2)), (JSON_ARRAY(2, 1)), (JSON_ARRAY(10, 10)), (JSON_ARRAY(5));
SELECT * FROM t ORDER BY stored_gc;
SELECT * FROM t ORDER BY virtual_gc;
DROP TABLE t;

CREATE TABLE t(vc varchar(10));
INSERT INTO t VALUES ('["abc"]'), ('[1');
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT * FROM t ORDER BY CAST(vc AS JSON);
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT * FROM t ORDER BY JSON_EXTRACT(vc, '$[0]');
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT CAST(vc AS JSON) AS j FROM t ORDER BY j;
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_EXTRACT(vc, '$[0]') AS j FROM t ORDER BY j;
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT CAST(vc AS JSON) FROM t ORDER BY 1;
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_EXTRACT(vc, '$[0]') FROM t ORDER BY 1;
DROP TABLE t;

--echo #
--echo # Interal ordering of arrays and objects. Ordered by cardinality.
--echo #
CREATE TABLE t(i int, j json);
INSERT INTO t VALUES
(1, '{}'), (2, '{"a":1}'), (3, '{"ab":2}'), (4, '{"a":1,"b":2}'),
(5, '{"c":3,"d":4}'), (6, '{"a":1,"b":2,"c":3,"d":4}');
INSERT INTO t VALUES
(1, '[]'), (2, '[1]'), (3, '[2]'), (4, '[1,2]'), (5, '[2,1]'), (6, '[1,2,3]'),
(7, '[1,2,3,4]'), (8, '[4,3,2,1]'), (9, '[1,2,3,4,5]');
INSERT INTO t SELECT i+100, j FROM t;
SELECT * FROM t ORDER BY j, i;
SELECT * FROM t ORDER BY j DESC, i;
# GROUP BY knows how to distinguish the arrays and the objects, even
# if they have the same cardinality.
SELECT j, COUNT(*) FROM t GROUP BY j ORDER BY j;
# GROUP BY WITH ROLLUP, on the other hand, doesn't know how to
# distinguish them, and produces confusing results for arrays/objects.
# GROUP BY WITH ROLLUP is only useful on scalar results for now.
SELECT j, COUNT(*) FROM t GROUP BY j WITH ROLLUP;
DROP TABLE t;

--echo # Test NULLs sorting.
CREATE TABLE t(i int, j json);
INSERT INTO t(i) VALUES (1),(2),(3),(2),(1);
SELECT * FROM t ORDER BY j, i;
SELECT * FROM t ORDER BY j DESC, i;
SELECT i, JSON_EXTRACT(j, '$') AS je FROM t ORDER BY je, i;
SELECT i, JSON_EXTRACT(j, '$') AS je FROM t ORDER BY je DESC, i;
INSERT INTO t(i, j) VALUES (1, '1');
SELECT * FROM t ORDER BY j, i;
SELECT * FROM t ORDER BY j DESC, i;
SELECT i, JSON_EXTRACT(j, '$') AS je FROM t ORDER BY je, i;
SELECT i, JSON_EXTRACT(j, '$') AS je FROM t ORDER BY je DESC, i;
DROP TABLE t;

# Merging of sort results should not get confused if one of the sort columns is
# a JSON column.
CREATE TABLE t(vc TEXT, j JSON);
INSERT INTO t (vc) VALUES ('a'), ('b'), ('c');
INSERT INTO t SELECT * FROM t;
INSERT INTO t SELECT * FROM t;
INSERT INTO t SELECT * FROM t;
INSERT INTO t SELECT * FROM t;
INSERT INTO t SELECT * FROM t;
INSERT INTO t SELECT * FROM t;
INSERT INTO t SELECT * FROM t;
SELECT * FROM t ORDER BY vc, j;
DROP TABLE t;

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_VALID function.
--echo # ----------------------------------------------------------------------

--echo
--echo # Table - Json string column - utf-8, NULL
--echo Note: 'utf8' is a subset of internal 'utf8mb4'
--echo
create table utf8_t (c varchar(20)) CHARACTER SET 'utf8';
insert into utf8_t values (NULL);
-- echo # Expect NULL:
select JSON_VALID(c) from utf8_t;
delete from utf8_t;

--echo
--echo # Table - Json string column - utf-8, valid
insert into utf8_t values ('[123]');
select JSON_VALID(c) from utf8_t;
delete from utf8_t;

--echo
--echo # Table - Json string column - utf-8, non-utf8
insert into utf8_t values ('[123');
--echo expect 0 (false)
select JSON_VALID(c) from utf8_t;
delete from utf8_t;

--echo
--echo # Table - Try to extract JSON from TIMESTAMP column
ALTER TABLE utf8_t ADD d TIMESTAMP;

--echo # Should give false; not string or JSON type
--echo # and we do not convert automatically from TIMESTAMP to JSON
insert into utf8_t values (NULL, '2014-11-25 18:00');
select JSON_VALID(d) from utf8_t;

--echo # Explicit cast to a character data type
--echo # allows MySQL to parse this is a JSON text
--echo # The string isn't a legal JSON document, tho, so not valid.
select JSON_VALID(CAST(d as CHAR)) from utf8_t;

--echo # Should give true
select JSON_VALID(CONCAT( CONCAT('"', CAST(d as CHAR)), '"')) from utf8_t;
delete from utf8_t;
drop table utf8_t;

--echo
--echo # Table - JSON type; should give true by definition
create table json_t(t json);
insert into json_t values ('[123]');
select json_VALID(t) from json_t;

--echo
--echo # String literal - valid JSON
select JSON_VALID('123');             # uint
select JSON_VALID('-123');            # int
select JSON_VALID('5000000000');      # uint64
select JSON_VALID('-5000000000');     # int64
select JSON_VALID('1.23');            # double
select JSON_VALID('"123"');
select JSON_VALID('true');
select JSON_VALID('false');
select JSON_VALID('null');
select JSON_VALID('{"address": "Trondheim"}');

--echo
--echo # String literal - not valid JSON
select JSON_VALID('12 3');

--echo
--echo # String literal not in UTF-8
set names 'ascii';
# auto-convert to utf-8
select JSON_VALID('123');

set names 'utf8';

--echo
--echo # Json expression
select JSON_VALID(cast('[123]' as JSON ));

--echo
--echo # Json expression NULL
select JSON_VALID(cast(NULL as JSON ));

--echo
--echo # Bare NULL
select JSON_VALID( NULL );

--echo
--echo # Function result - string
select JSON_VALID( UPPER('"abc"') );

set names 'latin1';
--echo
--echo # Function result - string
# auto-convert to utf-8
select JSON_VALID( UPPER('"abc"') );

set names 'utf8';

--echo
--echo # Function result - date, not valid as JSON without CAST
select JSON_VALID( CAST('2015-01-15' AS DATE) );

--echo
--echo # The date string doesn't parse as JSON text, so wrong:
select JSON_VALID( CAST(CAST('2015-01-15' AS DATE) as CHAR CHARACTER SET 'utf8') );
--echo # OK, though:
select JSON_VALID( CAST(CURDATE() as JSON) );

--echo
--echo # Function result - NULL
select JSON_VALID( UPPER(NULL) );
select JSON_VALID( UPPER(CAST(NULL as CHAR)) );

--echo
--echo # Function result - JSON
select JSON_VALID( JSON_ARRAY(t, t) ) from json_t;

drop table json_t;

# examples from wl7909 spec
# returns 1
SELECT JSON_VALID( '{ "firstName" : "Fred", "lastName" : "Flintstone" }' );

# returns 1
SELECT JSON_VALID( '3' );

# returns NULL as IS JSON would
SELECT JSON_VALID( null );


--echo # ----------------------------------------------------------------------
--echo # Test of JSON_CONTAINS_PATH function.
--echo # ----------------------------------------------------------------------

--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_contains_path();
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_contains_path('{ "a": true }' );
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_contains_path('{ "a": true }', 'all' );
--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_contains_path('{ "a": tru }', 'all', '$' );
--error ER_INVALID_JSON_PATH
select json_contains_path('{ "a": true }', 'all', '$[' );
--error ER_INVALID_JSON_PATH
select json_contains_path('{ "a": true }', 'all', '$a.***[3]' );
--error ER_JSON_BAD_ONE_OR_ALL_ARG
select json_contains_path('{ "a": true }', 'foo', '$.a' );
--error ER_INVALID_JSON_CHARSET
select json_contains_path('{}', 'all', cast('$' as binary));

select json_contains_path(null, 'all', '$.a' );
select json_contains_path('{ "a": true }', null, '$.a' );
select json_contains_path('{ "a": true }', 'all', null );

# degenerate path
select json_contains_path('{ "a": true }', 'all', '$' );

# positive, one path
select json_contains_path('{ "a": true }', 'all', '$.a' );
select json_contains_path('{ "a": true }', 'one', '$.a' );

# negative, one path
select json_contains_path('{ "a": true }', 'all', '$.b' );
select json_contains_path('{ "a": true }', 'one', '$.b' );

# all
select json_contains_path('{ "a": true }', 'all', '$.a', '$.b' );
select json_contains_path('{ "a": true }', 'all', '$.b', '$.a' );
select json_contains_path('{ "a": true }', 'ALL', '$.a', '$.b' );
select json_contains_path('{ "a": true }', 'aLl', '$.a', '$.b' );

# some
select json_contains_path('{ "a": true }', 'one', '$.a', '$.b' );
select json_contains_path('{ "a": true }', 'one', '$.b', '$.a' );
select json_contains_path('{ "a": true }', 'ONE', '$.a', '$.b' );
select json_contains_path('{ "a": true }', 'oNe', '$.a', '$.b' );

# some wildcards
select json_contains_path('{ "a": true, "b": [ 1, 2, { "c": [ 4, 5, { "d": [ 6, 7, 8, 9, 10 ]} ] } ] }', 'all', '$**[4]' );
select json_contains_path('{ "a": true, "b": [ 1, 2, { "c": [ 4, 5, { "d": [ 6, 7, 8, 9, 10 ]} ] } ] }', 'all', '$**[4]', '$**[5]' );
select json_contains_path('{ "a": true, "b": [ 1, 2, { "c": [ 4, 5, { "d": [ 6, 7, 8, 9, 10 ]} ] } ] }', 'all', '$**.c[2]' );
select json_contains_path('{ "a": true, "b": [ 1, 2, { "c": [ 4, 5, { "d": [ 6, 7, 8, 9, 10 ]} ] } ] }', 'all', '$**.c[3]' );
select json_contains_path('{"a":1, "b":2}', 'one', '$.*');
select json_contains_path('[1,2,3]', 'one', '$.*');
select json_contains_path('{}', 'one', '$[*]');

# combine ellipsis and wildcard
SELECT JSON_CONTAINS_PATH('[1, [[{"x": [{"a":{"b":{"c":42}}}]}]]]',
                          'one', '$**.a.*');
SELECT JSON_CONTAINS_PATH('[1, [[{"x": [{"a":{"b":{"c":42}}}]}]]]',
                          'all', '$**.a.*');
SELECT JSON_CONTAINS_PATH('[1,2,3]', 'one', '$**[*]');
SELECT JSON_CONTAINS_PATH('[1,2,3]', 'all', '$**[*]');

# 3 paths
select json_contains_path('{ "a": true, "b": [ 1, 2 ] }', 'all', '$**[1]', '$.b[0]', '$.c' );
select json_contains_path('{ "a": true, "b": [ 1, 2 ] }', 'all', '$.c', '$**[1]', '$.b[0]' );
select json_contains_path('{ "a": true, "b": [ 1, 2 ] }', 'all', '$.b[0]', '$.c', '$**[1]' );
select json_contains_path('{ "a": true, "b": [ 1, 2 ] }', 'one', '$**[1]', '$.b[0]', '$.c' );
select json_contains_path('{ "a": true, "b": [ 1, 2 ] }', 'one', '$.c', '$**[1]', '$.b[0]' );
select json_contains_path('{ "a": true, "b": [ 1, 2 ] }', 'one', '$.b[0]', '$.c', '$**[1]' );

# examples from the wl7909 spec
# returns 0 because there is no element at $.a.c
SELECT JSON_CONTAINS_PATH
(
  '{ "a" : 123, "b" : [ 123, 456 ] }',
  'all',
  '$.a.c',
  '$.b[1]'
);

# returns 1 because there is an element at $.b[1]
SELECT JSON_CONTAINS_PATH
(
  '{ "a" : 123, "b" : [ 123, 456 ] }',
  'one',
  '$.a.c',
  '$.b[1]'
);

# returns 0 because there is no element at the given path
SELECT JSON_CONTAINS_PATH
(
  '{ "a" : 123, "b" : [ 123, 456 ] }',
  'all',
  '$.c'
);

# returns 1 because there is an element at $.b[1].c.d
SELECT JSON_CONTAINS_PATH
(
  '{ "a" : 123, "b" : [ 123, { "c" : { "d" : true } } ] }',
  'all',
  '$.b[1].c.d'
);

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_LENGTH function.
--echo # ----------------------------------------------------------------------

create table utf8_mj_length (a int, c varchar(20)) CHARACTER SET 'utf8';
insert into utf8_mj_length values( 1, null );
insert into utf8_mj_length values( 2, '1' );
insert into utf8_mj_length values( 3, 'abc' );
insert into utf8_mj_length values( 4, '"abc"' );
insert into utf8_mj_length values ( 5, 'true' );
insert into utf8_mj_length values ( 6, 'false' );
insert into utf8_mj_length values ( 7, 'null' );

select a, c, json_length( c ) from utf8_mj_length where a = 1;

select a, c, json_length( c ) from utf8_mj_length where a = 2;

--echo
--echo # invalid json text
--error ER_INVALID_JSON_TEXT_IN_PARAM
select a, c, json_length( c ) from utf8_mj_length where a = 3;

select a, c, json_length( c ) from utf8_mj_length where a = 4;
select a, c, json_length( c ) from utf8_mj_length where a = 5;
select a, c, json_length( c ) from utf8_mj_length where a = 6;
select a, c, json_length( c ) from utf8_mj_length where a = 7;

create table json_mj_length( a int, b json );

insert into json_mj_length values( 1, NULL );

select a, b, json_length( b ) from json_mj_length where a = 1;

# json_length() with vacuous path expressions

set names 'ascii';

--echo
--echo # path auto-converted to a utf8 string from ascii
--echo
select a, c, json_length( c, '$' ) from utf8_mj_length where a = 2;

set names 'utf8';

select a, c, json_length( c, '$' ) from utf8_mj_length where a = 1;
select a, c, json_length( c, '$' ) from utf8_mj_length where a = 2;

--echo
--echo # invalid json text
--error ER_INVALID_JSON_TEXT_IN_PARAM
select a, c, json_length( c, '$' ) from utf8_mj_length where a = 3;

select a, c, json_length( c, '$' ) from utf8_mj_length where a = 4;
select a, c, json_length( c, '$' ) from utf8_mj_length where a = 5;
select a, c, json_length( c, '$' ) from utf8_mj_length where a = 6;
select a, c, json_length( c, '$' ) from utf8_mj_length where a = 7;

select a, b, json_length( b, '$' ) from json_mj_length where a = 1;

drop table utf8_mj_length;
drop table json_mj_length;

select json_length( null );
select json_length( '1' );
--echo
--echo # invalid json text
--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_length( 'abc' );
select json_length( '"abc"' );
select json_length( 'true' );
select json_length( 'false' );
select json_length( 'null' );

select json_length( '{}' );
select json_length( '{ "a" : 100, "b" : 200 }' );
select json_length( '{ "a" : 100, "b" : [ 300, 400, 500 ] }' );
select json_length( '[]' );
select json_length( '[ null, "foo", true, 1.1 ]' );
select json_length( '[ null, "foo", true, { "a" : "b", "c" : "d" } ]' );
select json_length( '"foo"' );
select json_length( '1.2' );


# bad path expressions
--echo
--echo # invalid json path
--error ER_INVALID_JSON_PATH
select json_length( 'true', 'c$' );
--echo
--echo # invalid json path
--error ER_INVALID_JSON_PATH
select json_length( '{ "foo" : [ true, false ] }', '$.foo[bar]' );
--echo
--echo # wildcards not allowed in path expressions for this function
--error ER_INVALID_JSON_PATH_WILDCARD
select json_length( 'true', '$.*' );
--echo
--echo # wildcards not allowed in path expressions for this function
--error ER_INVALID_JSON_PATH_WILDCARD
select json_length( 'true', '$.foo**.bar' );

# json_length() with non-vacuous path expressions

# 1
select json_length( '[ 1, [ 2, 3, 4 ], 5 ]', '$[0]' );
# 3
select json_length( '[ 1, [ 2, 3, 4 ], 5 ]', '$[1]' );
# 1
select json_length( '[ 1, [ 2, 3, 4 ], 5 ]', '$[2]' );
# auto-wrapping: 1
select json_length( '[ 1, [ 2, 3, 4 ], 5 ]', '$[2][0]' ); # auto-wrap scalar
select json_length( '[ 1, [ 2, 3, 4 ], {"a": 1} ]', '$[2][0]' ); # ditto object

# non-existent path: null
select json_length( '[ 1, [ 2, 3, 4 ], 5 ]', '$[2][1]' );
# 3
select json_length( '[ 1, { "a": true, "b": false, "c": null }, 5 ]', '$[1]' );

# examples from the wl7909 spec
# returns 0
SELECT JSON_LENGTH
(
  '{}'
);

# returns 1
SELECT JSON_LENGTH
(
  '3'
);

# returns 2
SELECT JSON_LENGTH
(
  '{ "a" : 123, "b" : [ 123, 456, 789 ] }'
);

# returns 3
SELECT JSON_LENGTH
(
  '{ "a" : 123, "b" : [ 123, 456, 789 ] }',
  '$.b'
);

# returns null because the path does not exist
SELECT JSON_LENGTH
(
  '{ "a" : 123, "b" : [ 123, 456, 789 ] }',
  '$.c'
);

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_DEPTH function.
--echo # ----------------------------------------------------------------------
select json_depth(null);
select json_depth(cast(null as json));
select i, json_depth(j) from t1;
select json_depth(cast('[]' as json)),
       json_depth(cast('{}' as json)),
       json_depth(cast('null' as json)),
       json_depth(json_quote('foo'));
select json_depth(cast('[[2], 3, [[[4]]]]' as json));
select json_depth(cast('{"a": {"a1": [3]}, "b": {"b1": {"c": {"d": [5]}}}}' as json));

# examples from the wl7909 spec
# returns 1
SELECT JSON_DEPTH
(
  '{}'
);

# returns 1
SELECT JSON_DEPTH
(
  '[]'
);

# returns 1
SELECT JSON_DEPTH( '"abc"' );

# returns 1
SELECT JSON_DEPTH( CAST( '"abc"' AS JSON ) );

--error ER_INVALID_TYPE_FOR_JSON
SELECT JSON_DEPTH( 1 );

--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_DEPTH( 'abc' );

# returns 1
SELECT JSON_DEPTH( CAST( 1 AS JSON ) );

# returns 2
SELECT JSON_DEPTH
(
  '{ "a" : true, "b" : false, "c" : null }'
);

# returns 2
SELECT JSON_DEPTH
(
  '[ "a", true, "b" , false, "c" , null ]'
);

# returns 2
SELECT JSON_DEPTH
(
  '{ "a" : true, "b" : {}, "c" : null }'
);

# returns 2
SELECT JSON_DEPTH
(
  '[ "a", true, "b" , {}, "c" , null ]'
);

# returns 3
SELECT JSON_DEPTH
(
  '{ "a" : true, "b" : { "e" : false }, "c" : null }'
);

# returns 3
SELECT JSON_DEPTH
(
  '[ "a", true, "b" , { "e" : false }, "c" , null ]'
);

--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_DEPTH
(
  '[ "a", true, "b" , { "e" : false }, "c" , null'
);

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_REMOVE function.
--echo # ----------------------------------------------------------------------

# null args
select json_remove( null, '$[1]' );
select json_remove( null, '$[1]' ) is null;
select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ]', null );
select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ]', null ) is null;
select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ]', '$[1]', null );
select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ]', '$[1]', null ) is null;

# too few args

--echo
--echo # not enough args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_remove();
--echo
--echo # not enough args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ]' );
--echo
--echo # not enough args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_remove( '$[1]' );

# malformed args
--echo
--echo # invalid json text
--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ', '$[1]', '$[2]' );
--echo
--echo # invalid json path
--error ER_INVALID_JSON_PATH
select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ]', '$[1', '$[2]' );
--echo
--echo # invalid json path
--error ER_INVALID_JSON_PATH
select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ]', '$[1]', '$[2' );
--echo
--echo # Vacuous path expression
--error ER_JSON_VACUOUS_PATH
select json_remove( '[ 1, 2, 3 ]', '$' );
--echo
--echo # Vacuous path expression
--error ER_JSON_VACUOUS_PATH
select json_remove( '[ 1, 2, 3 ]', '$', '$[2]' );
--echo
--echo # Vacuous path expression
--error ER_JSON_VACUOUS_PATH
select json_remove( '[ 1, 2, 3 ]', '$[1]', '$' );

# wildcard/ellipsis not allowed in paths
--error ER_INVALID_JSON_PATH_WILDCARD
select json_remove( '[ 1, 2, 3 ]', '$[*]' );
--error ER_INVALID_JSON_PATH_WILDCARD
select json_remove( '[ 1, 2, 3 ]', '$**[2]' );
--error ER_INVALID_JSON_PATH_WILDCARD
select json_remove( '[ 1, 2, 3 ]', '$[2]', '$[*]' );
--error ER_INVALID_JSON_PATH_WILDCARD
select json_remove( '[ 1, 2, 3 ]', '$[2]', '$**[2]' );

# simple matches

select json_remove( '[ 1, 2, 3 ]', '$[0]' );
select json_remove( '[ 1, 2, 3 ]', '$[1]' );
select json_remove( '[ 1, 2, 3 ]', '$[2]' );
select json_remove( '[ 1, 2, 3 ]', '$[3]' );
select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ]', '$[1]' );

# one match nested inside another
--error ER_INVALID_JSON_PATH_WILDCARD
select json_remove( '[ { "a": { "a": true } } ]', '$**.a' );

# multiple paths
select json_remove( '[ { "a": true }, { "b": false }, { "c": null }, { "a": null } ]', '$[0].a', '$[2].c' );

# ellipsis with matches at different levels
--error ER_INVALID_JSON_PATH_WILDCARD
select json_remove( '[ { "a": true }, { "b": [ { "c": { "a": true } }  ] }, { "c": null }, { "a": null } ]', '$**.a' );

# nonsense path
select json_remove( '{"id": 123, "name": "systemQA", "array": [1, 2, 3]}', '$[0]' );

# different paths for each row
CREATE TABLE json_remove_t(j JSON, p TEXT);
INSERT INTO json_remove_t(p) VALUES ('$.a'), ('$.b'), ('$.c');
UPDATE json_remove_t SET j = '{"a":1,"b":2,"c":3}';
SELECT j, p, json_remove(j, p) FROM json_remove_t ORDER BY p;
DROP TABLE json_remove_t;

# examples from wl7909 spec
# returns the document {"a": "foo", "b": [true]}
SELECT JSON_REMOVE
(
  '{"a" : "foo", "b" : [true, {"c" : 123}]}',
  '$.b[ 1 ]'
);

# returns {"a": "foo", "b": [true, {}]} due to normalization
SELECT JSON_REMOVE
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123, "c" : 456 } ] }',
  '$.b[ 1 ].c'
);

# returns {"a": "foo", "b": [true, {}]}
SELECT JSON_REMOVE
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
  '$.b[ 1 ].c'
);

# returns the original document because the path doesn't identify an element
SELECT JSON_REMOVE
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123, "d" : 456 } ] }',
  '$.b[ 1 ].e'
);

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_MERGE function.
--echo # ----------------------------------------------------------------------

--echo
--echo # not enough args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_merge();
--echo
--echo # not enough args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_merge( '[ 1, 2, 3 ]' );

# null args result in NULL value
select json_merge( null, null );
select json_merge( null, '[ 1, 2, 3 ]' );
select json_merge( '[ 1, 2, 3 ]', null );
select json_merge( null, '[ 1, 2, 3 ]', '[ 4, 5, 6 ]' );
select json_merge( '[ 1, 2, 3 ]', null, '[ 4, 5, 6 ]' );
select json_merge( '[ 1, 2, 3 ]', '[ 4, 5, 6 ]', null );

--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_merge( '[1, 2]', '[3, 4' );
--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_merge( '[1, 2', '[3, 4]' );

# good json_merge() expressions
select json_merge( '1', '2' );
select json_merge( '1', '[2, 3]' );
select json_merge( '[1, 2]', '3' );
select json_merge( '1', '{ "a": 2 }' );
select json_merge( '{ "a": 2 }', '1' );
select json_merge( '[1, 2]', '[3, 4]' );
select json_merge( '{ "a": 2 }', '{ "b": 3}' );
select json_merge( '[1, 2]', '{ "a": 2 }' );
select json_merge( '{ "a": 2 }', '[1, 2]' );

select json_merge( '{"a": 1, "b": 2 }', '{"b": 3, "d": 4 }' );
select json_merge( '{"a": 1, "b": 2 }', '{"b": [3, 4], "d": 4 }' );
select json_merge( '{"a": 1, "b": [2, 3] }', '{"b": 4, "d": 4 }' );
select json_merge( '{"a": 1, "b": 2 }', '{"b": {"e": 7, "f": 8}, "d": 4 }' );
select json_merge( '{"b": {"e": 7, "f": 8}, "d": 4 }', '{"a": 1, "b": 2 }' );
select json_merge( '{"a": 1, "b": [2, 9] }', '{"b": [10, 11], "d": 4 }' );
select json_merge( '{"a": 1, "b": [2, 9] }', '{"b": {"e": 7, "f": 8}, "d": 4 }' );
select json_merge( '{"b": {"e": 7, "f": 8}, "d": 4 }', '{"a": 1, "b": [2, 9] }' );
select json_merge( '{"b": {"e": 7, "f": 8}, "d": 4 }', '{ "a": 1, "b": {"e": 20, "g": 21 } }' );

select json_merge( '1', '2', '3' );
select json_merge( '[1, 2 ]', '3', '[4, 5]' );
select json_merge
(
  '{ "a": true, "b": { "c": 3, "d": 4 }, "e": [ 1, 2 ] }',
  '{ "d": false, "b": { "g": 3, "d": 5 }, "f": [ 1, 2 ] }',
  '{ "m": true, "b": { "h": 8, "d": 4 }, "e": [ 3, 4 ] }'
);

CREATE TABLE json_merge_t(i INT, j JSON);
INSERT INTO json_merge_t VALUES
(0, NULL),
(1, 'true'),
(2, '5'),
(3, '[1,2]'),
(4, '{"a":["x", "y"]}'),
(5, '{"a":"b","c":"d"}');
SELECT t1.j, t2.j, json_merge(t1.j, t2.j), json_merge(t2.j, t1.j)
FROM json_merge_t t1, json_merge_t t2 ORDER BY t1.i, t2.i;
DROP TABLE json_merge_t;

# examples from the wl7909 spec
# returns [{"a": "foo", "b": [true, {"c": 123}]}, 5, 6]
SELECT JSON_MERGE
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
  '[ 5, 6]'
);

# returns {"a": "foo", "b": [true, {"c": 123}, false, 34]}
SELECT JSON_MERGE
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
  '{ "b": [ false, 34 ] }'
);

# returns {"a": "foo", "b": [true, {"c": 123}, "bar"]}
SELECT JSON_MERGE
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
  '{ "b": "bar" }'
);

# returns {"a": {"b": 1, "c": 1}}
SELECT JSON_MERGE
(
  '{ "a" : { "b" : 1 } }',
  '{ "a" : { "c" : 1 } }'
);

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_TYPE function.
--echo # ----------------------------------------------------------------------

# negative test
--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_type('abc');

select i, json_type(j) from t1;
select json_type('{"a": 2}');
select json_type('[1,2]');
select json_type('"scalar string"');
select json_type('true');
select json_type('false');
select json_type('null');
select json_type('1');
select json_type('-0');
select json_type('-0.0');
--error ER_INVALID_TYPE_FOR_JSON
select json_type(-1);
--error ER_INVALID_TYPE_FOR_JSON
select json_type(CAST(1 AS UNSIGNED));
select json_type('32767');
--error ER_INVALID_TYPE_FOR_JSON
select json_type(PI());
select json_type('3.14');

--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_type(CAST(CAST('2015-01-15' AS DATE) as CHAR CHARACTER SET 'utf8'));

create table keys1(i int, j json);
insert into keys1 select * from t1;

DROP TABLE t1;

# example from the wl7909 spec

create table rawOrders( orderID int, doc json );
insert into rawOrders values ( 1, '100' ), ( 2, '{ "id": 2, "quantity": 200 }' );

create table orders( orderID int, quantity int unsigned );

INSERT INTO orders( orderID, quantity )
  SELECT
   r.orderID,
   CASE( JSON_TYPE( r.doc ) )
     WHEN "INTEGER" THEN CAST( r.doc AS UNSIGNED INT )
     WHEN "OBJECT" THEN CAST( JSON_EXTRACT( r.doc, '$.quantity' ) AS UNSIGNED INT )
     ELSE NULL
   END
  FROM rawOrders r;

select * from rawOrders;
select * from orders;

drop table rawOrders;
drop table orders;

# the value here isn't important, but it should be stable
select charset(json_type('{}'));


--echo # ----------------------------------------------------------------------
--echo # Test of CAST(<column> AS JSON)
--echo # ----------------------------------------------------------------------
create table t1(dati datetime, da date,
                tim time, ts timestamp,
                y year,
                --
                ti tinyint,   tiu tinyint unsigned,
                si smallint,  siu smallint unsigned,
                mi mediumint, miu mediumint unsigned,
                i  int,       iu  int unsigned,
                bi bigint,    biu bigint unsigned,
                boo boolean,
                --
                dc decimal(5,2),
                n numeric(5,2),
                --
                f float, d double,
                bitt bit(10),
                blb blob,
                bin binary(10),
                en enum('a','b','c'),
                se set('a','b','c'),
                --
                ge geometry,
                po point,
                ls linestring,
                py polygon,
                js json
                );


insert into t1 values('2014-11-25 18:00', '2014-11-25',
                      '18:00:59', '2014-11-25 18:00',
                      '1999',
                      --
                      127, 255,
                      32767, 65535,
                      8388607, 16777215, -- 3 bytes
                      2147483647, 4294967295, -- 4 bytes
                      9223372036854775807, 18446744073709551615,
                      true,
                                            --
                      3.14,
                      3.14,
                      --
                      3.14, 3.14,
                      b'10101',
                      '10101abcde',
                      '10101abcde',
                      'b',
                      'a,c',
                      --
                      ST_GeomFromText('POINT(1 1)'),
                      ST_GeomFromText('POINT(1 1)'),
                      ST_GeomFromText('LINESTRING(0 0,1 1,2 2)'),
                      ST_GeomFromText('POLYGON((0 0,10 0,10 10,0 10,0 0),
                                            (5 5,7 5,7 7,5 7, 5 5))'),
                      '[123]'
                      );

select json_type(cast(dati as json)) from t1;
select json_type(cast(da as json)) from t1;
select json_type(cast(tim as json)) from t1;
select json_type(cast(ts as json)) from t1;

select json_type(cast(y as json)) from t1;
select json_type(cast(ti as json)) from t1;
select json_type(cast(tiu as json)) from t1;
select json_type(cast(si as json)) from t1;
select json_type(cast(siu as json)) from t1;
select json_type(cast(mi as json)) from t1;
select json_type(cast(miu as json)) from t1;
select json_type(cast(i as json)) from t1;
select json_type(cast(iu as json)) from t1;
select json_type(cast(bi as json)) from t1;
select json_type(cast(biu as json)) from t1;
select json_type(cast(boo as json)) from t1; # INTEGER (not enough info)

select json_type(cast(dc as json)) from t1;
# select json_type(cast(n as json)) from t1;

select json_type(cast(f as json)) from t1;
select json_type(cast(d as json)) from t1;

select json_type(cast(bitt as json)) from t1;
select json_type(cast(blb as json)) from t1;
select json_type(cast(bin as json)) from t1;

select json_type(cast(en as json)) from t1;
select json_type(cast(se as json)) from t1;

select json_type(cast(ge as json)) from t1;
select json_type(cast(po as json)) from t1;
select json_type(cast(ls as json)) from t1;
select json_type(cast(py as json)) from t1;

select json_type(cast(js as json)) from t1;

#
# same, but now show the printable value:
#
select cast(dati as json) from t1;
select cast(da as json) from t1;
select cast(tim as json) from t1;
select cast(ts as json) from t1;

select cast(y as json) from t1;
select cast(ti as json) from t1;
select cast(tiu as json) from t1;
select cast(si as json) from t1;
select cast(siu as json) from t1;
select cast(mi as json) from t1;
select cast(miu as json) from t1;
select cast(i as json) from t1;
select cast(iu as json) from t1;
select cast(bi as json) from t1;
select cast(biu as json) from t1;
select cast(boo as json) from t1; # INTEGER (not enough info)

select cast(dc as json) from t1;
# select cast(n as json) from t1;

select cast(f as json) from t1;
select cast(d as json) from t1;

select cast(bitt as json) from t1;
select cast(blb as json) from t1;
select cast(bin as json) from t1;

select cast(en as json) from t1;
select cast(se as json) from t1;

select cast(ge as json) from t1;
select cast(po as json) from t1;
select cast(ls as json) from t1;
select cast(py as json) from t1;

select cast(js as json) from t1;

--echo #
--echo # Bug#21442878 INCORRECT RETURN STATUS FROM
--echo #              ITEM_JSON_TYPECAST::VAL_JSON() ON PARSE ERRORS
--echo #
--error ER_INVALID_TYPE_FOR_JSON
select json_extract(en, '$') from t1;

drop table t1;

create table t1 ( c1 varchar(200) character set 'latin1',
                  c2 varchar(200) character set 'utf8' );
insert into t1 values ('[1,2]',  # legal json, but not utf-8
                       '[1,2 '); # illegal json, but utf-8

# convert latin1 to UTF-8
select cast(c1 as json) from t1;
--error ER_INVALID_JSON_TEXT_IN_PARAM
select cast(c2 as json) from t1;
--error ER_INVALID_JSON_TEXT_IN_PARAM
select cast(c2 as json) is null from t1;

drop table t1;

--echo # ----------------------------------------------------------------------
--echo # Test of CAST(literal AS JSON)
--echo # ----------------------------------------------------------------------
select json_type(cast(cast('2014-11-25 18:00' as datetime) as json));
select json_type(cast(cast('2014-11-25' as date) as json));
select json_type(cast(cast('18:00:59' as time) as json));
# select json_type(cast(cast('2014-11-25 18:00' as timestamp) as json)); -- cast target type not supported

# select json_type(cast(cast('1999' as year) as json)); -- cast target type not supported
select json_type(cast(127 as json));
select json_type(cast(255 as json));
select json_type(cast(32767 as json));
select json_type(cast(65535 as json));
select json_type(cast(8388607 as json));
select json_type(cast(16777215 as json));
select json_type(cast(2147483647 as json));
select json_type(cast(4294967295 as json));
select json_type(cast(9223372036854775807 as json));
select json_type(cast(18446744073709551615 as json));
select json_type(cast(true as json));
select json_type(cast(b'10101' as json));

select json_type(cast(cast(3.14 as decimal(5,2)) as json));
select json_type(cast(3.14 as json));
select json_type(cast(3.14E30 as json));
# select json_type(cast(cast(3.14 as numeral(5,2)) as json)); -- cast target type not supported

# select json_type(cast(cast(3.14 as double) as json)); -- cast target type not supported
# select json_type(cast(cast(3.14 as float) as json)); -- cast target type not supported

# select json_type(cast(cast(b'10101' as bit(10)) as json));  -- cast target type not supported
# select json_type(cast(cast('10101abcde' as blob) as json)); -- cast target type not supported
select json_type(cast(cast('10101abcde' as binary) as json));

# select json_type(cast(cast('a' as enum('a','b','c')) as json)); -- cast target type not supported
# select json_type(cast(cast('a,c' as set('a','b','c')) as json)); -- cast target type not supported

select json_type(cast(ST_GeomFromText('POINT(1 1)') as json));
select json_type(cast(ST_GeomFromText('LINESTRING(0 0,1 1,2 2)') as json));
select json_type(cast(ST_GeomFromText('POLYGON((0 0,10 0,10 10,0 10,0 0),
                                     (5 5,7 5,7 7,5 7, 5 5))') as json));
select json_type(cast(null as json));
select json_type(cast(null as json)) is null; # check that it is an SQL NULL
select json_type(null) is null;               # is an SQL NULL

#
# same, but now show the printable value:
#
select cast(cast('2014-11-25 18:00' as datetime) as json);
select cast(cast('2014-11-25' as date) as json);
select cast(cast('18:00:59' as time) as json);
# select cast(cast('2014-11-25 18:00' as timestamp) as json); -- cast target type not supported

# select cast(cast('1999' as year) as json); -- cast target type not supported
select cast(127 as json);
select cast(255 as json);
select cast(32767 as json);
select cast(65535 as json);
select cast(8388607 as json);
select cast(16777215 as json);
select cast(2147483647 as json);
select cast(4294967295 as json);
select cast(9223372036854775807 as json);
select cast(18446744073709551615 as json);
select cast(true as json);
select cast(b'10101' as json);

select cast(cast(3.14 as decimal(5,2)) as json);
select cast(3.14 as json);
select cast(3.14e0 as json);
# select cast(cast(3.14 as numeral(5,2)) as json); -- cast target type not supported

# select cast(cast(3.14 as double) as json); -- cast target type not supported
# select cast(cast(3.14 as float) as json); -- cast target type not supported

# select cast(cast(b'10101' as bit(10) as json);  -- cast target type not supported
# select cast(cast('10101abcde' as blob) as json); -- cast target type not supported
select cast(cast('10101abcde' as binary) as json);

# select cast(cast('a' as enum('a','b','c') as json); -- cast target type not supported
# select cast(cast('a,c' as set('a','b','c') as json); -- cast target type not supported

select cast(ST_GeomFromText('POINT(1 1)') as json);
select cast(ST_GeomFromText('LINESTRING(0 0,1 1,2 2)') as json);
select cast(ST_GeomFromText('POLYGON((0 0,10 0,10 10,0 10,0 0),
                                     (5 5,7 5,7 7,5 7, 5 5))') as json);
select cast(null as json);
select cast(null as json) is null; # check that it is an SQL NULL

# Two distinct but related bugs detected by Knut 2015-02-05 caused NULL for y here:
create table t2(x int);
insert into t2 values (1), (2);
select x, cast(y as json) from (select x, cast(x as json) as y from t2) s;
select x, cast(y as json) from (select x, cast(cast(x as json) as char charset utf8) as y from t2) s;

# Test that CAST string argument isn't treated as ANY_JSON_ATOM
# in that a MySQL string needs to be parsed to JSON here; it is not
# auto-converted to a JSON string as in ANY_JSON_ATOM contexts.
select cast('"abc"' as json);
--error ER_INVALID_JSON_TEXT_IN_PARAM
select cast('abc' as json);


drop table t2;

--echo # ----------------------------------------------------------------------
--echo # Test of CAST(<select> AS JSON)
--echo # ----------------------------------------------------------------------
# positive test cases
select cast((select 1) as json);

create table t(i int, j json, c char(10) character set 'utf8');
insert into t values (5, '6', '{}');
select cast((select i from t) as json);
select cast((select j from t) as json);
select cast((select c from t) as json);
select cast((select cast(i as json) from t) as json);
select cast((select cast(j as json) from t) as json);
select cast((select cast(c as json) from t) as json);

# negative test cases
--error ER_OPERAND_COLUMNS
select cast((select i,i from t) as json);
--error ER_OPERAND_COLUMNS
select cast((select * from t) as json);

insert into t values (7, '8', '[]');
--error ER_SUBQUERY_NO_1_ROW
select cast((select i from t) as json);

# Test what happens if the subquery returns NULL. The casts should
# return SQL NULL.
delete from t;
insert into t values (null, null, null);
select cast((select i from t) as json);
select cast((select j from t) as json);
select cast((select cast(i as json) from t) as json);
select cast((select cast(j as json) from t) as json);
select cast((select cast(c as json) from t) as json);

drop table t;

--echo # ----------------------------------------------------------------------
--echo # Test of generated columns that call JSON functions
--echo # ----------------------------------------------------------------------
CREATE TABLE t(id INT, j JSON,
               gc INT GENERATED ALWAYS AS (JSON_EXTRACT(j, '$[0]')));
INSERT INTO t(id, j) VALUES (0, '"5"'), (1, '[]'), (2, '[1,2]'), (3, '5');
--error ER_INVALID_JSON_VALUE_FOR_CAST
INSERT INTO t(j) VALUES ('{}');
--error ER_INVALID_JSON_VALUE_FOR_CAST
INSERT INTO t(j) VALUES ('{"a":1}');
--error ER_INVALID_JSON_VALUE_FOR_CAST
INSERT INTO t(j) VALUES ('"abc"');
--error ER_INVALID_JSON_TEXT
INSERT INTO t(j) VALUES ('');
--error ER_INVALID_JSON_TEXT
INSERT INTO t(j) VALUES ('[');
SELECT * FROM t ORDER BY id;
UPDATE t SET j = '[123]';
SELECT * FROM t ORDER BY id;
--error ER_INVALID_JSON_TEXT
UPDATE t SET j = '[';
DROP TABLE t;

CREATE TABLE t(id INT, j JSON,
               gc JSON GENERATED ALWAYS AS (JSON_ARRAY(j)));
INSERT INTO t(id, j)
  VALUES (1, '1'), (2, '[true, false]'), (3, '{"a":1,"b":2}');
--error ER_INVALID_JSON_TEXT
INSERT INTO t(j) VALUES ('');
--error ER_INVALID_JSON_TEXT
INSERT INTO t(j) VALUES ('[');
SELECT * FROM t ORDER BY id;
UPDATE t SET j = '"abc"';
SELECT * FROM t ORDER BY id;
--error ER_INVALID_JSON_TEXT
UPDATE t SET j = '[';
DROP TABLE t;

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_KEYS function.
--echo # ----------------------------------------------------------------------

select i, json_keys(j) from keys1;

# should all give NULL:
select json_keys(NULL);
select json_keys(NULL, '$.b');
select json_keys(NULL, NULL);
select json_keys('{"a": 1, "b": {"e": "foo", "b": 3}}', '$.a');
select json_keys('{"a": 1, "b": {"e": "foo", "b": 3}}', NULL);

# non NULL results
select json_keys('{"a": 1, "b": {"e": "foo", "b": 3}}');
select json_keys('{"a": 1, "b": {"e": "foo", "b": 3}}', '$.b');

--error ER_INVALID_JSON_PATH_WILDCARD
select json_keys('{"a": 1, "b": {"e": "foo", "b": 3}}', '$.*.b');

delete from keys1;
insert into keys1 values (0, NULL),
                        (1, '{"a": 1, "b": {"e": "foo", "b": 3}}');
select i, json_keys(j), json_keys(j, '$.b') from keys1;

# Examples from the specification
--echo # returns [ "a", "b" ]
SELECT JSON_KEYS('{ "a" : "foo", "b" : [ true, { "c" : "123" } ] }');

--echo # returns []
SELECT JSON_KEYS('{ "a" : "foo", "b" : [ true, { "c" : {} } ] }',
                '$.b[1].c');

--echo # returns NULL
SELECT JSON_KEYS('{ "a" : "foo", "b" : [ true, { "c" : {} } ] }',
                '$.a.b[2]');

--error ER_INVALID_JSON_PATH
SELECT JSON_KEYS('{"a":1}', '1010');
--error ER_INVALID_JSON_PATH
SELECT JSON_KEYS('{"a":1}', '1010') IS NULL;

# examples from the wl7909 spec
# returns [ "a", "b" ]
SELECT JSON_KEYS
(
  '{ "a" : "foo", "b" : [ true, { "c" : "123" } ] }'
);

# returns []
SELECT JSON_KEYS
(
  '{ "a" : "foo", "b" : [ true, { "c" : {} } ] }',
  '$.b[1].c'
);

# returns NULL
SELECT JSON_KEYS
(
  '{ "a" : "foo", "b" : [ true, { "c" : {} } ] }',
  '$.a.b[2]'
);

--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
SELECT JSON_KEYS();
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
SELECT JSON_KEYS('{}', '$', '$');

--echo # ----------------------------------------------------------------------
--echo # CAST(<json> AS CHAR). See also 'json_conversions.test' for other
--echo # conversion tests.
--echo # ----------------------------------------------------------------------
select cast(json_keys('{"a": 1}') as char);
select cast(cast(1 as json) as char);
select cast(json_keys(NULL) as char);
select cast(j as char) from keys1;

create table t(i int);
select cast(json_extract(j, '$.b.b') as char) from keys1;
insert into t select cast(json_extract(j, '$.b.b') as char) from keys1;
select * from t;
drop table t;
drop table keys1;

# Timestamp values sometimes got printed as base64 strings.
CREATE TABLE t(ts TIMESTAMP, j JSON AS (CAST(ts AS JSON)));
INSERT INTO t(ts) VALUES ('2000-01-01 00:00:00');
SELECT CAST(JSON_ARRAY(ts, j) AS CHAR) FROM t;
DROP TABLE t;

--echo # ----------------------------------------------------------------------
--echo # Path matching with double-quotes
--echo # ----------------------------------------------------------------------

# matches
select json_extract( '{ "one potato" : 1 }', '$."one potato"' );
# matches
select json_extract( '{ "a.b" : 1 }', '$."a.b"' );

# doesn't match
select json_extract( '{ "\\"a\\"": 1}', '$."a"' );
# matches
select json_extract( '{ "\\"a\\"": 1}', '$."\\"a\\""' );
# matches
select json_extract( '{ "a": 1}', '$."a"' );
# matches
select json_extract( '{ "a": 1}', '$.a' );

# examples from functional spec section on Path Syntax

# [3, 2]
select json_extract( '{ "a": [ [ 3, 2 ], [ { "c" : "d" }, 1 ] ], "b": { "c" : 6 }, "one potato": 7, "b.c" : 8 }', '$.a[0]' );
# 2
select json_extract( '{ "a": [ [ 3, 2 ], [ { "c" : "d" }, 1 ] ], "b": { "c" : 6 }, "one potato": 7, "b.c" : 8 }', '$.a[0][1]' );
# [ { "c": "d" }, 1 ]
select json_extract( '{ "a": [ [ 3, 2 ], [ { "c" : "d" }, 1 ] ], "b": { "c" : 6 }, "one potato": 7, "b.c" : 8 }', '$.a[1]' );
# { "c": "d" }
select json_extract( '{ "a": [ [ 3, 2 ], [ { "c" : "d" }, 1 ] ], "b": { "c" : 6 }, "one potato": 7, "b.c" : 8 }', '$.a[1][0]' );
# "d"
select json_extract( '{ "a": [ [ 3, 2 ], [ { "c" : "d" }, 1 ] ], "b": { "c" : 6 }, "one potato": 7, "b.c" : 8 }', '$.a[1][0].c' );
# 7
select json_extract( '{ "a": [ [ 3, 2 ], [ { "c" : "d" }, 1 ] ], "b": { "c" : 6 }, "one potato": 7, "b.c" : 8 }', '$."one potato"' );
# 6
select json_extract( '{ "a": [ [ 3, 2 ], [ { "c" : "d" }, 1 ] ], "b": { "c" : 6 }, "one potato": 7, "b.c" : 8 }', '$.b.c' );
# 8
select json_extract( '{ "a": [ [ 3, 2 ], [ { "c" : "d" }, 1 ] ], "b": { "c" : 6 }, "one potato": 7, "b.c" : 8 }', '$."b.c"' );


--echo # ----------------------------------------------------------------------
--echo # Test of JSON_EXTRACT function.
--echo # ----------------------------------------------------------------------

# errors
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_extract(NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_extract('$.b');
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_extract('{"a": 1, "b": {"e": "foo", "b": 3}}');

# Confused argument order
--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_extract('$.a', '{"a": 1, "b": {"e": "foo", "b": 3}}');

# NULLs
select json_extract(NULL, '$.b');
select json_extract(NULL, NULL);

# non-NULLs
select json_extract('{"a": 1, "b": {"e": "foo", "b": 3}}', '$.a');
select json_extract('{"a": 1, "b": {"e": "foo", "b": 3}}', '$.*');
select json_extract('{"a": 1, "b": {"e": "foo", "b": 3}}', '$.a', '$.b.e');
select json_extract('{"a": 1, "b": [1,2,3]}', '$.b[2]');
# one path is NULL
select json_extract('{"a": 1, "b": {"e": "foo", "b": 3}}', '$.a', NULL);

# Examples from the specification
--echo # returns a JSON value containing just the string "123"
SELECT JSON_EXTRACT('{ "a" : "foo", "b" : [ true, { "c" : "123" } ] }',
                   '$.b[ 1 ].c');

--echo # returns a JSON value containing just the number 123
SELECT JSON_EXTRACT('{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
                   '$.b[ 1 ].c');

--echo # raises an error because the document is not valid
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_EXTRACT('{ "a" : [ }',
                   '$.b[ 1 ].c');

--echo # raises an error because the path is invalid
--error ER_INVALID_JSON_PATH
SELECT JSON_EXTRACT('{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
                   '$.b[ 1 ].');

--echo # returns a JSON value containing the number 123 (because of
--echo # auto-wrapping the scalar)
SELECT JSON_EXTRACT('{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
                   '$.b[ 1 ].c[ 0 ]');
--echo # returns a JSON value containing the object because of auto-wrapping
SELECT JSON_EXTRACT('{ "a" : "foo", "b" : [ true, { "c" : {"not array": 4} } ] }',
                   '$.b[ 1 ].c[ 0 ]');

--echo # returns null because the path, although valid, does not identify a value
SELECT JSON_EXTRACT('{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
                   '$.b[ 1 ].c[ 1 ]');

--echo # returns a JSON value containing the number 123 (due to normalization)
SELECT JSON_EXTRACT('{ "a" : "foo", "b" : [ true, { "c" : 123, "c" : 456 } ] }',
                   '$.b[ 1 ].c');

--echo # returns a JSON array [ "foo", true ]
SELECT JSON_EXTRACT('{ "a" : "foo", "b" : [ true, { "c" : 123, "c" : 456 } ] }',
                   '$.a', '$.b[0]');

--echo # returns a JSON array [ true ]
SELECT JSON_EXTRACT('{ "a" : "foo", "b" : [ true, { "c" : 123, "c" : 456 } ] }',
                   '$.d', '$.b[0]');

# some examples verifying ellipsis behavior

# should have same result
select json_extract( '[1]', '$[0][0]' );
select json_extract( '[1]', '$**[0]' );

# should have same result
select json_extract( '{ "a": 1 }', '$.a[0]' );
select json_extract( '{ "a": 1 }', '$**[0]' );

# should have same result
select json_extract( '{ "a": 1 }', '$[0].a' );
select json_extract( '{ "a": 1 }', '$**.a' );

# should have same result
select json_extract( '{ "a": 1 }', '$[0].a[0]' );
select json_extract( '{ "a": 1 }', '$**[0]' );

# should have the same result
select json_extract( '{ "a": 1 }', '$[0].a' );
select json_extract( '{ "a": 1 }', '$**.a' );
select json_extract( '{ "a": 1 }', '$[0][0].a' );
select json_extract( '{ "a": 1 }', '$[0][0][0].a' );

# should have the same result
SELECT JSON_EXTRACT('[1, [[{"x": [{"a":{"b":{"c":42}}}]}]]]', '$**.a.*');
SELECT JSON_EXTRACT('[1, [[{"x": [{"a":{"b":{"c":42}}}]}]]]',
                    '$[1][0][0].x[0].a.*');

# examples from the wl7909 spec
# returns a JSON value containing just the string "123"
SELECT JSON_EXTRACT
(
  '{ "a" : "foo", "b" : [ true, { "c" : "123" } ] }',
  '$.b[ 1 ].c'
);

# returns a JSON value containing just the number 123
SELECT JSON_EXTRACT
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
  '$.b[ 1 ].c'
);

--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_EXTRACT
(
  '{ "a" : [ }',
  '$.b[ 1 ].c'
);

--error ER_INVALID_JSON_PATH
SELECT JSON_EXTRACT
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
  '$.b[ 1 ].'
);

# returns a JSON value containing the number 123 (because of auto-wrapping)
SELECT JSON_EXTRACT
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
  '$.b[ 1 ].c[ 0 ]'
);

# returns null because the path, although valid, does not identify a value
SELECT JSON_EXTRACT
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123 } ] }',
  '$.b[ 1 ].c[ 1 ]'
);

# returns a JSON value containing the number 123 (due to normalization)
SELECT JSON_EXTRACT
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123, "c" : 456 } ] }',
  '$.b[ 1 ].c'
);

# returns a JSON array ["foo", true]
SELECT JSON_EXTRACT
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123, "c" : 456 } ] }',
  '$.a', '$.b[0]'
);

# returns a the 'true' literal
SELECT JSON_EXTRACT
(
  '{ "a" : "foo", "b" : [ true, { "c" : 123, "c" : 456 } ] }',
  '$.d', '$.b[0]'
);

# should return NULL
select json_extract( '[ { "a": 1 }, { "a": 2 } ]', '$[*].b' ) jdoc;

# should return NULL
select json_extract( '[ { "a": 1 }, { "a": 2 } ]', '$[0].b' ) jdoc;

# should return 1
select json_extract( '[ { "a": 1 }, { "a": 2 } ]', '$[0].a' ) jdoc;

# should return [1, 2]
select json_extract( '[ { "a": 1 }, { "a": 2 } ]', '$[*].a' ) jdoc;

# should return [1]
select json_extract( '[ { "a": 1 }, { "b": 2 } ]', '$[*].a' ) jdoc;

# should return [3, 4]
select json_extract( '[ { "a": [3,4] }, { "b": 2 } ]', '$[0].a' ) jdoc;

# should return [[3, 4]]
select json_extract( '[ { "a": [3,4] }, { "b": 2 } ]', '$[*].a' ) jdoc;

# should return [[3, 4]]
select json_extract( '[ { "a": [3,4] }, { "b": 2 } ]', '$[0].a', '$[1].a' ) jdoc;

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_ARRAY_APPEND function.
--echo # ----------------------------------------------------------------------

# NULLs
select json_array_append(NULL, '$.b', cast(1 as json));
select json_array_append('[1,2,3]', NULL, cast(1 as json));
select json_array_append('[1,2,3]', '$', NULL);

# wrong # args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_array_append(NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_array_append(NULL, NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_array_append(NULL, NULL, NULL, NULL);

# positive test cases
create table t(j json);
insert into t values ('[ 1, 2, 3, {"a": [4,5,6]}]');
select json_array_append(j, '$[3].a', cast(7 as json)) from t;
select json_array_append(j, '$', 7) from t;
select json_array_append(j, '$', cast(7 as json), '$[3].a', 3.14) from t;
--echo # second path's append ignored since it doesn't specify an array
--echo # nor is it an existing scalar,  so no auto-wrapping either
select json_array_append(j, '$', 7, '$[3].b', cast(8 as json)) from t;
drop table t;

# auto-wrap
SELECT JSON_ARRAY_APPEND(cast('1' as json), '$', 3);
SELECT JSON_ARRAY_APPEND(cast('{"a": 3}' as json), '$', 3);

--error ER_INVALID_JSON_PATH_WILDCARD
select json_array_append(cast('{"a": {"b": [3]}}' as json), '$**[0]', 6);

# Examples from the specification

--echo # Auto-wrapping, since because the paths identify scalars.
--echo # should return {"a": "foo", "b": ["bar", 4], "c": ["wibble", "grape"]}
SELECT JSON_ARRAY_APPEND('{"a": "foo", "b": "bar", "c": "wibble"}',
                  '$.b', cast(4 as json),
                  '$.c', cast('"grape"' as json));

--echo # should return {"a": "foo", "b": [1, 2, 3, 4],
--echo #                "c": ["apple", "pear", "grape"]}
SELECT JSON_ARRAY_APPEND('{"a" : "foo","b": [1, 2, 3], "c": ["apple", "pear"]}',
                 '$.b', cast(4 as json),
                 '$.c', cast('"grape"' as json));

# without CAST: cf. not required for ANY_JSON_ATOM arguments in specification
SELECT JSON_ARRAY_APPEND('{"a" : "foo","b": [1, 2, 3], "c": ["apple", "pear"]}',
                 '$.b', 4,
                 '$.c', 'grape');

# wild cards, multiple pairs
--error ER_INVALID_JSON_PATH_WILDCARD
select json_array_append( '[[], [], []]', '$[*]', 3, '$[*]', 4);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_array_append( '[[], "not array", []]', '$[*]', 3, '$[*]', 4);

# examples from wl7909 spec
# should return {"a": "foo", "b": ["bar", 4], "c": ["wibble", "grape"]} due to autowrapping
SELECT JSON_ARRAY_APPEND
(
   '{ "a" : "foo", "b" : "bar", "c" : "wibble" }',
   '$.b', 4,
   '$.c', "grape"
);

# should return {"a": "foo", "b": [1, 2, 3, 4], "c": ["apple", "pear", "grape"]}
SELECT JSON_ARRAY_APPEND
(
   '{ "a" : "foo", "b" : [ 1, 2, 3 ], "c" : [ "apple", "pear" ] }',
   '$.b', 4,
   '$.c', "grape"
);

--echo # ----------------------------------------------------------------------
--echo # Bug#21373874 ASSERTION `PARENT' FAILED
--echo # ----------------------------------------------------------------------

select json_array_append('{"a":1}', '$[0]', 100);
select json_array_append('3', '$[0]', 100);
select json_array_append('3', '$[0][0][0][0]', 100);

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_INSERT function.
--echo # ----------------------------------------------------------------------

# NULLs
select json_insert(NULL, '$.b', cast(1 as json));
select json_insert('[1,2,3]', NULL, cast(1 as json));
select json_insert('[1,2,3]', '$[3]', NULL);

# wrong # args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_insert(NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_insert(NULL, NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_insert(NULL, NULL, NULL, NULL);

# positive test cases

select json_insert('[1,2,3]', '$[2]', 4);
select json_insert('[1,2,3]', '$[3]', 4);
select json_insert('[1,2,3]', '$[10]', 4);

select json_insert('{"c":4}', '$.c', 4);
select json_insert('{"c":4}', '$.a', 4);

select json_insert('1', '$', 4);
select json_insert('1', '$[0]', 4);
select json_insert('1', '$[1]', 4);
select json_insert('1', '$[10]', '4', '$[11]', 5);

select json_insert('[1,2,3]', '$[2][0]', 4);
select json_insert('[1,2,3]', '$[2][2]', 4);

select json_insert('{"a": 3}', '$.a[0]', 4);
select json_insert('{"a": 3}', '$.a[1]', 4, '$.a[2]', '5');

# wild card & auto-wrap (scalars)
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('{"a": [1], "b": 2}' as json), '$.*[1]', 6);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('{"a": 1, "b": 2}' as json), '$.*[1]', 6);

--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('{"a": {"b": 3}}' as json),   '$.a.*[1]', 6);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('{"a": {"b": [3]}}' as json), '$.a.*[1]', 6);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('{"a": {"b": 3}}' as json),   '$**[1]', 6);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('{"a": {"b": [3]}}' as json), '$**[1]', 6);

--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('[1]' as json), '$[*][1]', 6);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('[1]' as json), '$**[1]', 6);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('[1, [2], 3]' as json), '$[*][1]', 6);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('[1, [2], 3]' as json), '$**[1]', 6);

--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('[[1]]' as json), '$[*][1]', 6);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert(cast('[[1]]' as json), '$**[1]', 6);

# auto-wrap object
select json_insert(cast('{"a": 3}' as json), '$[1]', 6);


# Examples from the specification

# returns the original document because the path does exist
SELECT JSON_INSERT('{ "a" : "foo", "b" : [ 1, 2, 3 ] }', '$.a', true);

# inserts a number, returns '{ "a" : "foo", "b" : [ 1, 2, 3 ], "c" : 123 }
SELECT JSON_INSERT('{ "a" : "foo", "b" : [ 1, 2, 3 ] }', '$.c', 123);

# inserts a string, returns '{ "a" : "foo", "b" : [ 1, 2, 3 ], "c" : "123" }
SELECT JSON_INSERT('{ "a" : "foo", "b" : [ 1, 2, 3 ] }', '$.c', '123');

# returns '{ "a" : [ "foo", true ], "b" : [ 1, 2, 3 ] }'
SELECT JSON_INSERT('{ "a" : "foo", "b" : [ 1, 2, 3 ] }', '$.a[1]', true);

# should return { "a" : "foo", "b": true }
SELECT JSON_INSERT('{ "a" : "foo"}', '$.b', true, '$.b', false);

# examples from the wl7909 spec
# returns the original document because the path does exist
SELECT JSON_INSERT
(
  '{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
  '$.a',
  true
);

# inserts a number, returns '{"a": "foo", "b": [1, 2, 3], "c": 123}
SELECT JSON_INSERT
(
  '{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
  '$.c',
  123
);

# inserts a string, returns '{"a": "foo", "b": [1, 2, 3], "c": "123"}
SELECT JSON_INSERT
(
  '{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
  '$.c',
  '123'
);

# returns '{"a": ["foo", true], "b": [1, 2, 3]}'
SELECT JSON_INSERT
(
  '{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
  '$.a[1]',
  true
);

# returns {"a": "foo", "b": true}
SELECT JSON_INSERT
(
   '{ "a" : "foo"}',
   '$.b', true,
   '$.b', false
);

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_ARRAY_INSERT function.
--echo # ----------------------------------------------------------------------

# NULLs
select json_array_insert(NULL, '$.b[1]', 1);
select json_array_insert('[1,2,3]', NULL, 1);
select json_array_insert('[1,2,3]', '$[3]', NULL);

# wrong # args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_array_insert(NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_array_insert(NULL, NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_array_insert(NULL, NULL, NULL, NULL);

# path does not indicate a cell position
--error ER_INVALID_JSON_PATH_ARRAY_CELL
select json_array_insert('true', '$', 1);
--error ER_INVALID_JSON_PATH_ARRAY_CELL
select json_array_insert('true', '$.a', 1);
--error ER_INVALID_JSON_PATH_ARRAY_CELL
select json_array_insert('true', '$.a[1].b', 1);

# nop if there is no array at the path's parent
select json_array_insert( 'true', '$[0]', false );
select json_array_insert( 'true', '$[1]', false );
select json_array_insert( '{ "a": true }', '$.a[0]', false );
select json_array_insert( '{ "a": true }', '$.a[1]', false );

# positive tests
select json_array_insert( '[]', '$[0]', false );
select json_array_insert( '[]', '$[1]', false );
select json_array_insert( '[true]', '$[0]', false );
select json_array_insert( '[true]', '$[1]', false );
select json_array_insert( '[true]', '$[2]', false );
select json_array_insert( '{ "a": [] }', '$.a[0]', false );
select json_array_insert( '{ "a": [] }', '$.a[1]', false );
select json_array_insert( '{ "a": [true] }', '$.a[0]', false );
select json_array_insert( '{ "a": [true] }', '$.a[1]', false );
select json_array_insert( '{ "a": [true] }', '$.a[2]', false );

# insert into the middle of an array
select json_array_insert( '[1, 2, 3, 4]', '$[0]', false );
select json_array_insert( '[1, 2, 3, 4]', '$[1]', false );
select json_array_insert( '[1, 2, 3, 4]', '$[2]', false );
select json_array_insert( '[1, 2, 3, 4]', '$[3]', false );
select json_array_insert( '[1, 2, 3, 4]', '$[4]', false );
select json_array_insert( '[1, 2, 3, 4]', '$[5]', false );

select json_array_insert( '{ "a": [1, 2, 3, 4] }', '$.a[0]', false );
select json_array_insert( '{ "a": [1, 2, 3, 4] }', '$.a[1]', false );
select json_array_insert( '{ "a": [1, 2, 3, 4] }', '$.a[2]', false );
select json_array_insert( '{ "a": [1, 2, 3, 4] }', '$.a[3]', false );
select json_array_insert( '{ "a": [1, 2, 3, 4] }', '$.a[4]', false );
select json_array_insert( '{ "a": [1, 2, 3, 4] }', '$.a[5]', false );

# nop
select json_array_insert( '{ "a": [1, 2, 3, 4] }', '$.b[0]', false );
select json_array_insert( '{ "a": [1, 2, 3, 4] }', '$.b[1]', false );

# path caching and leg popping
create table jdoc( id int, doc json );
insert into jdoc values
( 1, '[ [ true ], [ false ] ]' ),
( 2, '[ [ 0 ], [ 1 ] ]' ),
( 3, '[ [ "abc" ], [ "def" ] ]' );

select id, json_array_insert( doc, '$[0][1]', 'fred' )
from jdoc order by id;

select id, json_array_insert( doc, '$[1][0]', 'fred' )
from jdoc order by id;

drop table jdoc;

create table t( id int, v varchar(10));
insert into t values (1, 'a'), (2, null), (3, 'a');
select id v, json_array_insert('[[1]]', '$[0][0]', v) from t order by id;
drop table t;

# no auto-wrapping
select json_array_insert( '"a"', '$[0]', true );
select json_array_insert( '[ "a" ]', '$[0][0]', true );
select json_array_insert( '"a"', '$[1]', true );

--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert('[]', '$.a.*[1]', 6);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert('[]', '$**[1]', 6);
--error ER_INVALID_JSON_PATH_WILDCARD
select json_insert('[]', '$[*][1]', 6);

# multiple paths,
select json_array_insert( '[ 1, 2, 3 ]', '$[1]', true, '$[1]', false );
select json_array_insert( '[ 1, 2, 3 ]', '$[1]',
       cast( '[ "a", "b", "c", "d" ]' as json ), '$[1][2]', false );

# test an error while evaluating the document expression
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_ARRAY_INSERT(JSON_EXTRACT('[1', '$'), '$[0]', 1);

# error in reading new value
--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_array_insert( '[ 1, 2, 3 ]', '$[1]', json_extract( '[', '$' ) );

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_SET function.
--echo # ----------------------------------------------------------------------

# NULLs
select json_set(NULL, '$.b', cast(1 as json));
select json_set('[1,2,3]', NULL, cast(1 as json));
select json_set('[1,2,3]', '$[3]', NULL);

# wrong # args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_set(NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_set(NULL, NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_set(NULL, NULL, NULL, NULL);

# Detect errors in nested function calls.
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_SET('{}', '$.name', JSON_EXTRACT('', '$'));

# positive test cases

select json_set('[1,2,3]', '$[2]', 4);
select json_set('[1,2,3]', '$[3]', 4);
select json_set('[1,2,3]', '$[10]', 4);

select json_set('{"c":4}', '$.c', 5);
select json_set('{"c":4}', '$.a', 5);

select json_set('1', '$', 4);
select json_set('1', '$[0]', 4);
select json_set('1', '$[1]', 4);
select json_set('1', '$[10]', '4', '$[11]', 5);

select json_set('[1,2,3]', '$[2][0]', 4);
select json_set('[1,2,3]', '$[2][2]', 4);

select json_set('{"a": 3}', '$.a[0]', 4);
select json_set('{"a": 3}', '$.a[1]', 4, '$.a[2]', '5');

# auto-wrap plus ellipsis with nested hits should give: {"a": [{"b": [3, 6]}, 6]}
--error ER_INVALID_JSON_PATH_WILDCARD
select json_set(cast('{"a": {"b": [3]}}' as json), '$**[1]', 6);

# Examples from the specification: Include when missing functions are
# available.

# returns { "a" : {}, "b" : [ 1, 2, 3 ] }
 SELECT JSON_SET('{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
                '$.a',
                JSON_OBJECT());

# # returns { "a" : "foo", "b" : [ 1, 2, 3 ], "c" : [ true, false ] }
# SELECT JSON_SET('{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
#                '$.c',
#                JSON_ARRAY( true, false ));

# # returns { "a" : "foo", "b" : [ 1, 2, 3 ], "c" : [ true, false ] }
# SELECT JSON_SET('{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
#                '$.c',
#                JSON_ARRAY( CAST( 'true' AS JSON ), CAST( 'false' AS JSON ) ));

# # returns [ 1, null, null, 2 ]
# SELECT JSON_SET('1', '$[3]', 2);

# should return { "a": { "b": false, "c": true } }
SELECT JSON_SET('{ "a" : "foo"}', '$.a',
                JSON_OBJECT( 'b', false ), '$.a.c', true);

# returns { "a" : {}, "b" : [ 1, 2, 3 ] }
select json_set('{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
               '$.a',
               cast('{}' as json));

# returns { "a" : "foo", "b" : [ 1, 2, 3 ], "c" : [ true, false ] }
select json_set('{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
               '$.c',
               cast('[true, false]' as json));

# returns [ 1, null, null, 2 ]
select json_set('1', '$[3]', 2);

# should return { "a": { "b": false, "c": true } }
select json_set('{ "a" : "foo"}', '$.a',
               cast('{"b": false}' as json), '$.a.c', true);

# examples from wl7909 spec
# returns {"a": {}, "b": [1, 2, 3]}
SELECT JSON_SET
(
  '{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
  '$.a',
  JSON_OBJECT()
);

# returns {"a": "foo", "b": [1, 2, 3], "c": [true, false]}
SELECT JSON_SET
(
  '{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
  '$.c',
  JSON_ARRAY( true, false )
);

# returns {"a": "foo", "b": [1, 2, 3], "c": [true, false]}
SELECT JSON_SET
(
  '{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
  '$.c',
  JSON_ARRAY( CAST( 'true' AS JSON ), CAST( 'false' AS JSON ) )
);

# returns [1, 2]
SELECT JSON_SET
(
  '1',
  '$[3]',
  2
);

# returns {"a": {"b": false, "c": true}}
SELECT JSON_SET
(
   '{ "a" : "foo"}',
   '$.a', JSON_OBJECT( 'b', false ),
   '$.a.c', true
);

--echo #
--echo # Bug #21304639: JSON_SET() WITH MULTI-LEG PATH RETURNS DIFFERENT
--echo #                RESULTS ON FIRST ROW VS NEXT
--echo #
create table t21304639(pk int);
insert into t21304639 values (2), (1), (3);
select json_set(
         json_object('existing', pk),
         '$.key_b.test',
         json_object('new', 'apple')
       ) as field1 from t21304639;

select json_set(
         json_object('existing', pk),
         '$.key_b.test',
         json_object('new', 'apple')
       ) as field1 from t21304639 order by field1;

drop table t21304639;

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_REPLACE function.
--echo # ----------------------------------------------------------------------

# NULLs
select json_replace(NULL, '$.b', cast(1 as json));
select json_replace('[1,2,3]', NULL, cast(1 as json));
select json_replace('[1,2,3]', '$[2]', NULL);

# wrong # args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_replace(NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_replace(NULL, NULL);
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_replace(NULL, NULL, NULL, NULL);

# positive test cases

select json_replace('[1,2,3]', '$[2]', 4);
select json_replace('[1,2,3]', '$[3]', 4);
select json_replace('[1,2,3]', '$[10]', 4);

select json_replace('{"c":4}', '$.c', 5);
select json_replace('{"c":4}', '$.a', 5);

select json_replace('1', '$', 4);
select json_replace('1', '$[0]', 4);
select json_replace('1', '$[1]', 4);
select json_replace('1', '$[10]', '4', '$[11]', 5);

select json_replace('[1,2,3]', '$[2][0]', 4);
select json_replace('[1,2,3]', '$[2][2]', 4);

select json_replace('{"a": 3}', '$.a[0]', 4);
select json_replace('{"a": 3}', '$.a[1]', 4, '$.a[2]', '5');


# Examples from the specification

# returns the original document because the path doesn't exist
SELECT JSON_REPLACE('{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
                   '$.c',
                   true);

# returns '{ "a" : true, "b" : [ 1, 2, 3 ] }'
SELECT JSON_REPLACE('{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
                   '$.a[0]',
                   true);

# returns the original document because the path doesn't exist
SELECT JSON_REPLACE('{ "a" : "foo", "b" : [ 1, 2, 3 ] }',
                   '$.b[5]',
                   true);

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_ARRAY function.
--echo # ----------------------------------------------------------------------

# NULLs
select json_array(NULL, '$.b', cast(1 as json));
select json_array('[1,2,3]', NULL, cast(1 as json));
select json_array('[1,2,3]', '$[3]', NULL);


# positive test cases
select json_array();
select json_array(3.14);
select json_array('[1,2,3]');
select json_array(cast('[1,2,3]' as json));
select json_array(1,2,3);
select json_array(b'0', b'1', b'10');
create table t (i int, j json, d double);
insert into t values (3, '["a", "b"]', 3.14);
select json_array(i, j, d) from t;
drop table t;

# Array with the smallest possible signed integer and the largest possible
# unsigned integer.
CREATE TABLE t(j JSON);
INSERT INTO t VALUES (JSON_ARRAY(-9223372036854775808, 18446744073709551614));
SELECT * FROM t;
DROP TABLE t;

# examples from the wl7909 spec
create table department( id int, deptName varchar(50), isExempt boolean, blobColumn blob );
insert into department values ( 405, 'Accounting', true, '<a><b>ccc</b><d></d></a>' );

# returns the empty array: []
SELECT JSON_ARRAY();

# returns ["Accounting", {"processed": true }]
SELECT JSON_ARRAY( d.deptName, CAST( '{ "processed" : true }' AS JSON ) )
FROM department d
WHERE id = 405;

# stores a JSON value in a JSON-typed column
create table json_table( json_column json );
INSERT INTO json_table( json_column )
  SELECT JSON_ARRAY( d.deptName, d.id, d.blobColumn )
  FROM department d
  WHERE id = 405;
drop table json_table;

drop table department;

create table misc_dt
(
  id int, py polygon
);

insert into misc_dt values
(
  1, ST_GeomFromText('POLYGON((0 0,10 0,10 10,0 10,0 0),
    (5 5,7 5,7 7,5 7, 5 5))')
),
(
  2, null
);

select id, json_array( true, py, false ) from misc_dt order by id;

drop table misc_dt;

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_OBJECT function.
--echo # ----------------------------------------------------------------------

# odd number of args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_object( 'a' );
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_object( 'a', 1, 'b' );

# null arg
--error ER_JSON_DOCUMENT_NULL_KEY
select json_object( null, 1 );

# positive tests
select json_object();
select json_object( 'a', null );
select json_object( 'a', 1 );
select json_object( 'a', 1, 'b', 'foo' );
select json_object( 'a', 1, 'b', 'foo', 'c', cast( '{ "d": "wibble" }' as json ) );
select json_object( 'a', true, 'b', false, 'c', cast( 'null' as json) );
select json_valid( json_object( '"a"', 1 ) );

# long key
select json_object( REPEAT('a', 64 * 1024), 1 );

# construct from data in a table
create table jro
(
  a int,
  b varchar( 10 ),
  c boolean
);
insert into jro( a, b, c ) values
( 0, 'zero', false ),
( 1, 'one', true ),
( null, null, null );

select a, json_object( 'a', a, 'b', b, 'c', c )
from jro
order by a;

drop table jro;

create table jro2( a int, b varchar( 10 ), c json );
insert into jro2 ( a, b, c ) values
( 1, 'array', '[ 1, 2, 3 ]' ), ( 2, 'object', '{ "d": "foo", "e": true }' );

select a, json_object( 'type', b, 'value', c )
from jro2 order by a;

drop table jro2;

# non-string keyNames are cast to CHAR
select json_object(json_array(), json_array());
select json_object( cast(json_array() as char), json_array());

select json_object( 1, json_array());
select json_object( cast(1 as char), json_array());

# examples from the wl7909 spec
create table department( id int, deptName varchar(50), isExempt boolean, blobColumn blob );
insert into department values ( 405, 'Accounting', true, '<a><b>ccc</b><d></d></a>' );

# returns the empty object: {}
SELECT JSON_OBJECT();

# returns {"deptName": "Accounting", "id": 405, "isExempt": true, "date": 2014-11-0400:00:00.000000}
SELECT JSON_OBJECT
(
  'deptName', d.deptName,
  'id', d.id,
  'isExempt', d.isExempt and true
)
FROM department d
WHERE id = 405;

drop table department;

# key names which aren't strings

create table misc_dt
(
  py polygon
);

insert into misc_dt values
(
  ST_GeomFromText('POLYGON((0 0,10 0,10 10,0 10,0 0),
    (5 5,7 5,7 7,5 7, 5 5))')
);

--error ER_INVALID_JSON_CHARSET
select json_object( py, 'def' ) from misc_dt;

drop table misc_dt;


--echo # ----------------------------------------------------------------------
--echo # Test of JSON_SEARCH function.
--echo # ----------------------------------------------------------------------

# wrong  number of args
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_search();
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_search( '{ "a": true }' );
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_search( '{ "a": true }', 'one' );

# null args
select json_search( null, 'one', 'foo' );
select json_search( '{ "a": "foo" }', null, 'foo' );
# FIXME. what should happen here?
#select json_search( '{ "a": "foo" }', 'one', null );
select json_search( '{ "a": "foo" }', 'one', 'foo', null, null );
select json_search( '{ "a": "foo" }', 'one', 'foo', null, '$.a', null );

# bad values for the oneOrAll arg
--error ER_JSON_BAD_ONE_OR_ALL_ARG
select json_search( '{ "a": "foo" }', 'twof', 'foo' );
--error ER_JSON_BAD_ONE_OR_ALL_ARG
select json_search( '{ "a": "foo" }', 'two', 'foo' );

# bad escape arg
--error ER_WRONG_ARGUMENTS
select json_search( '{ "a": "foo" }', 'one', 'foo', 'ab' );

# bad path args
--error ER_INVALID_JSON_PATH
select json_search( '{ "a": "foo" }', 'one', 'foo', null, '$a' );
--error ER_INVALID_JSON_PATH
select json_search( '{ "a": "foo" }', 'all', 'foo', null, '$.a', '$b' );

--error ER_BAD_FIELD_ERROR
select json_search(a, b, c);

# simple tests for search without path arguments
select json_search( '{ "a": "foobar" }', 'one', 'foo%' );
select json_search( '{ "a": "foobar", "b": "focus", "c": [ "arm", "foot", "shoulder" ] }', 'one', 'foo%' );
select json_search( '{ "a": "foobar", "b": "focus", "c": [ "arm", "foot", "shoulder" ] }', 'all', 'foo%' );
select json_search( '{ "a": "foobar", "b": "focus", "c": [ "arm", "foot", "shoulder" ] }', 'all', 'f__us' );

create table json_search_table( id_col int, json_col json );
insert into json_search_table values
( 1, '{ "a": "foobar" }' ),
( 2, '{ "a": "foobar", "b": "focus", "c": [ "arm", "foot", "shoulder" ] }' );

select id_col, json_search( json_col, 'all', 'foo%' )
from json_search_table
order by id_col;

select id_col, json_search( json_col, 'all', 'foot' )
from json_search_table
order by id_col;

select id_col, json_search( json_col, 'all', 'f__us' )
from json_search_table
order by id_col;

# tests with path arguments
delete from json_search_table;
insert into json_search_table values
( 1, '{ "a": "foobar" }' ),
( 2, '{ "a": [ "foolish", "folly", "foolhardy"  ], "b" : "fool" }' );

select json_search( '{ "a": [ "foolish", "folly", "foolhardy"  ], "b" : "fool" }', 'all', 'foo%', null, '$.a' );
select json_search( '{ "a": [ "foolish", "folly", "foolhardy"  ], "b" : "fool" }', 'all', 'foo%', null, '$.a', '$.b' );
select json_search( '{ "a": [ "foolish", "folly", "foolhardy"  ], "b" : "fool" }', 'one', 'foo%', null, '$.a', '$.b' );

select json_search( '{ "a": [ "foolish", "folly", "foolhardy"  ], "b" : "fool" }', 'ALL', 'foo%', null, '$.a' );
select json_search( '{ "a": [ "foolish", "folly", "foolhardy"  ], "b" : "fool" }', 'aLl', 'foo%', null, '$.a', '$.b' );
select json_search( '{ "a": [ "foolish", "folly", "foolhardy"  ], "b" : "fool" }', 'ONE', 'foo%', null, '$.a', '$.b' );

select id_col, json_search( json_col, 'all', 'foo%', null, '$.a' )
from json_search_table
order by id_col;
select id_col, json_search( json_col, 'all', 'foo%', null, '$.a', '$.b' )
from json_search_table
order by id_col;
select id_col, json_search( json_col, 'one', 'foo%', null, '$.a', '$.b' )
from json_search_table
order by id_col;

# wildcards in the path expression
select json_search
(
  '[ { "a": { "b": { "c": "fool" } } }, { "b": { "c": "shoulder" } }, { "c": { "c": "food"} } ]',
  'all',
  'foo%',
  null,
  '$**.c'
);

delete from json_search_table;
insert into json_search_table values
( 1, '{ "a": "foobar" }' ),
( 2, '[ { "a": { "b": { "c": "fool" } } }, { "b": { "c": "shoulder" } }, { "c": { "c": "food"} } ]' );

select id_col, json_search( json_col, 'all', 'foo%', null, '$.a', '$**.c' )
from json_search_table
order by id_col;
select id_col, json_search( json_col, 'one', 'foo%', null, '$.a', '$**.c' )
from json_search_table
order by id_col;

select json_search
(
  '[ { "a": { "b": { "c": "showtime" } } }, { "b": { "c": "shoulder" } }, { "c": { "c": "shoe"} } ]',
  'all',
  'sho%',
  null,
  '$**.c'
);

select json_search
(
  '[ { "a": { "b": { "c": "showtime" } } }, { "b": { "c": "shoulder" } }, { "c": { "c": "shoe"} } ]',
  'all',
  'sho%e',
  null,
  '$**.c'
);

select json_search
(
  '[ { "a": { "b": { "c": "showtime" } } }, { "b": { "c": "shoulder" } }, { "c": { "c": "shoe"} } ]',
  'all',
  'sho%',
  null,
  '$[*].c'
);

select json_search
(
  '[ { "a": { "b": { "c": "showtime" } } }, [ { "b": { "c": "shout" } }, { "c": { "c": "shoe"} } ] ]',
  'all',
  'sho%',
  null,
  '$[1]**.c'
);

# escape character
select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo%bar' );
select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo\%bar' );
select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo|%bar', '|' );
select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo|%bar', '|', '$[0]' );
select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo|%bar', '|', '$[0]', '$[1]' );
select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo|%bar', '|', '$[0]', '$[1]', '$[2]' );

select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo\%bar', null );
select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo\%bar', null, '$[0]' );
select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo\%bar', null, '$[1]' );
select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo|%bar', '|', '$[0]' );
select json_search( '[ "footbar", "foo%bar" ]', 'all', 'foo|%bar', '|', '$[1]' );

# search is case-sensitive
select json_search( '[ "abc", "ABC" ]', 'all', 'aBc' );
select json_search( '[ "abc", "ABC" ]', 'all', 'abc' );
select json_search( '[ "abc", "ABC" ]', 'all', 'ABC' );

# only matches strings, not numerics
select json_search( '[ 10, "10", 1.0, "1.0" ]', 'all', '1%' );

drop table json_search_table;

# verify that the double-quoted strings returned by json_search()
# are valid path expressions when unpacked via json_unquote().

create table jep( key_col int primary key, doc json, path varchar( 50 ) );
insert into jep values
( 1, '{ "onepotato": "seven"  }', '$.onepotato' ),
( 2, '{ "one potato": "seven"  }', '$."one potato"' ),
( 3, '{ "one \\"potato": "seven"  }', '$."one \\"potato"' ),
( 4, '{ "one \\npotato": "seven"  }', '$."one \\npotato"' );

select key_col,
       json_search( doc, 'all', 'seven' ) paths,
       json_unquote( cast( json_search( doc, 'all', 'seven' ) as char ) ) unquoted,
       path
from jep order by key_col;

drop table jep;

# examples from the wl7909 spec
# returns null because numeric values don't match string values
SELECT JSON_SEARCH
(
  '{ "a" : 123, "b" : [ 123, 456 ] }',
  'one',
  '123'
);

# returns "$.b[2]"
SELECT JSON_SEARCH
(
  '{ "a" : "123", "b" : [ 123, "789", "123", "456", "123" ] }',
  'one',
  '123',
  null,
  '$.b'
);

# could return either "$.a" or "$.b.key"
SELECT JSON_SEARCH
(
  '{ "a" : "123", "b" : { "key" : "123" } }',
  'one',
  '123'
);

# returns "$.b.key"
SELECT JSON_SEARCH
(
  '{ "a" : "1243", "b" : { "key" : "1234" } }',
  'one',
  '123%'
);

# returns "$.b.c"
SELECT JSON_SEARCH
(
  '{ "a" : "1243", "b" : { "key" : "1234", "c": "directorysub%directoryabc" } }',
  'one',
  'dir%torysub@%dir%',
  '@'
);

# returns null because the path doesn't exist
SELECT JSON_SEARCH
(
  '{ "a" : "1243", "b" : { "key" : "1234" } }',
  'one',
  '123%',
  null,
  '$.c'
);

# returns $."one potato"
SELECT JSON_UNQUOTE
(
  JSON_SEARCH
  (
    '{ "onepotato": "foot", "one potato": "food" , "one \\"potato": "fool" }',
    'all',
    'food'
  )
);

--echo # ----------------------------------------------------------------------
--echo # Test of CASE and IF expressions returning JSON
--echo # ----------------------------------------------------------------------
create table t(j json);
insert into t values (null), ('[3,4,5]');

select json_type(case (null is null) when 1 then
                cast('null' as json) else
                cast('[1,2,3]' as json) end);

select json_type(case (null is not null) when 1 then
                cast('null' as json) else
                cast('[1,2,3]' as json) end);

select json_type(case (j is null) when 1 then
                cast('null' as json) else
                cast('[1,2,3]' as json) end) from t;

# no else clause
select json_type(case (j is null) when 1 then cast(1 as json) end) from t;

select json_type( if(null is null,
                    cast('null' as json),
                    cast('[1,2,3]' as json)) );

select json_type( if(null is not null,
                    cast('null' as json),
                    cast('[1,2,3]' as json)));

select json_type( if(j is null,
                    cast('{"a": 6}' as json),
                    cast('[1,2,3]' as json))) from t;

select json_type( if(j is null,
                    NULL,
                    cast('[1,2,3]' as json))    ) from t;
--echo # ----------------------------------------------------------------------
--echo # Test of CASE and IF expressions with mix of JSON and other types
--echo # Common result type is VARCHAR
--echo # ----------------------------------------------------------------------

select json_type(case (j is null) when 1 then
                3.14 else
                cast('[1,2,3]' as json) end) from t;

select case (j is null) when 1 then
       3.14 else
       cast('[1,2,3]' as json) end from t;

select case (j is null) when 1 then
       'foobar' else
       cast('[1,2,3]' as json) end from t;

select json_type( if(j is null,
                 3.14,
                 cast('[1,2,3]' as json))) from t;

select if(j is null,
          3.14,
          cast('[1,2,3]' as json)) from t;


--echo # ----------------------------------------------------------------------
--echo # Test of IFNULL
--echo # ----------------------------------------------------------------------
select json_type(ifnull(j, cast(3 as json))) from t;
select ifnull(j, cast(3 as json)) from t;      # json_type masked a bug
select json_type(ifnull(NULL, cast(3 as json)));
select json_type(ifnull(cast(3 as json), NULL));
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_TYPE(IFNULL(JSON_EXTRACT(CONCAT(t1.j, 'abc'), '$'), t2.j))
FROM t t1, t t2;
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_TYPE(IFNULL(t1.j, JSON_EXTRACT(CONCAT(t2.j, 'abc'), '$')))
FROM t t1, t t2;

--echo # ----------------------------------------------------------------------
--echo # Json values used in text contexts
--echo # ----------------------------------------------------------------------
delete from t;
insert into t values (NULL), (cast('"aBc"' as json));
select upper(j) from t;
select cast(json_extract(cast(concat('[', cast('["A",2]' as json), ']') as json),
            '$[0][1]') as char) = 2;

--echo # ----------------------------------------------------------------------
--echo # Test of aggregate function MAX, MIN.
--echo # ----------------------------------------------------------------------

select max(cast('[1,2,3]' as json));
delete from t;
insert into t values (cast(1 as json)), (cast(10 as json)), (cast(2 as json));
select * from t order by j;

select max(j) from t;
select json_type(max(j)) from t;
select min(j) from t;
select json_type(max(j)) from t;

# if we want another sorting, cast to suitable type
select max(cast(j as unsigned)) from t;
--error ER_INVALID_TYPE_FOR_JSON
select json_type(max(cast(j as unsigned))) from t;
drop table t;

--echo #
--echo # Full Text Search on columns generated from JSON
--echo #

set names utf8;

create table messages
(
 id int,
 raw_message json,
 sender varchar(50) generated always as (json_unquote( json_extract( raw_message, '$.sender' ) )) stored,
 receiver varchar(50) generated always as (json_unquote( json_extract( raw_message, '$.receiver' ) )) stored,
 subject text generated always as (json_extract( raw_message, '$.subject' )) stored,
 received datetime generated always as (json_unquote( json_extract( raw_message, '$.received' ) )) stored,
 body text generated always as (json_extract( raw_message, '$.body' )) stored,
 fulltext key( subject ),
 fulltext key( body ),
 primary key( id ),
 key( received, sender, receiver ),
 key( sender, receiver, received )
) ENGINE = InnoDB;

insert into messages(id, raw_message) values
(
  1,
  '{
     "sender": "fred",
     "receiver": "alice",
     "subject": "lunch today?",
     "received": "2015-05-11 09:30:05",
     "body": "How about lunch around 11:30? Thai food sounds good to me."
  }'
),
(
  2,
  '{
     "sender": "alice",
     "receiver": "fred",
     "subject": "re: lunch today?",
     "received": "2015-05-11 09:45:05",
     "body": "Great! No big deadline today."
  }'
),
(
  3,
  '{
     "sender": "fred",
     "receiver": "alice",
     "subject": "tea at 4:00?",
     "received": "2015-05-11 14:30:05",
     "body": "I have some yummy scones."
  }'
),
(
  4,
  '{
     "sender": "alice",
     "receiver": "fred",
     "subject": "re: tea at 4:00?",
     "received": "2015-05-11 14:45:05",
     "body": "Yes indeed."
  }'
),
(
  5,
  '{
     "sender": "fred",
     "receiver": "alice",
     "subject": "lunch today?",
     "received": "2015-05-18 09:30:05",
     "body": "Chinese for lunch?"
  }'
),
(
  6,
  '{
     "sender": "alice",
     "receiver": "fred",
     "subject": "re: lunch today?",
     "received": "2015-05-18 09:45:05",
     "body": "Sorry. I have a tight deadline I have to meet."
  }'
)
;

analyze table messages;

select * from messages order by id;

# FULLTEXT INDEX can't be created on JSON column type.
--error ER_BAD_FT_COLUMN
alter table messages add fulltext index (raw_message) ;

# Search the message subject and body
select id from messages
  where match( body ) against ( 'thai' )
  and match( subject ) against( 'lunch' )
  order by id;
select id from messages
  where match( body ) against ( 'deadline' )
  and received > timestamp '2015-05-13 09:45:05'
  order by id;

# Search query with condition involving FTS clause and JSON function
select id from messages
  where match( body ) against ( 'thai' )
  and match( subject ) against( 'lunch' )
  and JSON_VALID(raw_message)
  and ( JSON_TYPE(raw_message) = "OBJECT")
  order by id;

select id from messages
  where match( body ) against ( 'thai' )
  and match( subject ) against( 'lunch' )
  and ( JSON_TYPE(JSON_KEYS(raw_message)) != "ARRAY"  or JSON_CONTAINS (raw_message,CAST('{"received": "2015-05-11 09:30:05"}' AS JSON) ) )
  order by id;

select id from messages
  where match( subject ) against( '+tea' in boolean mode)
  and LENGTH(JSON_UNQUOTE(JSON_EXTRACT( raw_message, '$.sender' ))) = 5
  order by id;

select id from messages
   where match( body ) against( 'lunch' with query expansion)
   and JSON_DEPTH(raw_message) != 0;

# update the message body
update messages
  set raw_message = json_replace(raw_message, '$.body', 'On second thought, I want Chinese.')
where id=1;
select * from messages order by id;
select id from messages
  where match( body ) against ( 'thai' )
  and match( subject ) against( 'lunch' )
  order by id;

drop table messages;

--echo # ----------------------------------------------------------------------
--echo # Test JSON arguments and return values of stored functions
--echo # ----------------------------------------------------------------------

create function make_message
(
 sender varchar(50),
 receiver varchar(50),
 subject text,
 received datetime,
 body text
)
returns json
language sql deterministic no sql
return json_object
(
  'sender', sender,
  'receiver', receiver,
  'subject', subject,
  'received', received,
  'body', body
);

create function extract_date( message json )
returns datetime
language sql deterministic no sql
return json_extract( message, '$.received' );

create table messages
(
 id int,
 raw_message json
);

insert into messages(id, raw_message) values
(
 1,
 make_message
  (
   'fred',
   'alice',
   'lunch today?',
   timestamp( '2015-05-11 09:30:05' ),
   'How about lunch at 11:30?'
  )
),
(
 2,
 make_message
  (
   'alice',
   'fred',
   're: lunch today?',
   timestamp( '2015-05-11 09:45:05' ),
   'Sorry. I am in meetings all day long.'
  )
),
(
 3,
 json_object
  (
    'sender', 'fred',
    'receiver', 'alice',
    'subject', 're: lunch today?',
    'received', timestamp( '2015-05-11 09:50:05' ),
    'body', 'Oh, bummer.'
  )
)
;
select * from messages order by id;

# should be DATETIME
select json_type
(
  json_extract
  (
    json_object
    (
      'sender', 'fred',
      'receiver', 'alice',
      'subject', 'lunch today?',
      'received', timestamp( '2015-05-11 09:45:05' ),
      'body', 'How about lunch at 11:30?'
    ),
    '$.received'
  )
) received_type
;

select id, extract_date( raw_message ) extracted_date
from messages order by id;

create function show_received_type( message json )
returns tinytext
language sql deterministic no sql
return json_type( json_extract( message, '$.received' ) );

# should be DATETIME
select show_received_type
(
 json_object
  (
    'sender', 'fred',
    'receiver', 'alice',
    'subject', 're: lunch today?',
    'received', timestamp( '2015-05-11 09:50:05' ),
    'body', 'Oh, bummer.'
  )
) received_type;

# should be DATETIME
select show_received_type
(
     make_message
     (
      'fred',
      'alice',
      'lunch today?',
      timestamp( '2015-05-11 09:30:05' ),
      'How about lunch at 11:30?'
     )
) received_type;

# should be DATETIME
select id, show_received_type( raw_message ) received_type
from messages order by id;

drop function show_received_type;
drop function make_message;
drop function extract_date;
drop table messages;

--echo # Test a function that fails.
CREATE FUNCTION func_that_fails() RETURNS JSON
LANGUAGE SQL DETERMINISTIC NO SQL
RETURN '[not valid json]';
--error ER_INVALID_JSON_TEXT
SELECT JSON_EXTRACT(func_that_fails(), '$');
DROP FUNCTION func_that_fails;

# test a more complicated stored function which declares a JSON variable

delimiter //;
create function get_types( input_value json )
returns json
language sql deterministic contains sql
begin
  declare array_length integer;
  declare return_value json;
  declare idx int;
  declare path varchar(100);

  set array_length = json_length( input_value );
  set return_value = json_array();
  set idx = 0;

  while idx < array_length do
    set path = concat( '$[', idx, ']' );
    set return_value = json_array_append
    (
      return_value,
      '$',
      json_type( json_extract( input_value, path ) )
    );

    set idx = idx + 1;
  end while;

  return return_value;
end//


delimiter ;//

create table blob_table( blob_col blob );
insert into blob_table values( '10101abcde' );

select json_type( dt.a ), dt.a
from
( select get_types
  (
    json_array
    (
      cast( '{}' as json ),
      cast( '[]' as json ),
      'null',
      true,
      1,
      2.3,
      timestamp( '2015-05-11 09:30:05' ),
      cast('23:24:25' as time),
      cast('2015-01-15' as date),
      b'10101',
      blob_col
    )
  ) a
  from blob_table
) dt;

drop table blob_table;
drop function get_types;

delimiter //;
create procedure merge_docs
(
  inout inout_value json
)
begin
  set inout_value = json_object();
end//
delimiter ;//


delimiter //;
create procedure merge_doc_types()
begin
  declare proc_inout json;
  declare tmp_types varchar(100);

  set proc_inout = null;

  call merge_docs( proc_inout );
  set tmp_types = json_type( proc_inout );
end//
delimiter ;//

call merge_doc_types();

drop procedure merge_doc_types;
drop procedure merge_docs;

delimiter //;
create function get_types( input_value json )
returns json
language sql deterministic contains sql
begin
  declare array_length integer;
  declare return_value json;
  declare idx int;
  declare path varchar(100);

  set array_length = json_length( input_value );
  set return_value = json_array();
  set idx = 0;

  while idx < array_length do
    set path = concat( '$[', idx, ']' );
    set return_value = json_array_append
    (
      return_value,
      '$',
      json_type( json_extract( input_value, path ) )
    );

    set idx = idx + 1;
  end while;

  return return_value;
end//
delimiter ;//

delimiter //;
create procedure merge_docs
(
  in in_value json,
  inout inout_value json,
  out out_value json
)
language sql deterministic contains sql
begin
  set out_value = json_merge( in_value, inout_value );
  set inout_value = in_value;
end//
delimiter ;//


delimiter //;
create procedure merge_doc_types
(
  out in_types varchar(100),
  out inout_types varchar(100),
  out out_types varchar(100)
)
language sql deterministic contains sql
begin
  declare proc_in json;
  declare proc_inout json;
  declare proc_out json;

  set proc_in = json_array
  (
   cast( '{}' as json ),
   cast( '[]' as json ),
   'null',
   true
  );

  set proc_inout = json_array
  (
   1,
   2.3,
   timestamp( '2015-05-11 09:30:05' ),
   cast('23:24:25' as time),
   cast('2015-01-15' as date),
   b'10101'
  );

  set proc_out = null;

  call merge_docs( proc_in, proc_inout, proc_out );
  set in_types = get_types( proc_in );
  set inout_types = get_types( proc_inout );
  set out_types = get_types( proc_out );
end//
delimiter ;//

call merge_doc_types( @in_types, @inout_types, @out_types );

select @in_types, @inout_types, @out_types;

drop procedure merge_doc_types;
drop procedure merge_docs;
drop function get_types;

--echo #
--echo # Bug#20898238: WRONG RESULT FOR MAX() OF JSON SCALARS RETURNED
--echo #               WHEN NULL IS PRESENT
--echo #
CREATE TABLE bug20898238(j JSON);
INSERT INTO bug20898238 VALUES ('{"id":1}'), (NULL), ('{"id":2}'), ('{"id":0}');
SELECT MIN(JSON_EXTRACT(j, '$.id')),
       MAX(JSON_EXTRACT(j, '$.id')) FROM bug20898238;
DROP TABLE bug20898238;

--echo # ----------------------------------------------------------------------
--echo # Test of aggregate function SUM, AVG: in constrast to strings, we do not
--echo # auto-convert to numeric (double) type:
--echo # ----------------------------------------------------------------------
create table t(j json, c varchar(20));
insert into t values (cast('[1,2,3]' as json), '[a,b,c]');
insert into t values (cast(7 as json), '7'),  (cast(2 as json), '2');
select sum(j), sum(cast(j as unsigned)), sum(c) from t;
select avg(j), avg(cast(j as unsigned)), avg(c) from t;

--echo # ----------------------------------------------------------------------
--echo # Test of aggregate function GROUP_CONCAT
--echo # ----------------------------------------------------------------------
select group_concat(j), group_concat(distinct j), group_concat(c) from t;
insert into t values (NULL, NULL);
select group_concat(j), group_concat(distinct j), group_concat(c) from t;
create table t2(j json);
insert into t2 values (json_array(ST_GeomFromText('LineString(0 0, 1 1, 2 2)')));
select group_concat(j) from t2;
drop table t2;

--echo # ----------------------------------------------------------------------
--echo # Test of aggregate function COUNT(DISTINCT) and unaggregated DISTINCT
--echo # ----------------------------------------------------------------------

create table t_doc( bucket int, doc json);

insert into t_doc values
( 1, cast( 1 as json ) ),
( 1, cast( 1.0 as json ) ),
( 1, cast( 1e0 as json ) ),
( 2, cast( cast( 1 as unsigned ) as json ) ),
( 2, cast( 2 as json ) ),
( 2, cast( 2.0 as json ) ),
( 3, cast( 2e0 as json ) ),
( 3, cast( cast( 7 as unsigned ) as json ) ),
( 3, cast( 7 as json ) ),
( 4, cast( 7.0 as json ) ),
( 4, cast( 7e0 as json ) ),
( 4, cast( cast( 7 as unsigned ) as json ) ),
( 5, cast( true as json ) ),
( 5, cast( true as json ) ),
( 5, cast( false as json ) ),
( 6, cast( false as json ) ),
( 6, cast( 'null' as json ) ),
( 6, cast( 'null' as json ) ),
( 7, cast( '"abc"' as json ) ),
( 7, cast( '"abc"' as json ) ),
( 7, cast( '"abcd"' as json ) ),
( 8, cast( '"abcd"' as json ) ),
( 8, cast( '{ "a": 1, "b": 2 }' as json ) ),
( 8, cast( '{ "a": 1, "b": 2 }' as json ) ),
( 9, cast( '{ "a": 1, "b": 3 }' as json ) ),
( 9, cast( '{ "a": 1, "b": 3 }' as json ) ),
( 9, cast( '[ true, false ]' as json ) ),
( 10, cast( '[ true, false ]' as json ) ),
( 10, cast( '[ true, true ]' as json ) );

select distinct( doc ) a from t_doc order by a;

select count( distinct doc ) from t_doc;
select bucket, count( distinct doc ) from t_doc group by bucket;

delete from t_doc;

create table dt(dati datetime, da date,
                tim time, ts timestamp,
                y year,
                --
                ti tinyint,   tiu tinyint unsigned,
                si smallint,  siu smallint unsigned,
                mi mediumint, miu mediumint unsigned,
                i  int,       iu  int unsigned,
                bi bigint,    biu bigint unsigned,
                boo boolean,
                --
                dc decimal(5,2),
                n numeric(5,2),
                --
                f float, d double,
                bitt bit(10),
                blb blob,
                bin binary(10),
                en enum('a','b','c'),
                se set('a','b','c'),
                --
                ge geometry,
                po point,
                ls linestring,
                py polygon,
                jso json,
                jsa json,
                id int
                );

# test with distinct values
insert into dt values('2014-11-25 18:00', '2014-11-25',
                      '18:00:59', '2014-11-25 17:00',
                      '1999',
                      --
                      127, 255,
                      32767, 65535,
                      8388607, 16777215, -- 3 bytes
                      2147483647, 4294967295, -- 4 bytes
                      9223372036854775807, 18446744073709551615,
                      true,
                                            --
                      3.1,
                      3.2,
                      --
                      3.3, 3.4,
                      b'10101',
                      '10101abcde',
                      '10001abcde',
                      'b',
                      'a,c',
                      --
                      ST_GeomFromText('POINT(1 1)'),
                      ST_GeomFromText('POINT(1 2)'),
                      ST_GeomFromText('LINESTRING(0 0,1 1,2 2)'),
                      ST_GeomFromText('POLYGON((0 0,10 0,10 10,0 10,0 0),
                                            (5 5,7 5,7 7,5 7, 5 5))'),
                      '{"a": 1, "b": 2 }',
                      '[1, 2]',
                      1
                      ),

                      ('2013-11-25 18:00', '2013-11-25',
                      '17:00:59', '2013-11-25 17:00',
                      '1998',
                      --
                      126, 254,
                      32766, 65534,
                      8388606, 16777214, -- 3 bytes
                      2147483646, 4294967294, -- 4 bytes
                      9223372036854775806, 18446744073709551614,
                      false,
                                            --
                      4.1,
                      4.2,
                      --
                      4.3, 4.4,
                      b'10111',
                      '10001abcdf',
                      '10101abcdf',
                      'a',
                      'a,b',
                      --
                      ST_GeomFromText('POINT(1 3)'),
                      ST_GeomFromText('POINT(1 4)'),
                      ST_GeomFromText('LINESTRING(0 0,1 1,2 3)'),
                      ST_GeomFromText('POLYGON((0 0,10 0,10 10,0 9,0 0),
                                            (5 5,7 5,7 7,5 7, 5 5))'),
                      '{"a": 1, "b": 3 }',
                      '[1, 3]',
                      2
                      );

# types whose representations are unstable across platforms
insert into t_doc select id, cast(f as json) from dt;
insert into t_doc select id, cast(d as json) from dt;

insert into t_doc select * from t_doc;

select count( distinct doc ) from t_doc;
select bucket, count( distinct doc ) from t_doc group by bucket;

delete from t_doc;

# types which have stable representations across platforms

insert into t_doc select id, cast(dati as json) from dt;
insert into t_doc select id, cast(da as json) from dt;
insert into t_doc select id, cast(tim as json) from dt;
insert into t_doc select id, cast(ts as json) from dt;
insert into t_doc select id, cast(y as json) from dt;

insert into t_doc select id, cast(ti as json) from dt;
insert into t_doc select id, cast(tiu as json) from dt;
insert into t_doc select id, cast(si as json) from dt;
insert into t_doc select id, cast(siu as json) from dt;
insert into t_doc select id, cast(mi as json) from dt;
insert into t_doc select id, cast(miu as json) from dt;
insert into t_doc select id, cast(i as json) from dt;
insert into t_doc select id, cast(iu as json) from dt;
insert into t_doc select id, cast(bi as json) from dt;
insert into t_doc select id, cast(biu as json) from dt;

# FIXME: booleans don't retain their boolean values. they become ints.
#insert into t_doc select id, cast(boo as json) from dt;

insert into t_doc select id, cast(dc as json) from dt;
insert into t_doc select id, cast(n as json) from dt;

insert into t_doc select id, cast(bitt as json) from dt;
insert into t_doc select id, cast(blb as json) from dt;
insert into t_doc select id, cast(bin as json) from dt;
insert into t_doc select id, cast(en as json) from dt;
insert into t_doc select id, cast(se as json) from dt;

insert into t_doc select id, cast(ge as json) from dt;
insert into t_doc select id, cast(po as json) from dt;
insert into t_doc select id, cast(ls as json) from dt;
insert into t_doc select id, cast(py as json) from dt;
insert into t_doc select id, jso from dt;
insert into t_doc select id, jsa from dt;

insert into t_doc select * from t_doc;

select distinct( doc ) a from t_doc order by a;
select count( distinct doc ) from t_doc;
select bucket, count( distinct doc ) from t_doc group by bucket;

# test with non-distinct values

delete from t_doc;

create table ndt(dati datetime,
                ts timestamp,
                --
                ti tinyint,   tiu tinyint unsigned,
                si smallint,  siu smallint unsigned,
                mi mediumint, miu mediumint unsigned,
                i  int,       iu  int unsigned,
                bi bigint,    biu bigint unsigned,
                --
                dc decimal(5,2),
                n numeric(5,2),
                --
                f float, d double,
                id int
                );


insert into ndt values('2014-11-25 18:00',
                      '2014-11-25 18:00',
                      --
                      1, 1,
                      1, 1,
                      1, 1,
                      1, 1,
                      1, 1,
                      --
                      1.0,
                      1.0,
                      --
                      1.0, 1.0,
                      1
                      ),

                      ('2013-11-25 18:00',
                      '2013-11-25 18:00',
                      --
                      2, 2,
                      2, 2,
                      2, 2,
                      2, 2,
                      2, 2,
                      --
                      2.0,
                      2.0,
                      --
                      2.0, 2.0,
                      2
                      );

insert into t_doc select id, cast(dati as json) from ndt;
insert into t_doc select id, cast(ts as json) from ndt;

insert into t_doc select id, cast(ti as json) from ndt;
insert into t_doc select id, cast(tiu as json) from ndt;
insert into t_doc select id, cast(si as json) from ndt;
insert into t_doc select id, cast(siu as json) from ndt;
insert into t_doc select id, cast(mi as json) from ndt;
insert into t_doc select id, cast(miu as json) from ndt;
insert into t_doc select id, cast(i as json) from ndt;
insert into t_doc select id, cast(iu as json) from ndt;
insert into t_doc select id, cast(bi as json) from ndt;
insert into t_doc select id, cast(biu as json) from ndt;

insert into t_doc select id, cast(dc as json) from ndt;
insert into t_doc select id, cast(n as json) from ndt;

insert into t_doc select id, cast(f as json) from ndt;
insert into t_doc select id, cast(d as json) from ndt;

insert into t_doc select * from t_doc;

select distinct( doc ) a from t_doc order by a;
select count( distinct doc ) from t_doc;
select bucket, count( distinct doc ) from t_doc group by bucket;

drop table t_doc;
drop table dt;
drop table ndt;

--echo # ----------------------------------------------------------------------
--echo # Special CASTing behavior of geometry types
--echo # ----------------------------------------------------------------------

create table jtable( id int, descr varchar(20), doc json );

create table misc_dt
(
  ge geometry,
  po point,
  ls linestring,
  py polygon
);

insert into misc_dt values
(
  ST_GeomFromText('POINT(1 1)'),
  ST_GeomFromText('POINT(1 1)'),
  ST_GeomFromText('LINESTRING(0 0,1 1,2 2)'),
  ST_GeomFromText('POLYGON((0 0,10 0,10 10,0 10,0 0),
    (5 5,7 5,7 7,5 7, 5 5))')
);

insert into jtable select 1, 'geometry', cast(ge as json) from misc_dt;
insert into jtable select 2, 'point', cast(po as json) from misc_dt;
insert into jtable select 3, 'linestring', cast(ls as json) from misc_dt;
insert into jtable select 4, 'polygon', cast(py as json) from misc_dt;
#
select id, descr, json_type( doc ), doc from jtable order by id;

select json_object
(
  'geometry', ST_GeomFromText('POINT(1 1)'),
  'point', ST_GeomFromText('POINT(1 1)'),
  'linestring', ST_GeomFromText('LINESTRING(0 0,1 1,2 2)'),
  'polygon', ST_GeomFromText('POLYGON((0 0,10 0,10 10,0 10,0 0),
    (5 5,7 5,7 7,5 7, 5 5))')
);

# verify the workaround for CASTing JSON values to GEOMETRY
delete from misc_dt;
select * from misc_dt;
insert into misc_dt values
(
  (select ST_GeomFromGeoJSON( cast( doc as char ) ) from jtable where id = 1),
  (select ST_GeomFromGeoJSON( cast( doc as char ) ) from jtable where id = 2),
  (select ST_GeomFromGeoJSON( cast( doc as char ) ) from jtable where id = 3),
  (select ST_GeomFromGeoJSON( cast( doc as char ) ) from jtable where id = 4)
);
select ST_AsGeoJSON( ge ),
       ST_AsGeoJSON( po ),
       ST_AsGeoJSON( ls ),
       ST_AsGeoJSON( py )
from misc_dt;

drop table misc_dt;
drop table jtable;

create table jtable( id int, descr varchar(20), doc json );

create table misc_dt
(
  ge geometrycollection,
  po multipoint,
  ls multilinestring,
  py multipolygon
);

insert into misc_dt values
(
  geometrycollection(point(1, 1), point(2, 2)),
  multipoint(point(1, 1), point(2, 2)),
  multilinestring
  (
    linestring(point(0, 0), point(1, 1), point(2, 2)),
    linestring(point(0, 0), point(11, 11), point(12, 12))
  ),
  multipolygon
  (
      polygon
      (
        linestring(point(0, 0), point(10, 0), point(10, 10), point(0, 10), point(0, 0)),
        linestring(point(5, 5), point(7, 5), point(7, 7), point(5, 7), point(5, 5))
      ),
      polygon
      (
        linestring(point(0, 0), point(10, 0), point(10, 10), point(0, 10), point(0, 0)),
        linestring(point(5, 5), point(7, 5), point(7, 7), point(5, 7), point(5, 5))
      )
  )
);

insert into jtable select 1, 'geometrycollection', cast(ge as json) from misc_dt;
insert into jtable select 2, 'multipoint', cast(po as json) from misc_dt;
insert into jtable select 3, 'multilinestring', cast(ls as json) from misc_dt;
insert into jtable select 4, 'multipolygon', cast(py as json) from misc_dt;
#
select id, descr, json_type( doc ), doc from jtable order by id;

select ST_AsGeoJSON( ge ),
       ST_AsGeoJSON( po ),
       ST_AsGeoJSON( ls ),
       ST_AsGeoJSON( py )
from misc_dt;

delete from misc_dt;
select * from misc_dt;
insert into misc_dt values
(
  (select ST_GeomFromGeoJSON( cast( doc as char ) ) from jtable where id = 1),
  (select ST_GeomFromGeoJSON( cast( doc as char ) ) from jtable where id = 2),
  (select ST_GeomFromGeoJSON( cast( doc as char ) ) from jtable where id = 3),
  (select ST_GeomFromGeoJSON( cast( doc as char ) ) from jtable where id = 4)
);
select ST_AsGeoJSON( ge ),
       ST_AsGeoJSON( po ),
       ST_AsGeoJSON( ls ),
       ST_AsGeoJSON( py )
from misc_dt;

drop table misc_dt;
drop table jtable;

--echo # ----------------------------------------------------------------------
--echo # Test of COALESCE
--echo # ----------------------------------------------------------------------

select coalesce(cast(1 as json), cast(2 as json));
select coalesce(j, cast(3 as json)) from t;
select coalesce(j, 666) from t;
select json_type(coalesce(j, '[1,2,3]')) from t;
--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_type(coalesce(j, 'abc')) from t;
select json_type(coalesce(j, cast('"arg2"' as json))) from t order by c;
select json_type(coalesce(j, j)) from t order by c;
--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_type(coalesce(json_extract(concat(j, 'abc'), '$'), j)) from t;
--error ER_INVALID_JSON_TEXT_IN_PARAM
select json_type(coalesce(t1.j, json_extract(concat(t2.j, 'abc'), '$')))
from t t1, t t2;

drop table t;

--echo # ----------------------------------------------------------------------
--echo # Auto-convert of non-utf8 returning system function
--echo # ----------------------------------------------------------------------
create table t(j json, id int);
insert into t values ('{"user": "foo"}', 8), (NULL, 8);
update t set j=json_set(j, '$.user', current_user()) where id=8;
select j from t;
update t set j=json_set(j, '$.user', rtrim('foo    '))  where id=8;
select j from t;
update t set j=json_set(j, '$.user', hex('abc'))  where id=8;
select j from t;
update t set j=json_set(j, '$.user', md5('bingle'))  where id=8;
select j from t;
update t set j=json_set(j, '$.user', database())  where id=8;
select j from t;
update t set j=json_set(j, '$.user', schema()) where id=8;
select j from t;
#
# The hex of some UTF-8 from supplementary plane: U+2070E
update t set j=json_set(j, '$.user',
  cast(UNHEX('F0A09C8E') as char character set 'utf8mb4')) where id=8;
set names 'utf8mb4'; # se we can see the character
select j from t;
select char_length(json_extract(j, '$.user')) from t;
drop table t;

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_QUOTE, JSON_UNQUOTE
--echo # ----------------------------------------------------------------------

--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_quote();
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_quote('abc', 'def');
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_quote(NULL, 'def');
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_quote('abc', NULL);

--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_unquote();
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_unquote('"abc"', '"def"');
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_unquote(NULL, 'def');
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
select json_unquote('"abc"', NULL);

select json_quote(NULL);
select json_unquote(NULL);

select json_quote('abc');
select json_quote(convert('abc' using ascii));
select json_quote(convert('abc' using latin1));
select json_quote(convert('abc' using utf8));
select json_quote(convert('abc' using utf8mb4));

select json_unquote('abc');                      # should do nothing

select json_unquote('"abc"');
select json_unquote(convert('"abc"' using ascii));
select json_unquote(convert('"abc"' using latin1));
select json_unquote(convert('"abc"' using utf8));
select json_unquote(convert('"abc"' using utf8mb4));

select json_quote('"');
select json_unquote('"');                        # should do nothing

--error ER_INCORRECT_TYPE
select json_quote(123);                          # integer not allowed
--error ER_INCORRECT_TYPE
select json_unquote(123);                        # integer not allowed

select json_unquote('""');                       # empty string
select char_length(json_unquote('""'));          # verify empty string
select json_unquote('"" ');                      # unchanged: no final "
select json_unquote(cast(json_quote('abc') as json)); # round trip

# No change in this JSON string: it is an object
select cast('{"abc": "foo"}' as json);
select json_unquote(cast('{"abc": "foo"}' as json));

# This is a JSON string, so it is actually unquoted
select json_extract(cast('{"abc": "foo"}' as json), '$.abc');
select json_unquote(json_extract(cast('{"abc": "foo"}' as json), '$.abc'));

# Bug fix: thse should be the same
select json_unquote('["a", "b", "c"]');
select json_unquote(cast('["a", "b", "c"]' as json));

select charset(json_unquote('"abc"'));

# MySQL and ECMA strings have different sets of printable control
# character conventions. So make sure we get the desired test
# characters in by inserting them one by one and checking that we insert
# a single character,  which we can test JSON_QUOTE on:

create table t (c varchar(20));
insert into t values ('\\');
select char_length(c) from t;
insert into t values (X'0C');                   # formfeed \f
select sum(char_length(c)) from t;
insert into t values ('"');
select sum(char_length(c)) from t;
insert into t values ('\a');
select sum(char_length(c)) from t;
insert into t values ('\b');
select sum(char_length(c)) from t;
insert into t values ('\t');
select sum(char_length(c)) from t;
insert into t values ('\n');
select sum(char_length(c)) from t;
insert into t values ('\r');
select sum(char_length(c)) from t;
insert into t values (X'10');                   # needs \u escape
select sum(char_length(c)) from t;

select group_concat(c) from t;
select char_length(group_concat(c)) from t;
select json_quote(group_concat(c)) from t;
select char_length(json_quote(group_concat(c))) from t;

select convert(group_concat(c) using utf8mb4) =
       json_unquote(group_concat(c)) as should_be_equal from t;

select json_quote(convert(X'e68891' using utf8));    # chinese "I" (wo3)
select json_quote(convert(X'e68891' using utf8mb4)); # chinese "I" (wo3)

select cast(json_quote(convert(X'e68891' using utf8)) as json);

select json_unquote(convert(X'e68891' using utf8));  # chinese "I" (wo3)

select  json_quote(json_quote(json_quote('abc')));     # deep quote
select  json_unquote(json_unquote(json_unquote(        # long round trip of it
          json_quote(json_quote(json_quote('abc'))))));

# DATE/TIME will lose their quotes, too:
select cast(cast('2015-01-15 23:24:25' as datetime) as json);
select json_unquote(cast(cast('2015-01-15 23:24:25' as datetime) as json));

# as well as opaque values:
select cast(st_geomfromtext('point(1 1)') as json);
select json_unquote(cast(st_geomfromtext('point(1 1)') as json));

delete from t;
alter table t add column (j json);
insert into t values (NULL, NULL);

select json_quote(c), json_quote(j) from t;
select json_unquote(c), json_unquote(j) from t;

drop table t;

--echo #
--echo # Bug#21257946 JSON_TYPE(TEXT) OF TABLE COLUMN STICKS WITH NULL
--echo #              AFTER FIRST ENCOUNTER OF NULL
--echo #
CREATE TABLE T_WITH_NULLS(i INT, j JSON);
INSERT INTO T_WITH_NULLS VALUES
(0, NULL),
(1, '[1]'),
(2, NULL),
(3, '{"a":"b"}'),
(4, NULL),
(5, '"abc"');
let $query= SELECT
JSON_VALID(j),
JSON_TYPE(j),
JSON_KEYS(j),
JSON_EXTRACT(j, '\$'),
JSON_REMOVE(j, '\$.a.b.c'),
JSON_ARRAY_APPEND(j, '\$', 2),
JSON_SET(j, '\$[0]', 2),
JSON_INSERT(j, '\$[0]', 2),
JSON_REPLACE(j, '\$[0]', 2),
JSON_MERGE(j, j),
JSON_SEARCH(j, 'one', 'abc'),
JSON_CONTAINS(j, '[1]'),
JSON_CONTAINS_PATH(j, 'all', '\$.a'),
JSON_LENGTH(j),
JSON_DEPTH(j),
JSON_ARRAY(j, j),
JSON_OBJECT('k', j),
JSON_UNQUOTE(CAST(j AS CHAR)),
JSON_QUOTE(CAST(j AS CHAR))
FROM T_WITH_NULLS
ORDER BY i;
eval $query;
# It should work the same way with a TEXT column as with a JSON column.
ALTER TABLE T_WITH_NULLS MODIFY COLUMN j TEXT;
eval $query;
DROP TABLE T_WITH_NULLS;

# Make sure that every JSON function accepts latin1 text arguments. The JSON
# functions use utf8mb4 internally, so they will need to perform charset
# conversion.
CREATE TABLE t_latin1(id INT PRIMARY KEY AUTO_INCREMENT,
                      json_text VARCHAR(20),
                      json_atom_text VARCHAR(20),
                      json_path VARCHAR(20))
CHARACTER SET 'latin1';
INSERT INTO t_latin1 (json_text, json_atom_text, json_path) VALUES
(CONVERT(X'5B22E6F8E5225D' USING latin1),             # ["\u00e6\u00f8\u00e5"]
 CONVERT(X'E5F8E6' USING latin1),                     # \u00e5\u00f8\u00e6
 '$[0]'),
(CONVERT(X'7B22E6F8E5223A22E6F8E5227D' USING latin1),
                                  # {"\u00e6\u00f8\u00e5":"\u00e6\u00f8\u00e5"}
 CONVERT(X'E5F8E6' USING latin1),                     # \u00e5\u00f8\u00e6
 CONVERT(X'242E22E6F8E522' USING latin1));            # $."\u00e6\u00f8\u00e5"
SELECT * FROM t_latin1 ORDER BY id;
SELECT CAST(json_text AS JSON) FROM t_latin1 ORDER BY id;
SELECT JSON_VALID(json_text) FROM t_latin1 ORDER BY id;
SELECT JSON_VALID(json_atom_text) FROM t_latin1 ORDER BY id;
SELECT JSON_TYPE(json_text) FROM t_latin1 ORDER BY id;
SELECT JSON_EXTRACT(json_text, json_path) FROM t_latin1 ORDER BY id;
SELECT JSON_REMOVE(json_text, json_path) FROM t_latin1 ORDER BY id;
SELECT JSON_ARRAY_APPEND(json_text, json_path, json_atom_text)
FROM t_latin1 ORDER BY id;
SELECT JSON_SET(json_text, json_path, json_atom_text) FROM t_latin1 ORDER BY id;
SELECT JSON_INSERT(json_text, json_path, json_atom_text)
FROM t_latin1 ORDER BY id;
SELECT JSON_REPLACE(json_text, json_path, json_atom_text)
FROM t_latin1 ORDER BY id;
SELECT JSON_MERGE(json_text, json_text) FROM t_latin1 ORDER BY id;
SELECT JSON_SEARCH(json_text, CONVERT('one' USING latin1), json_atom_text,
                  CONVERT(X'F8' USING latin1), json_path)
FROM t_latin1 ORDER BY id;
SELECT JSON_CONTAINS(json_text, json_text, json_path) FROM t_latin1 ORDER BY id;
SELECT JSON_CONTAINS_PATH(json_text, CONVERT('one' USING latin1), json_path)
FROM t_latin1 ORDER BY id;
SELECT JSON_LENGTH(json_text, json_path) FROM t_latin1 ORDER BY id;
SELECT JSON_DEPTH(json_text) FROM t_latin1 ORDER BY id;
SELECT JSON_ARRAY(json_atom_text, json_atom_text) FROM t_latin1 ORDER BY id;
SELECT JSON_OBJECT(json_atom_text, json_atom_text) FROM t_latin1 ORDER BY id;
SELECT JSON_UNQUOTE(json_atom_text) FROM t_latin1 ORDER BY id;
SELECT JSON_UNQUOTE(CONVERT(CONCAT('"', json_atom_text, '"') USING latin1))
FROM t_latin1 ORDER BY id;
SELECT JSON_QUOTE(json_atom_text) FROM t_latin1 ORDER BY id;
DROP TABLE t_latin1;

# examples from the wl7909 spec
# returns the SQL string literal abc
SELECT JSON_UNQUOTE( '"abc"' );

# returns the SQL string literal "abc
SELECT JSON_UNQUOTE( '"abc' );

--error ER_INCORRECT_TYPE
SELECT JSON_UNQUOTE( 123 );

# returns the SQL string literal abc
SELECT JSON_UNQUOTE
( CAST( CAST( '"abc"' AS JSON ) AS CHAR ) );

# returns 1
SELECT JSON_UNQUOTE
(
  CAST(
    JSON_EXTRACT( '{ "userName" : "fred" }', '$.userName' )
    AS CHAR
  )
) = 'fred';

# returns 0
SELECT
  CAST(
    JSON_EXTRACT( '{ "userName" : "fred" }', '$.userName' )
    AS CHAR
  ) = 'fred';

# returns "abc"
SELECT JSON_QUOTE( 'abc' );

--error ER_INCORRECT_TYPE
SELECT JSON_QUOTE( 123 );

# returns the JSON document consisting of the string scalar "123"
SELECT CAST( JSON_QUOTE( '123' ) AS JSON );

--echo # ----------------------------------------------------------------------
--echo # Test that boolean expressions are treated as boolean atom literals
--echo # ----------------------------------------------------------------------

create table t_bool_literals( a int, b varchar(10) );
insert into t_bool_literals values ( 1, 'food' ), ( 2, 'fool' ), ( 3, 'water' );

# expressions built out of logical connectives should evaluate to boolean literals, but they don't
select a, json_array( ((a < 3) and (a > 1)) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', ((a < 3) and (a > 1)) ) from t_bool_literals order by a;

select a, json_array( not ((a < 3) and (a > 1)) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', not ((a < 3) and (a > 1)) ) from t_bool_literals order by a;

select a, json_array( ((a < 3) or (a > 1)) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', ((a < 3) or (a > 1)) ) from t_bool_literals order by a;

select a, json_array( not ((a < 3) or (a > 1)) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', not ((a < 3) or (a > 1)) ) from t_bool_literals order by a;

select json_array( not true, not false );
select json_array_append( '[]', '$', not true, '$', not false );

select a, json_array( 1 and true ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', 1 and true ) from t_bool_literals order by a;

select a, json_array( not 1 ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', not 1 ) from t_bool_literals order by a;

# true and false literals
select json_array( true, false );
select json_array_append( '[]', '$', true, '$', false );

# comparison operators should evaluate to boolean literals
select a, json_array( (a < 3) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', (a < 3) ) from t_bool_literals order by a;

select a, json_array( (a <= 3) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', (a <= 3) ) from t_bool_literals order by a;

select a, json_array( (a > 3) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', (a > 3) ) from t_bool_literals order by a;

select a, json_array( (a >= 3) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', (a >= 3) ) from t_bool_literals order by a;

select a, json_array( (a <> 3) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', (a <> 3) ) from t_bool_literals order by a;

select a, json_array( (a != 3) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', (a != 3) ) from t_bool_literals order by a;

# IS NULL and IS NOT NULL
select a, json_array( a is null ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', a is null ) from t_bool_literals order by a;

select a, json_array( a is not null ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', a is not null ) from t_bool_literals order by a;

# IS TRUE and IS NOT TRUE

select a, json_array( a is true ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', a is true ) from t_bool_literals order by a;

select a, json_array( a is not true ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', a is not true ) from t_bool_literals order by a;

# NULLIF which coalesce booleans should evaluate to boolean literals
select a, json_array(nullif(true, false)  ) from t_bool_literals order by a;
select a, json_array_append
(
  '[]',
  '$',  nullif(true, false)
) from t_bool_literals order by a;

# would be nice if CASE coalesced a boolean type if all branches are boolean. FIXME maybe
#select a, json_array( case when (a > 1) then true else false end ) from t_bool_literals order by a;
#select a, json_array_append
#(
#  '[]',
#  '$', case when (a > 1) then true else false end
#) from t_bool_literals order by a;

# as a workaround, you can always AND problematic expressions with true
select a, json_array( (case when (a > 1) then true else false end) and true ) from t_bool_literals order by a;
select a, json_array_append
(
  '[]',
  '$', (case when (a > 1) then true else false end) and true
) from t_bool_literals order by a;

# between predicates should evaluate to boolean literals
select a, json_array( a between 2 and 4 ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', a between 2 and 4 ) from t_bool_literals order by a;

# in predicates should evaluate to boolean literals
select a, json_array( a in (1,3) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', a in (1,3) ) from t_bool_literals order by a;

# like predicates should evaluate to boolean literals
select a, json_array( b like 'foo%' ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b like 'foo%' ) from t_bool_literals order by a;

# regexp predicates should evaluate to boolean literals
select a, json_array( b REGEXP '^fo+d' ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b REGEXP '^fo+d' ) from t_bool_literals order by a;

select a, json_array( b rlike '^fo+d' ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b rlike '^fo+d' ) from t_bool_literals order by a;

select a, json_array( b not REGEXP '^fo+d' ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b not REGEXP '^fo+d' ) from t_bool_literals order by a;

select a, json_array( b not rlike '^fo+d' ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b not rlike '^fo+d' ) from t_bool_literals order by a;

# quantified comparisons should evaluate to boolean literals
select a, json_array( b = some( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b = some( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b = all( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b = all( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b = any( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b = any( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b > some( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b > some( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b > all( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b > all( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b > any( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b > any( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b < some( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b < some( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b < all( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b < all( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b < any( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b < any( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b <= some( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b <= some( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b <= all( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b <= all( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b <= any( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b <= any( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b >= some( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b >= some( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b >= all( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b >= all( select b from t_bool_literals ) ) from t_bool_literals order by a;

select a, json_array( b >= any( select b from t_bool_literals ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', b >= any( select b from t_bool_literals ) ) from t_bool_literals order by a;

# exists predicates should evaluate to boolean literals
select a, json_array( exists( select b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', exists( select b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;

select a, json_array( not exists( select b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', not exists( select b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;

# json_valid() calls should evaluate to boolean literals
select a, json_array( json_valid( b ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', json_valid( b ) ) from t_bool_literals order by a;

select a, json_array( not json_valid( b ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$', not json_valid( b ) ) from t_bool_literals order by a;

# json_contains_path() calls should evaluate to boolean literals
select json_array( json_contains_path( '{ "a" : { "b" : 100 } }', 'all', '$.a.b' ) );

# gtid_subset() calls should evaluate to boolean literals
select a, json_array( gtid_subset('3E11FA47-71CA-11E1-9E33-C80AA9429562:23', '3E11FA47-71CA-11E1-9E33-C80AA9429562:21-57') )
from t_bool_literals order by a;
select a, json_array_append( '[]', '$', gtid_subset('3E11FA47-71CA-11E1-9E33-C80AA9429562:23', '3E11FA47-71CA-11E1-9E33-C80AA9429562:21-57') )
from t_bool_literals order by a;

select a, json_array( not gtid_subset('3E11FA47-71CA-11E1-9E33-C80AA9429562:23', '3E11FA47-71CA-11E1-9E33-C80AA9429562:21-57') )
from t_bool_literals order by a;
select a, json_array_append( '[]', '$', not gtid_subset('3E11FA47-71CA-11E1-9E33-C80AA9429562:23', '3E11FA47-71CA-11E1-9E33-C80AA9429562:21-57') )
from t_bool_literals order by a;

# comparisons to subqueries should evaluate to boolean literals
select a, json_array( b = ( select distinct b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$',  b = ( select distinct b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;

select a, json_array( b > ( select distinct b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$',  b > ( select distinct b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;

select a, json_array( b >= ( select distinct b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$',  b >= ( select distinct b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;

select a, json_array( b < ( select distinct b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$',  b < ( select distinct b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;

select a, json_array( b <= ( select distinct b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;
select a, json_array_append( '[]', '$',  b <= ( select distinct b from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;

# make sure ordinary subselects still function correctly
select a, json_array( ( select distinct a from t_bool_literals where a = 1 ) ) from t_bool_literals order by a;

drop table t_bool_literals;

--echo # ----------------------------------------------------------------------
--echo # Test of JSON_CONTAINS
--echo # ----------------------------------------------------------------------

--echo # should give NULL
select json_contains(NULL, NULL);
select json_contains(cast('{"a": 1, "b": 2}' as json), NULL);
select json_contains(NULL, cast('null' as json));
select json_contains(cast('[1]' as json), cast('[1]' as json), NULL);

--echo # should give 0:
select json_contains(cast(3.14 as json), cast(3 as json));

--echo # should give 0: not at top level
select json_contains(cast('{"a": {"b": 7}}' as json), cast('{"b": 7}' as json));

--echo # but path argument will fix it:
select json_contains(cast('{"a": {"b": 7}}' as json), cast('{"b": 7}' as json), '$.a');

--echo # but arrays "introspect"
select json_contains(cast('[1,[2.0, 3.0]]' as json), cast('[2.0]' as json));
select json_contains(cast('[1, 2, [3, [4, 5]], 6, 7]' as json), cast('5' as json));

--echo # should give 0: just a key
select json_contains(cast('{"a": 1, "b": 2}' as json), cast('"a"' as json));

--echo # should give 0: one candidate element doesn't match
select json_contains(cast('[1]' as json), cast('[1,2]' as json));

--echo # should all give 1
select json_contains(cast('null' as json), cast('null' as json));
--echo # simple object subset
select json_contains(cast('{"a": 1, "b": 2}' as json), cast( '{"a": 1}' as json));
--echo # simple vector subset
select json_contains(cast('[1, 2, 3]' as json), cast('[1, 3]' as json));
--echo # auto-wrap, should give 1
select json_contains(cast('[1, 2, 3]' as json), cast(3 as json));
--echo # ok even with nested cast off elements
select json_contains(cast('{"person": {"id": 1, "country": "norway"}}' as json),
                    cast('{"person": {"country": "norway"}}' as json));
--echo # vector reordering and duplicates is ok
select json_contains(cast('[1,3,5]' as json), cast('[5,3,1,5]' as json));
--echo # ok even with more elts in candidate than in doc
select json_contains(cast('[{"b": 4, "a":7}]' as json), cast('[{"a":7},{"b":4}]' as json));
select json_contains(cast('[{"b": 4, "a":7}, 5]' as json), cast('[5, {"a":7, "b":4}]' as json));
--echo # ok even with mixed number types that compare equal
select json_contains(cast('[{"b": 4, "a":7}, 5.0]' as json), cast('[5, {"a":7.0E0, "b":4}]' as json));

# Bug discovered by Rick: used to give 1 (true).
select json_contains( '{"customer": "cust3"}', '{"customer": "cust1"}' );

SELECT JSON_CONTAINS('[null,1,[2,3],true,false]', '[null,1,[3],false]');
SELECT JSON_CONTAINS('[null,1,[2,3],true,false]', '[null,1,[4],false]');
SELECT JSON_CONTAINS('[true,false]', '[[true]]');
SELECT JSON_CONTAINS('[1,2]', '[[1]]');
SELECT JSON_CONTAINS('[1,2]', '1', '$.abc');

# examples from the wl7909 spec
# returns 1
SELECT JSON_CONTAINS
(
   CAST('[1, 4, 6]' AS JSON),
   CAST('[1, 6]' AS JSON)
);

# returns 1; even with nested cast off elements
SELECT JSON_CONTAINS
(
   CAST('{"person": {"id": 1, "country": "norway"}}' AS JSON),
   CAST('{"person": {"country": "norway"}}' AS JSON)
);

# returns 1; reordering and duplicates are ok
SELECT JSON_CONTAINS
(
   CAST('[1,3,5]' AS JSON),
   CAST('[5,3,1,5]' AS JSON)
);

# return 0; no type conversion is performed
SELECT JSON_CONTAINS
(
   CAST('[3.14]' AS JSON),
   CAST('[3]' AS JSON)
);

# returns 1, due to auto-wrapping
SELECT JSON_CONTAINS
(
   CAST('[1, 2, 3]' AS JSON),
   CAST(3 AS JSON)
);

--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
SELECT JSON_CONTAINS();
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
SELECT JSON_CONTAINS('[1]');
--error ER_WRONG_PARAMCOUNT_TO_NATIVE_FCT
SELECT JSON_CONTAINS('[1]', '[1]', '$', '$[0]');


--echo # ----------------------------------------------------------------------
--echo # Verify that all of the string types behave similarly when used as ANY_JSON_ATOMS
--echo # ----------------------------------------------------------------------

create table t_char( a int, b char(20) );
insert into t_char values ( 1, 'foo' );

create table t_varchar( a int, b varchar(20) );
insert into t_varchar values ( 1, 'foo' );

Create table t_tinytext( a int, b tinytext );
insert into t_tinytext values ( 1, 'foo' );

create table t_text( a int, b text );
insert into t_text values ( 1, 'foo' );

create table t_mediumtext( a int, b mediumtext );
insert into t_mediumtext values ( 1, 'foo' );

create table t_longtext( a int, b longtext );
insert into t_longtext values ( 1, 'foo' );

# treated as a string. evaluates to ["foo"]
select json_array( b ) from t_char;
select json_array( b ) from t_varchar;
select json_array( b ) from t_tinytext;
select json_array( b ) from t_text;
select json_array( b ) from t_mediumtext;
select json_array( b ) from t_longtext;

# casts to CHAR should still be strings
select json_array( cast( b as char ) ) from t_char;
select json_array( cast( b as char ) ) from t_varchar;
select json_array( cast( b as char ) ) from t_tinytext;
select json_array( cast( b as char ) ) from t_text;
select json_array( cast( b as char ) ) from t_mediumtext;
select json_array( cast( b as char ) ) from t_longtext;

# string-valued XML functions should behave as strings when used as ANY_JSON_ATOMs
select json_array( UpdateXML('<a><b>ccc</b><d></d></a>', '/a/d', '<e>fff</e>') );
select json_array( cast( UpdateXML('<a><b>ccc</b><d></d></a>', '/a/d', '<e>fff</e>') as char ) );
select json_array( ExtractValue('<r><n id="1">v1</n><n id="2">v2</n></r>','//n[@id=1]' ) );
select json_array( cast( ExtractValue('<r><n id="1">v1</n><n id="2">v2</n></r>','//n[@id=1]' ) as char ) );

drop table t_char;
drop table t_varchar;
drop table t_tinytext;
drop table t_text;
drop table t_mediumtext;
drop table t_longtext;

--echo # ----------------------------------------------------------------------
--echo # Check that JSON values stemming from views and derived tables work
--echo # ----------------------------------------------------------------------
create table t(x int);
insert into t values (NULL), (4);
select json_array(x) from (select x from t) tt order by x;
create view v as select * from t;
select json_array(x) from v order by x;

drop view v;
drop table t;

--echo # ----------------------------------------------------------------------
--echo # Ignore collation.collation when handing off val_str to a JSON field -
--echo # bug found by John E.
--echo # ----------------------------------------------------------------------
create table t3( col_json json );
insert into t3(col_json) values ( json_quote( '1' ) );
select * from t3;
select json_type(col_json) from t3;

--echo # ----------------------------------------------------------------------
--echo # Wrong collation from JSON_QUOTE caused problems: Set it in
--echo # Item_func_json_quote::fix_length_and_dec.   Bug found by Knut.
--echo # Similar issue for JSON_UNQUOTE and JSON_TYPE.
--echo # ----------------------------------------------------------------------
select json_object("a", ifnull(json_quote('test'), cast('null' as json)));
select cast(concat('[', json_quote('ab'), ']') as json);
select cast(concat('[', json_unquote('"12"'), ']') as json);
select cast(concat('["', json_type(cast(1 as json)), '"]') as json);

drop table t3;

--echo # ----------------------------------------------------------------------
--echo # Correctly escape key names when pretty-printing JSON objects.
--echo # Correct behavior means that the strings can be re-used for
--echo # their original purposes as key names and paths.
--echo # ----------------------------------------------------------------------

create table jep( key_col int primary key, doc json, path varchar( 50 ) );
insert into jep values
( 1, '{ "one \\"potato": "seven"  }', '$."one \\"potato"' ),
( 2, '{ "one \\npotato": "seven"  }', '$."one \\npotato"' ),
( 3, '{ "one \\tpotato": "seven"  }', '$."one \\tpotato"' ),
( 4, '{ "one \\bpotato": "seven"  }', '$."one \\bpotato"' ),
( 5, '{ "one \\fpotato": "seven"  }', '$."one \\fpotato"' ),
( 6, '{ "one \\rpotato": "seven"  }', '$."one \\rpotato"' ),
( 7, '{ "one \\\\potato": "seven"  }', '$."one \\\\potato"' );

insert into jep select key_col + 100, cast( doc as char ), path from jep;

select key_col, doc, json_keys( doc ) from jep order by key_col;

select key_col, doc, json_extract( doc, cast(path as char) ) from jep order by key_col;

select * from jep order by key_col;

drop table jep;

--echo # ----------------------------------------------------------------------
--echo # Test that cached, constant path objects are restored
--echo # after the leg popping which happens inside json_insert()
--echo # and json_replace().
--echo # ----------------------------------------------------------------------

create table t_cache( id int, doc json );

insert into t_cache values
( 1, '{ "a": { "b": 1 } }' ),
( 2, '{ "a": { "c": 1 } }' ),
( 3, '{ "a": { "d": 1 } }' );

select id, doc, json_insert( doc, '$.a.c', 2 ) from t_cache order by id;
select id, doc, json_insert( doc, '$.a.c', 2, '$.a.d', 3 ) from t_cache order by id;

delete from t_cache;

insert into t_cache values
( 1, '{ "a": { "b": 1, "c": 2, "d": 3 } }' ),
( 2, '{ "a": { "c": 2, "d": 3 } }' ),
( 3, '{ "a": { "b": 1, "d": 3 } }' ),
( 4, '{ "a": { "b": 1, "c": 2 } }' ),
( 5, '{ "a": { "b": 1 } }' ),
( 6, '{ "a": { "c": 2 } }' ),
( 7, '{ "a": { "d": 3 } }' ),
( 8, '{ "a": {} }' );

select id, doc, json_replace( doc, '$.a.c', 20 ) from t_cache order by id;
select id, doc, json_replace( doc, '$.a.c', 20, '$.a.d', 30 ) from t_cache order by id;

drop table t_cache;

--echo # ----------------------------------------------------------------------
--echo # Test that one_or_all arguments are cached correctly.
--echo # ----------------------------------------------------------------------

create table t_ooa( id int, doc json, one_or_all varchar(10) );

insert into t_ooa values
( 1, '{ "a": 1, "b": 2, "c": 3 }', 'one' ),
( 2, '{ "d": 4 }', 'one' ),
( 3, '{ "a": 1, "b": 2, "d": 4 }', 'all' ),
( 4, '{ "a": 1, "c": 3 }', 'all' ),
( 5, '{ "d": 4 }', 'all' ),
( 6, '{ "a": 1, "b": 2, "c": 3 }', null );

select id, doc, one_or_all, json_contains_path( doc, one_or_all, '$.a', '$.b' ) from t_ooa order by id;
select id, doc, json_contains_path( doc, 'one', '$.a', '$.b' ) from t_ooa order by id;
select id, doc, json_contains_path( doc, 'all', '$.a', '$.b' ) from t_ooa order by id;
select id, doc, json_contains_path( doc, null, '$.a', '$.b' ) from t_ooa order by id;

delete from t_ooa;

insert into t_ooa values
( 1, '{ "a": "foot", "b": "fool", "c": "food" }', 'one' ),
( 1, '{ "a": "foot", "b": "fool", "c": "food" }', 'all' ),
( 1, '{ "a": "foot", "b": "fool", "c": "food" }', null );

select id, doc, one_or_all, json_search( doc, one_or_all, 'foo%' ) from t_ooa order by id;
select id, doc, json_search( doc, 'one', 'foo%' ) from t_ooa order by id;
select id, doc, json_search( doc, 'all', 'foo%' ) from t_ooa order by id;
select id, doc, json_search( doc, null, 'foo%' ) from t_ooa order by id;

drop table t_ooa;

# This test case reproduces a problem seen during development. The update
# statement crashed if the target table was the inner table of the join.
CREATE TABLE t1(j JSON);
CREATE TABLE t2(j JSON);
INSERT INTO t1 VALUES ('[1]'), ('[2]'), ('[3]'), ('[4]');
INSERT INTO t2 VALUES ('[1]');
ANALYZE TABLE t1, t2;
let $query=
UPDATE t1, t2 SET t1.j = JSON_INSERT(t2.j, '\$[1]', t2.j) WHERE t1.j=t2.j;
eval EXPLAIN $query;
eval $query;
SELECT * FROM t1 ORDER BY (CAST(j AS CHAR));
DROP TABLE t1, t2;

--echo #
--echo # Bug#20888919: ASSERT `!THD->IS_ERROR()' FAILED IN HANDLE_QUERY()
--echo #               ON EXPLAIN SELECT JSON
--echo #
create table t (pk int primary key, col_json json);
ANALYZE TABLE t;
explain SELECT col_json FROM t WHERE pk = 1;
drop table t;

--echo #
--echo # Bug#20912438: ITEM_TYPE_HOLDER::DISPLAY_LENGTH(ITEM*): ASSERTION `0' FAILED
--echo #
(SELECT JSON_KEYS('{ "key80": "2015-04-20 11:53:55"}')) UNION ALL
(SELECT JSON_KEYS('{ "key80": "2015-04-20 11:53:55" }') LIMIT 0);
SELECT CAST(1 AS JSON) UNION ALL SELECT CAST(1 AS JSON);

--echo # ----------------------------------------------------------------------
--echo # Bug#20889248 Used to crash the server
--echo # ----------------------------------------------------------------------
create table tt(i int, j json, si int);
select count(*) , json_keys('{"key17": {"a": {"b": "c"}}, "key88": "value94"}');

# Tests Item_copy_json::save_in_field. int target column here gets assigned via
# JSON->string->int parse since Field_long::store doesn't have an overload for
# JSON. Similar for other non-JSON target columns. The JSON column assignment
# does not go via string, since Field_json knows how to store JSON.
insert into tt(i, j)
  select count(*), json_extract('{"key17": {"a": {"b": "c"}}, "key88": 100}',
                               '$.key88');
insert into tt(i, si)
  select count(*), json_extract('{"key17": {"a": {"b": "c"}}, "key88": 100}',
                               '$.key88');
select * from tt;

# Exercise NULL handling and error handling in Item_copy_json::copy().
SELECT COUNT(*), CAST(NULL AS JSON);
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT COUNT(*), JSON_EXTRACT('not valid json!', '$');

# This exercises Item_copy_json::val_real
delete from tt;
insert into tt(j) values (cast(1 as json)), (null);
select sum( distinct j ) from tt group by j having j in ( avg( 1 ), 1 + j);

# Exercise Item_copy_json::val_json
SELECT JSON_ARRAY(j), COUNT(*) FROM tt GROUP BY j, i WITH ROLLUP;

# Exercise Item_copy_json::val_int
SELECT REPEAT('abc', j), COUNT(*) FROM tt GROUP BY j, i WITH ROLLUP;

# Exercise Item_copy_json::val_str
SELECT REPEAT(j, 2), COUNT(*) FROM tt GROUP BY j, i WITH ROLLUP;

# Exercise Item_copy_json::val_decimal
SELECT CAST(j AS DECIMAL(5,2)), COUNT(*) FROM tt GROUP BY j, i WITH ROLLUP;

# Exercise Item_copy_json::get_time
UPDATE tt SET j = CAST(CAST('12:13:14' AS TIME) AS JSON) WHERE j IS NOT NULL;
SELECT CAST(j AS TIME), COUNT(*) FROM tt GROUP BY j, i WITH ROLLUP;

# Exercise Item_copy_json::get_date
SELECT CAST(j AS DATE) = CURRENT_DATE, COUNT(*) FROM tt
GROUP BY j, i WITH ROLLUP;
UPDATE tt SET j = CAST(CAST('2015-06-19' AS DATE) AS JSON) WHERE j IS NOT NULL;
SELECT CAST(j AS DATE), COUNT(*) FROM tt GROUP BY j, i WITH ROLLUP;

# Exercise an error path through Item_copy_json::val_str
DELETE FROM tt;
INSERT INTO tt(j) VALUES (JSON_ARRAY(REPEAT('abc', 100)));
UPDATE tt SET j = JSON_ARRAY(j,j,j,j);
SET GLOBAL net_buffer_length = 1024;
SET GLOBAL max_allowed_packet = 1024;
CONNECT (con1,localhost,root,,);
CONNECTION con1;
SELECT REPEAT(j, 2), COUNT(*) FROM tt GROUP BY j, i WITH ROLLUP;
CONNECTION default;
DISCONNECT con1;
SET GLOBAL max_allowed_packet = default;
SET GLOBAL net_buffer_length = default;

DROP TABLE tt;

--echo # ----------------------------------------------------------------------
--echo # Bug#20914054 Used to crash the server
--echo # ----------------------------------------------------------------------
CREATE TABLE t1 (
  pk INT NOT NULL,
  col_int_key INT,
  col_json json,
  PRIMARY KEY (pk),
  KEY col_int_key (col_int_key)
);

INSERT INTO t1 VALUES (8, 4, '{}');

CREATE TABLE t2 (
  pk INT NOT NULL,
  PRIMARY KEY (pk)
);

INSERT INTO t2 VALUES (20);

SELECT MIN(JSON_KEYS( t1.col_json )) AS field1
FROM t1 JOIN t2
HAVING field1 = 7;

drop table t1;
drop table t2;

--echo # ----------------------------------------------------------------------
--echo # Bug#20920788 Used to give SQL state 22032: Cannot create a JSON value
--echo # from a string with CHARACTER SET 'binary'.
--echo #----------------------------------------------------------------------

CREATE TABLE t (
  col_json JSON,
  col_varchar VARCHAR(1),
  col_varchar_key VARCHAR(1),
  KEY col_varchar_key (col_varchar_key)
);

INSERT INTO t VALUES ('{}', 'a', 'a');

--echo # This always succeeded, group by column is indexed, optimizer does not
--echo # use filesort:
SELECT MAX(col_json) AS field1, col_varchar_key AS field2 FROM t GROUP BY field2;

--echo # This used to fail, group by column is not indexed, EXPLAIN says
--echo # filesort is used:
SELECT MAX(col_json) AS field1, col_varchar AS field2 FROM t GROUP BY field2;

drop table t;

--echo # ----------------------------------------------------------------------
--echo # Bug  report from John E.
--echo # Crash in Item_copy_json::~Item_copy_json
--echo # ----------------------------------------------------------------------
EXPLAIN SELECT COUNT(*), JSON_KEYS('{}');

--echo # ----------------------------------------------------------------------
--echo # Bug#20962317 WARNING 3150 'INVALID JSON VALUE FOR CAST TO INTEGER' ON
--echo #              SUBQUERY IN JSON_VALID
--echo #----------------------------------------------------------------------
create table myt(col_json json);
insert into myt values ('{}');
--echo # This statement used to give two wrong warnings
select json_valid((select col_json from myt));
drop table myt;

--echo # ----------------------------------------------------------------------
--echo # Bug#20954309 JSON_SEARCH() IN VIEWS DOES NOT WORK, ALWAYS RETURNS NULL
--echo #----------------------------------------------------------------------

CREATE TABLE t_20954309 (id int, col_json JSON);
INSERT INTO t_20954309 VALUES
  (2, '{"keyA": "eleven"}');
CREATE VIEW v1_20954309 AS SELECT id, JSON_SEARCH(col_json, 'one', 'ele%' ) FROM t_20954309;

CREATE VIEW v2_20954309 AS SELECT id, col_json FROM t_20954309;


SELECT id, JSON_SEARCH(col_json, 'one', 'ele%' ) from t_20954309 order by id;
SELECT id, JSON_SEARCH(col_json, 'one', 'eleven' ) from v2_20954309 order by id;

SELECT * FROM v1_20954309 order by id;

select json_search( '{ "a": "foo" }', 'one', 'foo', 'a' );
select json_search( '{ "a": "foo" }', 'one', 'foo', null );
select json_search( '{ "a": "foo" }', 'one', 'foo', convert(x'f8' using latin1) );

# bad escape arg
--error ER_WRONG_ARGUMENTS
select json_search( '{ "a": "foo" }', 'one', 'foo', 'ab' );

drop view v1_20954309;
drop view v2_20954309;
drop table t_20954309;

#
# Arguments vary from row to row.
#
create table t_20954309 (id int, doc JSON, search_string varchar(20), escape_char varchar(10) );
insert into t_20954309 values
  (1, '{"match11": "eleven", "match12": "element", "notMatch": "elven" }', 'ele%', null ),
  (2, '{"match21": "eleven", "match22": "element", "notMatch": "elven" }', 'ele%', 'z' ),
  (3, '{"match31": "tw%elve", "match32": "tw%ilight", "notMatch": "twitter" }', 'tw|%%', '|' );

select id, json_search( doc, 'all', search_string, '|' ) from t_20954309 order by id;

create view v_20954309 as select id, json_search( doc, 'all', search_string, '|' ) from t_20954309 order by id;
select * from v_20954309;

select id, json_search( doc, 'all', search_string, null ) from t_20954309 where id < 3 order by id;
create view v2_20954309 as select id, json_search( doc, 'all', search_string, null ) result from t_20954309 where id < 3 order by id;
select * from v2_20954309;

drop view v_20954309;
drop view v2_20954309;
drop table t_20954309;

create table t_doc (id int, doc JSON );
insert into t_doc values
  (1, '{"match11": "eleven", "match12": "element", "notMatch": "elven" }' ),
  (2, '{"match21": "eleven", "match22": "element", "notMatch": "elven" }' ),
  (3, '{"match31": "tw%elve", "match32": "tw%ilight", "notMatch": "twitter" }' );

create table t_search_string (id int, search_string varchar(20) );
insert into t_search_string values
  (1, 'ele%' ),
  (2, 'ele%' ),
  (3, 'tw|%%' );

select t.id, json_search( doc, 'all', (select search_string from t_search_string s where s.id = t.id), '|' )
from t_doc t order by id;

create view v_doc as
select t.id, json_search( doc, 'all', (select search_string from t_search_string s where s.id = t.id), '|' )
from t_doc t order by id;

select * from v_doc;

drop view v_doc;
drop table t_doc;
drop table t_search_string;

set names default;

--echo # ----------------------------------------------------------------------
--echo # Wrong results when Json_path_cache primed is accessed
--echo # during the prepare-phase.
--echo #----------------------------------------------------------------------

select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ]', null ) is null;

prepare stmt1 from 'select json_remove( ''[ 1, { "a": true, "b": false, "c": null }, 5 ]'', null ) is null';
execute stmt1;

--error ER_INVALID_JSON_PATH
select json_remove( '[ 1, { "a": true, "b": false, "c": null }, 5 ]', '$.' ) is null;

--error ER_INVALID_JSON_PATH
prepare stmt1 from 'select json_remove( ''[ 1, { "a": true, "b": false, "c": null }, 5 ]'', ''$.'' ) is null';

-- echo #
-- echo # Bug#20972793 ASSERT FIELD_TYPE() == MYSQL_TYPE_JSON...
-- echo #              IN ARG_COMPARATOR::COMPARE_JSON
-- echo #
CREATE TABLE t1 (
  pk INT NOT NULL,
  col_int_key INT,
  col_int INT,
  col_json JSON,
  PRIMARY KEY (pk),
  KEY col_int_key (col_int_key)
) ENGINE=InnoDB;
INSERT INTO t1 VALUES (2,4,2,NULL);
CREATE TABLE t2 (
  pk INT NOT NULL,
  col_int_key INT,
  PRIMARY KEY (pk),
  KEY col_int_key (col_int_key)
) ENGINE=InnoDB;
SELECT
  (SELECT MAX(sq1_alias1.pk) AS sq1_field1
   FROM (t1 AS sq1_alias1
     INNER JOIN t2 AS sq1_alias2
     ON (sq1_alias2.col_int_key = sq1_alias1.col_int_key)
   )
   WHERE sq1_alias2.pk <= alias1.col_int
  ) AS field1,
  MAX(alias1.col_json) AS field2
FROM (
  SELECT sq2_alias1.*
  FROM t1 AS sq2_alias1
) AS alias1
GROUP BY field1
HAVING field2 > 1;
DROP TABLE t1, t2;

--echo # ----------------------------------------------------------------------
--echo # Bug#20987329 VALUE OF PREPARED STATEMENT PLACEHOLDER FOR PARAMETER
--echo #              IN JSON_EXTRACT IS STICKY
--echo #----------------------------------------------------------------------

# should get different results with different parameter values

# json_extract()

CREATE TABLE t_reuse (pk INT, col_json JSON);
INSERT INTO t_reuse VALUES (1, '{"keyA": 1}'), (2, '{"keyA": 2, "keyB": 22}');

PREPARE getjson FROM 'SELECT JSON_EXTRACT(col_json, ?) FROM t_reuse';
SET @mypath = '$.keyA';
EXECUTE getjson USING @mypath;
SET @mypath = '$.keyB';
EXECUTE getjson USING @mypath;

# json_contains()

prepare json_stmt1 FROM 'select json_contains( ''{ "keyA": [1, 2, 3], "keyB": [4, 5, 6] }'', ''[2]'', ? )';
set @mypath = '$.keyA';
execute json_stmt1 USING @mypath;
set @mypath = '$.keyB';
execute json_stmt1 USING @mypath;

# json_contains_path()

prepare json_stmt2 FROM 'select json_contains_path( ''{ "keyA": [1, 2, 3] }'', ''all'', ? )';
set @mypath = '$.keyA';
execute json_stmt2 USING @mypath;
set @mypath = '$.keyB';
execute json_stmt2 USING @mypath;

# json_length()

prepare json_stmt3 FROM 'select json_length( ''{ "keyA": [1, 2, 3], "keyB": [1, 2, 3, 4] }'', ? )';
set @mypath = '$.keyA';
execute json_stmt3 USING @mypath;
set @mypath = '$.keyB';
execute json_stmt3 USING @mypath;

# json_keys()

prepare json_stmt4 FROM 'select json_keys( ''[ { "keyA": true }, { "keyB": false } ]'', ? )';
set @mypath = '$[0]';
execute json_stmt4 USING @mypath;
set @mypath = '$[1]';
execute json_stmt4 USING @mypath;

# json_array_append()

prepare json_stmt5 FROM 'select json_array_append( ''{ "keyA": [1, 2], "keyB": [3, 4] }'', ?, 5 )';
set @mypath = '$.keyA';
execute json_stmt5 USING @mypath;
set @mypath = '$.keyB';
execute json_stmt5 USING @mypath;

# json_insert()

prepare json_stmt6 FROM 'select json_insert( ''{ "keyA": [1, 2], "keyB": [3, 4] }'', ?, 5 )';
set @mypath = '$.keyA[2]';
execute json_stmt6 USING @mypath;
set @mypath = '$.keyB[2]';
execute json_stmt6 USING @mypath;

# json_set()

prepare json_stmt7 FROM 'select json_set( ''{ "keyA": [1, 2], "keyB": [3, 4] }'', ?, 5 )';
set @mypath = '$.keyA[2]';
execute json_stmt7 USING @mypath;
set @mypath = '$.keyB[2]';
execute json_stmt7 USING @mypath;

# json_replace()

prepare json_stmt8 FROM 'select json_replace( ''{ "keyA": [1, 2], "keyB": [3, 4] }'', ?, 5 )';
set @mypath = '$.keyA[1]';
execute json_stmt8 USING @mypath;
set @mypath = '$.keyB[1]';
execute json_stmt8 USING @mypath;

# json_search()

prepare json_stmt9 FROM 'select json_search( ''{ "keyA": [ "foot" ], "keyB": [ "food" ] }'', ''all'', ''foo%'', null, ? )';
set @mypath = '$.keyA';
execute json_stmt9 USING @mypath;
set @mypath = '$.keyB';
execute json_stmt9 USING @mypath;

# json_remove()

prepare json_stmt10 FROM 'select json_remove( ''{ "keyA": [ "foot" ], "keyB": [ "food" ] }'', ? )';
set @mypath = '$.keyA';
execute json_stmt10 USING @mypath;
set @mypath = '$.keyB';
execute json_stmt10 USING @mypath;

# similar caching problem for the oneOrAll args

prepare json_stmt11 FROM 'select json_contains_path( ''{ "keyA": true }'', ?, ''$.keyA'', ''$.keyB'' )';
set @mypath = 'one';
execute json_stmt11 USING @mypath;
set @mypath = 'all';
execute json_stmt11 USING @mypath;

prepare json_stmt12 FROM 'select json_search( ''{ "keyA": [ "foot" ], "keyB": [ "food" ] }'', ?, ''foo%'' )';
set @mypath = 'one';
execute json_stmt12 USING @mypath;
set @mypath = 'all';
execute json_stmt12 USING @mypath;

drop table t_reuse;

--echo #
--echo # Test that max_allowed_packet is respected.
--echo #
SET GLOBAL net_buffer_length = 1024;
SET GLOBAL max_allowed_packet = 1024;
CONNECT (con1,localhost,root,,);
CONNECTION con1;
CREATE TABLE t1(j JSON);
INSERT INTO t1 VALUES (JSON_ARRAY(REPEAT('abc', 100)));
SELECT JSON_ARRAY(j, j, j, j) FROM t1;
--error ER_WARN_ALLOWED_PACKET_OVERFLOWED
UPDATE t1 SET j = JSON_ARRAY(j, j, j, j);
CREATE TABLE t2(s TEXT);
--error ER_WARN_ALLOWED_PACKET_OVERFLOWED
INSERT INTO t2 SELECT JSON_ARRAY(j, j, j, j) FROM t1;
SELECT * FROM t2;
INSERT INTO t2 SELECT * FROM t1;
--error ER_WARN_ALLOWED_PACKET_OVERFLOWED
UPDATE t2 SET s = JSON_ARRAY(s, s, s, s);
DROP TABLE t1, t2;
CONNECTION default;
DISCONNECT con1;
SET GLOBAL max_allowed_packet = default;
SET GLOBAL net_buffer_length = default;

--echo #
--echo # Test that very deep documents are rejected.
--echo #

# Currently, the maximum accepted depth is 100. Make some documents that are
# nested to that exact depth.
CREATE TABLE t(jarray JSON, jobject JSON, jmix JSON) ROW_FORMAT=DYNAMIC;
INSERT INTO t VALUES ('1', '1', '1');
let $depth=1;
while ($depth < 100)
{
  eval UPDATE t SET jarray  = JSON_ARRAY(jarray),
                    jobject = JSON_OBJECT('a', jobject),
                    jmix    = CASE WHEN MOD($depth, 2) = 0
                                   THEN JSON_ARRAY(jmix)
                                   ELSE JSON_OBJECT('a', jmix)
                                   END;
  inc $depth;
}
SELECT JSON_DEPTH(jarray), JSON_DEPTH(jobject), JSON_DEPTH(jmix) FROM t;

SELECT JSON_DEPTH(CAST(jarray AS CHAR)),
       JSON_DEPTH(CAST(jobject AS CHAR)),
       JSON_DEPTH(CAST(jmix AS CHAR)) FROM t;

SELECT JSON_VALID(jarray), JSON_VALID(jobject), JSON_VALID(jmix) FROM t;

SELECT JSON_VALID(CAST(jarray AS CHAR)),
       JSON_VALID(CAST(jobject AS CHAR)),
       JSON_VALID(CAST(jmix AS CHAR)) FROM t;

--error ER_JSON_DOCUMENT_TOO_DEEP
INSERT INTO t(jarray) SELECT JSON_ARRAY(jarray) FROM t;
--error ER_JSON_DOCUMENT_TOO_DEEP
INSERT INTO t(jobject) SELECT JSON_OBJECT('a', jobject) FROM t;
--error ER_JSON_DOCUMENT_TOO_DEEP
INSERT INTO t(jmix) SELECT JSON_ARRAY(jmix) FROM t;
--error ER_JSON_DOCUMENT_TOO_DEEP
INSERT INTO t(jmix) SELECT JSON_OBJECT('a', jmix) FROM t;

CREATE TABLE too_deep_docs(id INT PRIMARY KEY AUTO_INCREMENT, x text);
INSERT INTO too_deep_docs(x) SELECT CONCAT('[', jarray, ']') FROM t;
INSERT INTO too_deep_docs(x) SELECT CONCAT('[', jobject, ']') FROM t;
INSERT INTO too_deep_docs(x) SELECT CONCAT('[', jmix, ']') FROM t;
INSERT INTO too_deep_docs(x) SELECT CONCAT('{"a":', jarray, '}') FROM t;
INSERT INTO too_deep_docs(x) SELECT CONCAT('{"a":', jobject, '}') FROM t;
INSERT INTO too_deep_docs(x) SELECT CONCAT('{"a":', jmix, '}') FROM t;
INSERT INTO too_deep_docs(x) SELECT CONCAT('["abc", [', jarray, ']]') FROM t;
INSERT INTO too_deep_docs(x)
  SELECT CONCAT('{"a":2,"b":{"c":', jobject, '}}') FROM t;

let $count= `SELECT COUNT(*) FROM too_deep_docs`;
while ($count)
{
  --error ER_JSON_DOCUMENT_TOO_DEEP
  eval SELECT CAST(x AS JSON) FROM too_deep_docs WHERE id=$count;
  --error ER_JSON_DOCUMENT_TOO_DEEP
  eval SELECT JSON_DEPTH(x) FROM too_deep_docs WHERE id=$count;
  --error ER_JSON_DOCUMENT_TOO_DEEP
  eval SELECT JSON_VALID(x) FROM too_deep_docs WHERE id=$count;
  --error ER_JSON_DOCUMENT_TOO_DEEP
  eval INSERT INTO t(jarray) SELECT x FROM too_deep_docs WHERE id=$count;
  dec $count;
}

--error ER_JSON_DOCUMENT_TOO_DEEP
SELECT CAST(JSON_ARRAY(jarray) AS CHAR) FROM t;
--error ER_JSON_DOCUMENT_TOO_DEEP
SELECT CAST(JSON_OBJECT('a', jobject) AS CHAR) FROM t;
--error ER_JSON_DOCUMENT_TOO_DEEP
SELECT CAST(JSON_ARRAY(jmix) AS CHAR) FROM t;
--error ER_JSON_DOCUMENT_TOO_DEEP
SELECT CAST(JSON_OBJECT('a', jmix) AS CHAR) FROM t;

DROP TABLE t, too_deep_docs;

--echo #
--echo # Bug#21054252 QUERY HAVING SQL_BIG_RESULT ON JSON DATA GIVES INVALID
--echo #              DATA ERROR
--echo #
CREATE TABLE t1 (i1 INT, KEY(i1)) ENGINE=innodb;
CREATE TABLE t2 (i2 INT, j2 JSON) ENGINE=innodb;
INSERT INTO t2 (i2, j2) VALUES
(1, '["a"]'),
(2, '["ab"]');
--source include/turn_off_only_full_group_by.inc
SELECT SQL_BIG_RESULT i1, j2
FROM t2 LEFT JOIN t1 ON i2 < i1 GROUP BY j2 ORDER BY i2;
--source include/restore_sql_mode_after_turn_off_only_full_group_by.inc
DROP TABLE t1, t2;

--echo #
--echo # Bug#21104470 WL8132:ASSERTION `! IS_SET()' FAILED.
--echo #

# abbreviated test case

CREATE TABLE t_21104470(j JSON);
INSERT INTO t_21104470 VALUES (NULL), (NULL);
# Should return one row with the value NULL.
SELECT j FROM t_21104470 GROUP BY j;
# This one should return NULL too
SELECT DISTINCT j FROM t_21104470;

# original test case in the bug report

CREATE TABLE D_21104470 (
  pk int(11) NOT NULL AUTO_INCREMENT,
  col_int_nokey json NOT NULL,
  col_int_key int(11) NOT NULL,
  col_date_key date NOT NULL,
  col_date_nokey json NOT NULL,
  col_time_key time NOT NULL,
  col_time_nokey json NOT NULL,
  col_datetime_key datetime NOT NULL,
  col_datetime_nokey json NOT NULL,
  col_varchar_key varchar(1) NOT NULL,
  col_varchar_nokey json NOT NULL,
  PRIMARY KEY (pk),
  KEY col_int_key (col_int_key),
  KEY col_date_key (col_date_key),
  KEY col_time_key (col_time_key),
  KEY col_datetime_key (col_datetime_key),
  KEY col_varchar_key (col_varchar_key),
  KEY col_int_key_2 (col_int_key,col_date_key,col_time_key,col_datetime_key,col_varchar_key)
) ENGINE=MyISAM AUTO_INCREMENT=11 DEFAULT CHARSET=latin1;
INSERT INTO D_21104470
VALUES
(1,'{"int1": "6"}',3,
  '2007-06-18','{"date": "2007-06-18"}',
  '00:00:00','{"time": null}',
  '2002-08-20 22:48:00','{"datetime": "2002-08-20 22:48:00.035785"}',
  'd','{"varc": "d"}'),
(2,'{"int1": "2"}',8,
  '2002-10-13','{"date": "2002-10-13"}',
  '00:00:00','{"time": "00:00:00"}',
  '1900-01-01 00:00:00','{"datetime": "1900-01-01 00:00:00"}',
  's','{"varc": "s"}'),
(3,'{"int1": "4"}',1,
  '1900-01-01','{"date": "1900-01-01"}',
  '15:57:25','{"time": "15:57:25.019666"}',
  '2005-08-15 00:00:00','{"datetime": "2005-08-15 00:00:00"}',
  'r','{"varc": "r"}'),
(4,'{"int1": "8"}',8,
  '0000-00-00','{"date": null}',
  '07:05:51','{"time": "07:05:51.006712"}',
  '1900-01-01 00:00:00','{"datetime": "1900-01-01 00:00:00"}',
  'm','{"varc": "m"}'),
(5,'{"int1": "4"}',8,
  '2006-03-09','{"date": "2006-03-09"}',
  '19:22:21','{"time": "19:22:21.057406"}',
  '2008-05-16 08:09:06','{"datetime": "2008-05-16 08:09:06.002924"}',
  'b','{"varc": "b"}'),
(6,'{"int1": "4"}',5,
  '2001-06-05','{"date": "2001-06-05"}',
  '03:53:16','{"time": "03:53:16.001370"}',
  '2001-01-20 12:47:23','{"datetime": "2001-01-20 12:47:23.022022"}',
  'x','{"varc": "x"}'),
(7,'{"int1": "7"}',7,
  '2006-05-28','{"date": "2006-05-28"}',
  '09:16:38','{"time": "09:16:38.034570"}',
  '2008-07-02 00:00:00','{"datetime": "2008-07-02 00:00:00"}',
  'g','{"varc": "g"}'),
(8,'{"int1": "4"}',5,
  '2001-04-19','{"date": "2001-04-19"}',
  '15:37:26','{"time": "15:37:26.028315"}',
  '1900-01-01 00:00:00','{"datetime": "1900-01-01 00:00:00"}',
  'p','{"varc": "p"}'),
(9,'{"int1": "1"}',1,
  '1900-01-01','{"date": "1900-01-01"}',
  '00:00:00','{"time": "00:00:00"}',
  '2002-12-08 11:34:58','{"datetime": "2002-12-08 11:34:58.001571"}',
  'q','{"varc": "q"}'),
(10,'{"int1": "9"}',6,
  '2004-08-20','{"date": "2004-08-20"}',
  '05:03:03','{"time": "05:03:03.047452"}',
  '1900-01-01 00:00:00','{"datetime": "1900-01-01 00:00:00"}',
  'w','{"varc": "w"}');

CREATE TABLE DD_21104470 (
  pk int(11) NOT NULL AUTO_INCREMENT,
  col_int_nokey json NOT NULL,
  col_int_key int(11) NOT NULL,
  col_date_key date NOT NULL,
  col_date_nokey json NOT NULL,
  col_time_key time NOT NULL,
  col_time_nokey json NOT NULL,
  col_datetime_key datetime NOT NULL,
  col_datetime_nokey json NOT NULL,
  col_varchar_key varchar(1) NOT NULL,
  col_varchar_nokey json NOT NULL,
  PRIMARY KEY (pk),
  KEY col_int_key (col_int_key),
  KEY col_date_key (col_date_key),
  KEY col_time_key (col_time_key),
  KEY col_datetime_key (col_datetime_key),
  KEY col_varchar_key (col_varchar_key),
  KEY col_int_key_2 (col_int_key,col_date_key,col_time_key,col_datetime_key,col_varchar_key)
) ENGINE=MyISAM AUTO_INCREMENT=20 DEFAULT CHARSET=latin1;
INSERT INTO DD_21104470
VALUES
(10,'{"int1": "6"}',3,
  '2002-12-16','{"date": "2002-12-16"}',
  '17:54:20','{"time": "17:54:20.050299"}',
  '2007-04-06 06:20:37','{"datetime": "2007-04-06 06:20:37.035492"}',
  'i','{"varc": "i"}'),
(11,'{"int1": "1"}',1,
  '2005-08-08','{"date": "2005-08-08"}',
  '08:53:47','{"time": "08:53:47.064602"}',
  '2008-03-07 00:00:00','{"datetime": "2008-03-07 00:00:00"}',
  'x','{"varc": "x"}'),
(12,'{"int1": "5"}',2,
  '2004-11-16','{"date": "2004-11-16"}',
  '01:40:28','{"time": "01:40:28.015719"}',
  '2002-01-25 10:15:08','{"datetime": "2002-01-25 10:15:08.014682"}',
  'l','{"varc": "l"}'),
(13,'{"int1": "6"}',7,
  '0000-00-00','{"date": null}',
  '16:37:21','{"time": "16:37:21.055310"}',
  '2001-02-26 23:35:13','{"datetime": "2001-02-26 23:35:13.014897"}',
  'q','{"varc": "q"}'),
(14,'{"int1": "2"}',6,
  '2006-05-14','{"date": "2006-05-14"}',
  '01:00:33','{"time": "01:00:33.038177"}',
  '2001-06-23 13:47:43','{"datetime": "2001-06-23 13:47:43.001775"}',
  'n','{"varc": "n"}'),
(15,'{"int1": "4"}',1,
  '2005-01-19','{"date": "2005-01-19"}',
  '03:06:43','{"time": "03:06:43.059217"}',
  '2007-01-17 05:06:55','{"datetime": "2007-01-17 05:06:55.064405"}',
  'r','{"varc": "r"}'),
(16,'{"int1": "231"}',156,
  '1900-01-01','{"date": "1900-01-01"}',
  '00:00:00','{"time": null}',
  '2001-10-23 00:00:00','{"datetime": "2001-10-23 00:00:00"}',
  'c','{"varc": "c"}'),
(17,'{"int1": "4"}',8,
  '2004-06-25','{"date": "2004-06-25"}',
  '16:13:44','{"time": "16:13:44.008978"}',
  '2003-11-24 04:13:27','{"datetime": "2003-11-24 04:13:27.046820"}',
  'h','{"varc": "h"}'),
(18,'{"int1": "3"}',7,
  '2009-12-22','{"date": "2009-12-22"}',
  '14:09:13','{"time": "14:09:13.028533"}',
  '2007-09-19 14:33:09','{"datetime": "2007-09-19 14:33:09.017132"}',
  'k','{"varc": "k"}'),
(19,'{"int1": "3"}',2,
  '2004-06-25','{"date": "2004-06-25"}',
  '10:09:51','{"time": "10:09:51.017335"}',
  '1900-01-01 00:00:00','{"datetime": "1900-01-01 00:00:00"}',
  't','{"varc": "t"}');

CREATE TABLE insert_select_21104470
SELECT
table3 . col_datetime_nokey AS field1
FROM
  D_21104470 AS table1
  LEFT  JOIN  DD_21104470 AS table2
  LEFT  JOIN DD_21104470 AS table3
  ON table2 . col_int_key <  table3 . col_int_key
  ON  table1 . pk =  table2 . col_int_key
WHERE   table1 . col_int_key >= 3
AND table1 . col_int_key < ( 3 + 7 )
OR table1 . col_int_key >= 3
AND table1 . col_int_key < ( 3 + 25 )
AND table1 . col_int_key != table2 . pk
AND table1 . col_int_key IS  NULL
AND  table1 . pk IS  NULL
OR table1 . col_int_key > 3
GROUP BY field1
ORDER BY field1;

DROP TABLE D_21104470;
DROP TABLE DD_21104470;
DROP TABLE insert_select_21104470;
DROP TABLE t_21104470;

CREATE TABLE t(j JSON NOT NULL);
# There is no default for the field, and it can't be NULL, so this should fail.
--error ER_NO_DEFAULT_FOR_FIELD
INSERT INTO t(j) VALUES (DEFAULT);
# Loosen up the checks. It works in non-strict mode.
SET sql_mode = '';
INSERT INTO t(j) VALUES (DEFAULT);
# This query failed with 'invalid data' before bug#21104470.
SELECT * FROM t;
SET sql_mode = default;
DROP TABLE t;

--echo #
--echo # Bug#21072360 ASSERTION `(*A)->FIELD_TYPE() == MYSQL_TYPE_JSON ||
--echo #              (*B)->FIELD_TYPE() == FAILED
--echo #

CREATE TABLE t (j JSON);
INSERT INTO t VALUES ('true'), ('"abc"');
SELECT j FROM t WHERE j <= 'xyz' AND j = 'abc';
DROP TABLE t;

--echo #
--echo # Bug#21094905 VALGRIND ERRORS WITH LATEST BUILDS OF WL7909
--echo #

CREATE TABLE F (i1 INT, j JSON);
INSERT INTO F VALUES (1, '1'), (2, '2');
CREATE TABLE H (i2 INT);
SELECT SUM(DISTINCT i2), j FROM F LEFT JOIN H ON i1 = i2 GROUP BY j ORDER BY j;
DROP TABLE F, H;

--echo #
--echo # Bug#21110783 WL8132:DEBUG CRASH AT WRAPPER_TO_STRING | SQL/JSON_DOM.CC
--echo #

CREATE TABLE t1 (i1 INT) ENGINE=MyISAM;
INSERT INTO t1 VALUES (1);
CREATE TABLE t2 (i2 INT, j JSON) ENGINE=MyISAM;
CREATE TABLE t3(v VARCHAR(100), j JSON) ENGINE=MyISAM;
INSERT INTO t3(v) SELECT j FROM t1 LEFT JOIN t2 ON i1 = i2 GROUP BY j;
INSERT INTO t3(j) SELECT j FROM t1 LEFT JOIN t2 ON i1 = i2 GROUP BY j;
SELECT * FROM t3;
DROP TABLE t1, t2, t3;

--echo #
--echo # Bug#21128632 JSON_QUOTE(JSON_TYPE(...)) GIVES ERROR 3139 ER_INVALID_JSON_CHARSET
--echo #

select json_quote( json_type( json_object() ) );
select json_quote( json_type( cast('{}' as json) ) );

--echo #
--echo # Bug#21096340 VIEW WITH QUERY HAVING SUM ON JSON COLUMN RETURNS INVALID DATA
--echo #

--source include/turn_off_only_full_group_by.inc

CREATE TABLE D (
col_varchar_10_latin1 varchar(10) CHARACTER SET latin1,
col_varchar_255_latin1_key varchar(255) CHARACTER SET latin1,
col_int_key int,
pk integer auto_increment,
col_varchar_255_utf8_key varchar(255) CHARACTER SET utf8,
col_int int,
col_varchar_10_utf8 varchar(10) CHARACTER SET utf8,
col_varchar_255_latin1 varchar(255) CHARACTER SET latin1,
col_varchar_255_utf8 varchar(255) CHARACTER SET utf8,
col_varchar_10_latin1_key varchar(10) CHARACTER SET latin1,
col_varchar_10_utf8_key varchar(10) CHARACTER SET utf8,
/*Indices*/
key (col_varchar_255_latin1_key),
key (col_int_key),
primary key (pk),
key (col_varchar_255_utf8_key),
key (col_varchar_10_latin1_key),
key (col_varchar_10_utf8_key)
) ENGINE=innodb;

INSERT INTO D VALUES  ('have', 'say', NULL, NULL, 'who',
NULL, 'JBIYP', 'he''s', 'VWJOC', 'your',
'VDMAE') ,  ('YQAHN', 'GIKYQ', 3, NULL, 'no', 1933049856,
'got', 'icz', 'RMHUR', 'MPCRS', 'so') ,
('had', 'PASVH', 2144141312, NULL, 'look',
808058880, 'czstj', 'z', 'MRRVF', 'UHWEV',
'that') ,  ('BSXXI', 'DLIRG', NULL, NULL,
'zstjzfsu', 1562116096, 'is', 'WKDGV', 'stj',
'tjzf', 'with') ,  ('JFSGY', 'c', 98828288,
NULL, 'n', 269418496, 'LDJWY', 'jzfsu', 'QPXVX',
'zfsunwdg', 'we');

ALTER TABLE D MODIFY COLUMN col_int JSON;

SELECT SUM(col_int), col_int FROM D AS table1 WHERE table1.pk = table1.pk;

CREATE OR REPLACE VIEW view1 AS SELECT SUM(col_int), col_int FROM D AS table1
WHERE table1.pk = table1.pk;
SELECT * FROM view1;

SELECT  SUM(col_int), col_int FROM D AS table1 WHERE table1.pk = table1.pk;

SELECT * FROM view1;

DROP VIEW view1;
DROP TABLE D;

--source include/restore_sql_mode_after_turn_off_only_full_group_by.inc

--echo #
--echo # Bug#21119971 WL8132:DEBUG CRASH AT ITEM_CACHE_JSON::CACHE_VALUE
--echo #
CREATE TABLE t1(j1 JSON);
CREATE TABLE t2(j2 JSON);
CREATE TABLE t3(j3 JSON);
INSERT INTO t1 VALUES ('1');
INSERT INTO t2 VALUES ('1');
INSERT INTO t3 VALUES ('1');
SELECT * FROM t1 WHERE j1 >= ALL (SELECT j3 FROM t2 LEFT JOIN t3 ON (j2 > j3));
DROP TABLE t1, t2, t3;

--echo #
--echo # Bug#21145759 ER_INVALID_CAST_TO_JSON ON CALL TO JSON_REMOVE
--echo #              WITH EMPTY ARG + ORDER BY
--echo #
CREATE TABLE t (pk INT PRIMARY KEY, col_json JSON);
INSERT INTO t VALUES (1, JSON_OBJECT());
SELECT JSON_REMOVE((SELECT col_json FROM t WHERE pk = 3),
                  '$.testpath1') AS field1
FROM t HAVING field1 >= 'c' ORDER BY field1;
SELECT JSON_REMOVE((SELECT col_json FROM t WHERE pk = 3),
                  '$.testpath1') AS field1
FROM t HAVING field1 >= 'c';
SELECT JSON_REMOVE((SELECT col_json FROM t WHERE pk = 3),
                  '$.testpath1') AS field1
FROM t ORDER BY field1;
SELECT JSON_REMOVE((SELECT col_json FROM t WHERE pk = 3),
                  '$.testpath1') AS field1
FROM t;
DROP TABLE t;

--echo #
--echo # Bug#21148020 OUTPUT FROM JSON_TYPE() IS TRUNCATED
--echo #              WHEN EXECUTED IN A VIEW OR JOIN
--echo #

SELECT JSON_TYPE(JSON_OBJECT());
CREATE VIEW v1 AS SELECT JSON_TYPE(JSON_OBJECT());
SELECT * FROM v1;

SELECT JSON_TYPE(CAST(CAST('2015-05-25 11:23:55' AS DATETIME) AS JSON));
CREATE VIEW v2 AS SELECT JSON_TYPE(CAST(CAST('2015-05-25 11:23:55' AS
DATETIME) AS JSON));
SELECT * FROM v2;

drop view v1;
drop view v2;

--echo #
--echo # Bug#21135312 BUG IN JSON COMPARE OPARATOR WHEN USING JSON_QUOTE
--echo #
CREATE TABLE t1(c VARCHAR(10) CHARACTER SET latin1);
INSERT INTO t1 VALUES ('abc'), ('"abc"');
CREATE TABLE t2(j JSON);
INSERT INTO t2 VALUES ('"abc"');
SELECT c,
       c = CAST('"abc"' AS JSON) AS eq1,
       c = JSON_EXTRACT('"abc"', '$') AS eq2,
       c = j AS eq3
FROM t1, t2 ORDER BY c;
DROP TABLE t1, t2;

--echo #
--echo # Bug#21147943 JSON_CONTAINS() RETURNS INCORRECT RESULT
--echo #
SELECT JSON_CONTAINS('[1, 2, [4] ]', '{ "b" : 2 }');
SELECT JSON_CONTAINS('[1, 2, [4,5] ]', '[1,2,3,4,5,6,7,8,9]');
SELECT JSON_CONTAINS('[1, 2, [4,5] ]', '[111111111111111111]');

--echo #
--echo # Bug#21169109 WL8249:WRONG RESULTS WHILE COMPARING A JSON COLUMN
--echo #              WITH AN INDEXED INT COLUMN
--echo #

CREATE TABLE t1(j JSON, i INT);
INSERT INTO t1 VALUES
('0', 0), ('1', 1), ('1.0', 2), ('[1,2,3]', 3), ('{}', 4), ('"abc"', 5);
ANALYZE TABLE t1;
let $query1=SELECT * FROM t1 WHERE j > i ORDER BY i;
let $query2=SELECT * FROM t1 AS a, t1 AS b WHERE a.j > b.i ORDER BY b.i, a.i;
eval EXPLAIN $query1;
eval EXPLAIN $query2;
eval $query1;
eval $query2;
--echo # Create an index on the INT column and verify that it is not used.
CREATE INDEX t1_idx ON t1(i);
ANALYZE TABLE t1;
eval EXPLAIN $query1;
eval EXPLAIN $query2;
eval $query1;
eval $query2;
CREATE TABLE t2(i INT, vc VARCHAR(10))
ENGINE=InnoDB CHARACTER SET 'utf8mb4' COLLATE 'utf8mb4_bin';
INSERT INTO t2 VALUES (1, 'abc'), (2, '"abc"');
ANALYZE TABLE t2;
let $query= SELECT i FROM t2 WHERE vc = CAST('"abc"' AS JSON);
eval EXPLAIN $query;
eval $query;
--echo # Create an index on the VARCHAR column and verify that it is not used.
--echo # Used to return 2 instead of 1.
CREATE INDEX t2_idx ON t2(vc);
ANALYZE TABLE t2;
eval EXPLAIN $query;
eval $query;
DROP INDEX t2_idx ON t2;
--echo # Create a unique index on the VARCHAR column and verify that it is not
--echo # used. Used to return an empty result.
CREATE UNIQUE INDEX t2_idx ON t2(vc);
ANALYZE TABLE t2;
eval EXPLAIN $query;
eval $query;
DROP TABLE t1, t2;

--echo #
--echo # Bug#21144949 WL8132:DELETE QUERY HAVING A SUB-QUERY DOES NOT
--echo #              DELETE ROWS IN THE TABLE
--echo #
CREATE TABLE t(j JSON);
INSERT INTO t VALUES (JSON_OBJECT('a', 'b')), (JSON_OBJECT('a', 'b'));
DELETE FROM t WHERE j IN (SELECT JSON_OBJECT('a', 'b') FROM DUAL WHERE 1);
SELECT * FROM t;
INSERT INTO t VALUES (JSON_OBJECT('a', 'b')), (JSON_OBJECT('a', 'b'));
DELETE FROM t WHERE j IN (SELECT JSON_OBJECT('a', 'b') FROM DUAL);
SELECT * FROM t;
INSERT INTO t VALUES (JSON_OBJECT('a', 'b')), (JSON_OBJECT('a', 'b'));
DELETE FROM t WHERE j IN (SELECT CAST(NULL AS JSON) FROM DUAL);
DELETE FROM t WHERE j IN (SELECT CAST(NULL AS JSON) FROM DUAL WHERE 1);
--error ER_INVALID_JSON_TEXT_IN_PARAM
DELETE FROM t WHERE j IN (SELECT CAST('not json' AS JSON) FROM DUAL);
--error ER_INVALID_JSON_TEXT_IN_PARAM
DELETE FROM t WHERE j IN (SELECT CAST('not json' AS JSON) FROM DUAL WHERE 1);
SELECT * FROM t;
DROP TABLE t;

--echo #
--echo # Bug#21198333 SIG 6 IN ITEM_CACHE_JSON::CACHE_VALUE AT SQL/ITEM.CC:9470
--echo #
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT MIN(JSON_EXTRACT('not json', '$'));

--echo #
--echo # Bug#21200657 DATA FROM DERIVED TABLE BASED
--echo # ON JSN_QUOTE()/JSN_UNQUOTE() CALL IS TRUNCATED
--echo #
SELECT JSON_QUOTE('This is a string that should not be truncated') AS field1;
SELECT JSON_UNQUOTE(JSON_QUOTE('This is a string that should not be truncated')) AS field1;

SELECT * FROM (SELECT JSON_QUOTE('This is a string that should not be truncated') AS field1) AS DERIVED_TABLE;
SELECT * FROM (SELECT JSON_UNQUOTE("This is a string that should not be truncated") AS field1) AS DERIVED_TABLE;
SELECT * FROM (SELECT JSON_UNQUOTE(JSON_QUOTE('This is a string that should not be truncated')) AS field1) AS DERIVED_TABLE;

# test that expanded escape sequences are not truncated
create table t_varchar( a varchar(3) );
insert into t_varchar values ( json_unquote( '"\\u0000\\u0001\\u0002"' ) );
select length(a) l from t_varchar;
select length( json_quote( a ) ) l, json_quote( a ) v from t_varchar;
select * from
(
 select length( json_quote( a ) ) as field0,
 json_quote( a ) as field1
 from t_varchar
) as derived_table;
drop table t_varchar;

--echo #
--echo # Bug#21193273 CREATE TABLE SELECT JSN_QUOTE() RESULTS
--echo # IN TRUNCATED DATA
--echo #

SELECT JSON_QUOTE('table') AS field1;
CREATE TABLE t SELECT JSON_QUOTE('table') AS field1;
SHOW WARNINGS;
SELECT * FROM t;
DESCRIBE t;
drop table t;

select json_unquote( '"table"' ) as field1;
create table t1 select json_unquote( '"table"' ) as field1;
SHOW WARNINGS;
SELECT * FROM t1;
DESCRIBE t1;
drop table t1;

--echo #
--echo # Bug#21230644 JSON_MERGE MEMORY USAGE
--echo #

CREATE TABLE t (doc json);
INSERT INTO t VALUES('{"array":[1,2,3,4]}');
UPDATE t SET doc=JSON_MERGE(doc,doc);
UPDATE t SET doc=JSON_MERGE(doc,doc);
drop table t;

--echo #
--echo # Bug#21224267 DEEPLY NESTED JSON OBJECTS MAY CAUSE CRASH
--echo #
CREATE TABLE t (j JSON);
--error ER_JSON_DOCUMENT_TOO_DEEP
INSERT INTO t VALUES (CONCAT(REPEAT('{"v":', 5000), '1', REPEAT('}', 5000)));
--error ER_JSON_DOCUMENT_TOO_DEEP
INSERT INTO t VALUES (CONCAT(REPEAT('{"v":[', 5000), '1', REPEAT(']}', 5000)));
--error ER_JSON_DOCUMENT_TOO_DEEP
INSERT INTO t VALUES (CONCAT(REPEAT('[{"v":', 5000), '1', REPEAT('}]', 5000)));
DROP TABLE t;

--echo #
--echo # JSON should work with INSERT .. ON DUPLICATE KEY UPDATE
--echo #
CREATE TABLE t(id INT PRIMARY KEY, j JSON);
INSERT INTO t VALUES (1, '[1]')
ON DUPLICATE KEY UPDATE j = JSON_OBJECT("a", VALUES(j));
SELECT * FROM t;
INSERT INTO t VALUES (1, '[1,2]')
ON DUPLICATE KEY UPDATE j = JSON_OBJECT("ab", VALUES(j));
SELECT * FROM t;
INSERT INTO t VALUES (1, '[1,2,3]')
ON DUPLICATE KEY UPDATE j = JSON_OBJECT("abc", VALUES(j));
SELECT * FROM t;
DROP TABLE t;

--echo #
--echo # Bug#21278178 JSON_QUOTE(LITERAL) GETS APPLIED DOUBLE DURING SELECT
--echo #              FROM TABLE WITH MTR
--echo #
CREATE TABLE t(x INT);
INSERT INTO t VALUES (1), (2), (3);
SET NAMES latin1;
SELECT JSON_QUOTE('abc') FROM t;
SET NAMES utf8mb4;
SELECT JSON_QUOTE('abc') FROM t;
SET NAMES default;
DROP TABLE t;

--echo #
--echo # Bug#21296173 JSON_OBJECT('KEY', BOOLEAN_LITERAL) USES VALUES 0, 1
--echo #               FOR BOOL WHEN USED VIA VIEW
--echo #

SELECT JSON_OBJECT('key1', false, 'key2', true);
SELECT JSON_ARRAY('key1', false, 'key2', true);
CREATE VIEW v1 AS SELECT JSON_OBJECT('key1', false, 'key2', true);
SELECT * FROM v1;
CREATE VIEW v2 AS SELECT JSON_ARRAY('key1', false, 'key2', true);
SELECT * FROM v2;

drop view v1;
drop view v2;

--echo #
--echo # Bug#21291993 ASSERT `0' FAILED AT THD::SEND_STATEMENT_STATUS()
--echo #              ON JSON_SEARCH(NULL, ...)+JOIN
--echo #
CREATE TABLE t (pk INT NOT NULL PRIMARY KEY, col_varchar VARCHAR(1));
SELECT COUNT(*), JSON_SEARCH(NULL, 'one', '5%')
FROM t t1, t t2 WHERE t1.pk = t2.pk;
DROP TABLE t;

--echo #
--echo # Bug#21293089 JSON_CONTAINS() RETURNS WRONG RESULT WITH EMPTY JSON ARRAY
--echo #
SELECT JSON_CONTAINS('[]', '{"a" : 1}');
SELECT JSON_CONTAINS('[]', '[1, 2, 3, 4, 5]');
SELECT JSON_CONTAINS('[]', '[1, 2, 3, 4, {"a" : 1}]');
SELECT JSON_CONTAINS('[]', '{"a" : [1, 2, 3, 4, 5]}');
SELECT JSON_CONTAINS('[]', '[]');

--echo #
--echo # Bug#21376088 JSON: CRASH IN VAL_JSON_FUNC_FIELD_SUBSELECT
--echo #
create table t(a json);
insert into t values('{}');
select a from t where (select a from t where 1) in (select 1 from t); # repro
select a from t where (select a from t where 1) in (select cast('{}' as json) from t);
drop table t;

--echo #
--echo # Bug#21054516:QUERY HAVING SQL_BIG_RESULT ON JSON DATA GIVES EXTRA
--echo #              ROWS IN OUTPUT
--echo #
CREATE TABLE t1 (
pk integer auto_increment key,
col_varchar_255_utf8_key varchar(255)  CHARACTER SET utf8
) ENGINE=innodb;

INSERT INTO t1 VALUES (NULL, 'q') , (NULL, 'tgzvsj') ,
(NULL, 'b') , (NULL, 'q') , (NULL, 'up') , (NULL, 'up') ;

ALTER TABLE t1 ADD COLUMN json_varchar255_utf8_key json;

UPDATE t1 SET json_varchar255_utf8_key =
  JSON_OBJECT('col_varchar_255_utf8_key', col_varchar_255_utf8_key);

ALTER TABLE t1 MODIFY col_varchar_255_utf8_key VARCHAR(255)
  GENERATED ALWAYS AS
    (JSON_EXTRACT(json_varchar255_utf8_key,'$.col_varchar_255_utf8_key[0]'))
  STORED;

SELECT SQL_BIG_RESULT table1.json_varchar255_utf8_key AS field1, count(*)
  FROM t1 AS table1 LEFT JOIN t1 AS table2 ON table1.pk <= table2.pk
  GROUP  BY field1;
DROP TABLE t1;

--echo #
--echo # Bug#21377136 STACK OVERFLOW IN RAPIDJSON::GENERICREADER
--echo #
--error ER_JSON_DOCUMENT_TOO_DEEP
SELECT JSON_VALID(REPEAT('[', 100000));
--error ER_JSON_DOCUMENT_TOO_DEEP
SELECT JSON_VALID(REPEAT('{"a":', 100000));
--error ER_JSON_DOCUMENT_TOO_DEEP
SELECT JSON_VALID(REPEAT('{"a":[', 100000));
--error ER_JSON_DOCUMENT_TOO_DEEP
SELECT JSON_VALID(REPEAT('[{"a":', 100000));

--echo #
--echo # Bug#21381806 JSON: ASSERTION FAILED: ARG->NULL_VALUE
--echo #
SELECT JSON_SET(CASE WHEN 1 THEN NULL ELSE NULL END, '{}', '{}');
SELECT JSON_VALID(CASE WHEN 1 THEN NULL ELSE NULL END);
SELECT JSON_ARRAY(CASE WHEN 1 THEN NULL ELSE NULL END);

--echo #
--echo # Bug#21383497 DBUG_ABORT() IN VAL_JSON
--echo #
CREATE TABLE t(a INT PRIMARY KEY);
INSERT INTO t VALUES (1), (2), (3), (4), (5);
SELECT 1 FROM t GROUP BY ST_ASGEOJSON(POINT(1, 1)) WITH ROLLUP;
SELECT JSON_EXTRACT('{"a":1}', '$.a') AS je, COUNT(DISTINCT a)
FROM t GROUP BY je WITH ROLLUP;
SELECT JSON_EXTRACT(NULL, '$.a') AS je, COUNT(DISTINCT a)
FROM t GROUP BY je WITH ROLLUP;
SELECT JSON_EXTRACT('{"a":1}', '$.a') AS je, a, COUNT(DISTINCT a)
FROM t GROUP BY je, a WITH ROLLUP;
SELECT JSON_EXTRACT('{"a":1}', '$.a') AS je, a, COUNT(DISTINCT a)
FROM t GROUP BY a, je WITH ROLLUP;
DROP TABLE t;

--echo #
--echo # Bug#21384048 ASSERTION FAILED: N >= 0 && N <= 308
--echo #              IN RAPIDJSON::INTERNAL::FASTPATH
--echo #
SELECT JSON_EXTRACT('-1E-36181012216111515851075235238', '$');
SELECT JSON_EXTRACT('1E-36181012216111515851075235238', '$');
SELECT JSON_EXTRACT('1E-325', '$');
SELECT JSON_EXTRACT('1E-324', '$');
SELECT JSON_EXTRACT('1E-323', '$');
SELECT JSON_EXTRACT('1E+308', '$');
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_EXTRACT('1E+309', '$');
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_EXTRACT('1E+36181012216111515851075235238', '$');
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT JSON_EXTRACT('-1E+36181012216111515851075235238', '$');

--echo #
--echo # Bug#21383284: ASSERTION IN SELECT_LEX::SETUP_CONDS
--echo #
--error ER_WRONG_ARGUMENTS
SELECT 1 FROM dual WHERE JSON_SEARCH('{}', 'one', 'foo', 'too-long-escape');
--error ER_INVALID_JSON_TEXT_IN_PARAM
SELECT 1 FROM dual WHERE JSON_SEARCH('{}', 'one', 'foo', JSON_EXTRACT('', '$'));

--echo #
--echo # Bug#21437989: ASSERTION FAILED:
--echo #               JSON_BINARY::PARSE_BINARY(PTR, LENGTH).IS_VALID()
--echo #
CREATE TABLE t(j JSON NOT NULL);
--error ER_NO_DEFAULT_FOR_FIELD
INSERT INTO t VALUES ();
--error ER_BAD_NULL_ERROR
INSERT INTO t VALUES (NULL);
INSERT IGNORE INTO t VALUES ();
INSERT IGNORE INTO t VALUES (NULL);
SELECT * FROM t;
# The next two statements used to trigger an assertion.
INSERT INTO t SELECT j FROM t;
REPLACE INTO t SELECT j FROM t;
SELECT * FROM t;
DROP TABLE t;

--echo #
--echo # Bug#21448719: WRONG RESULT FOR JSON VALUE IN OUTER JOIN WITH VIEW
--echo #
CREATE TABLE t1(j1 JSON);
CREATE TABLE t2(j2 JSON);
CREATE VIEW v AS SELECT CAST('1' AS JSON) AS jv, j2 FROM t2;
INSERT INTO t1 VALUES ('1');
SELECT j1, jv, j2, JSON_ARRAY(j1, jv, j2) FROM t1 LEFT JOIN v ON j1 = jv;
INSERT INTO t2 VALUES ('1');
SELECT j1, jv, j2, JSON_ARRAY(j1, jv, j2) FROM t1 LEFT JOIN v ON j1 = jv;
DROP TABLE t1, t2;
DROP VIEW v;


--echo #
--echo # Bug#21442624 INCORRECT RESULT FROM JSON_SET WITH AUTO-WRAPPING
--echo #

SELECT JSON_SET('1', '$', 100);
SELECT JSON_SET('1', '$[0]', 100);
SELECT JSON_SET('1', '$[0][0]', 100);
SELECT JSON_SET('1', '$[0][0][0]', 100);

SELECT JSON_SET('[]', '$', 100);
SELECT JSON_SET('[]', '$[0]', 100);
SELECT JSON_SET('[]', '$[0][0]', 100);
SELECT JSON_SET('[]', '$[0][0][0]', 100);

SELECT JSON_SET('[1]', '$', 100);
SELECT JSON_SET('[1]', '$[0]', 100);
SELECT JSON_SET('[1]', '$[0][0]', 100);
SELECT JSON_SET('[1]', '$[0][0][0]', 100);

SELECT JSON_SET('[[1]]', '$', 100);
SELECT JSON_SET('[[1]]', '$[0]', 100);
SELECT JSON_SET('[[1]]', '$[0][0]', 100);
SELECT JSON_SET('[[1]]', '$[0][0][0]', 100);

SELECT JSON_SET('[[[1]]]', '$', 100);
SELECT JSON_SET('[[[1]]]', '$[0]', 100);
SELECT JSON_SET('[[[1]]]', '$[0][0]', 100);
SELECT JSON_SET('[[[1]]]', '$[0][0][0]', 100);

SELECT JSON_REPLACE('1', '$', 100);
SELECT JSON_REPLACE('1', '$[0]', 100);
SELECT JSON_REPLACE('1', '$[0][0]', 100);
SELECT JSON_REPLACE('1', '$[0][0][0]', 100);

SELECT JSON_REPLACE('[]', '$', 100);
SELECT JSON_REPLACE('[]', '$[0]', 100);
SELECT JSON_REPLACE('[]', '$[0][0]', 100);
SELECT JSON_REPLACE('[]', '$[0][0][0]', 100);

SELECT JSON_REPLACE('[1]', '$', 100);
SELECT JSON_REPLACE('[1]', '$[0]', 100);
SELECT JSON_REPLACE('[1]', '$[0][0]', 100);
SELECT JSON_REPLACE('[1]', '$[0][0][0]', 100);

SELECT JSON_REPLACE('[[1]]', '$', 100);
SELECT JSON_REPLACE('[[1]]', '$[0]', 100);
SELECT JSON_REPLACE('[[1]]', '$[0][0]', 100);
SELECT JSON_REPLACE('[[1]]', '$[0][0][0]', 100);

SELECT JSON_REPLACE('[[[1]]]', '$', 100);
SELECT JSON_REPLACE('[[[1]]]', '$[0]', 100);
SELECT JSON_REPLACE('[[[1]]]', '$[0][0]', 100);
SELECT JSON_REPLACE('[[[1]]]', '$[0][0][0]', 100);


--echo #
--echo # Bug#21472872 WRONG RESULTS CAUSED BY PATH LEG POPPING
--echo # IN JSON FUNCTIONS
--echo #

create table tdoc( id int, doc json );
insert into tdoc values
( 1, '[]' ),
( 2, '{ "a": { "b": true } }' );

select id, json_insert( doc, '$.a.c', false ) from tdoc where id = 2;
select id, json_insert( doc, '$.a.c', false ) from tdoc order by id;

drop table tdoc;
create table tdoc( id int, doc json, new_value varchar( 10 ) );
insert into tdoc values
( 1, '{ "a": { "b": true } }', null ),
( 2, '{ "a": { "b": true } }', 'abc' );

select id, json_insert( doc, '$.a.c', new_value ) from tdoc where id = 2;
select id, json_insert( doc, '$.a.c', new_value ) from tdoc order by id;

drop table tdoc;

--echo #
--echo # Bug#21491442 VARIANT::FORCED_RETURN() [WITH T = JSON_SCALAR*]:
--echo #              ASSERTION `FALSE' FAILED.
--echo #
create table t (a json, b blob,
c int generated always as (1!=a) virtual not null) engine=innodb;
insert into t(a) values('[1]');
insert into t(a) values('[1]');
select a,c from t;
prepare ps1 from 'insert into t(a) values(?)';
set @a='[1]';
execute ps1 using @a;
execute ps1 using @a;
select a,c from t;
drop table t;

create temporary table t (a json, b blob,
c int generated always as (1!=a) virtual not null) engine=innodb;
insert into t(a) values('[1]');
insert into t(a) values('[1]');
select a,c from t;
prepare ps1 from 'insert into t(a) values(?)';
set @a='[1]';
execute ps1 using @a;
execute ps1 using @a;
select a,c from t;

drop table t;

--echo #
--echo # Bug#21487833: DBUG_ABORT() IN JSON_WRAPPER::MAKE_HASH_KEY
--echo #               WITH ORDERED JSON
--echo #
--source include/turn_off_strict_mode.inc
CREATE TABLE t (a BLOB, b JSON NOT NULL) ENGINE=InnoDB;
INSERT INTO t VALUES ('', NULL), ('', NULL);
UPDATE t SET a = 1 ORDER BY b;
SELECT COUNT(a) FROM t GROUP BY b;
SELECT DISTINCT B FROM t;
SELECT b FROM t UNION DISTINCT SELECT b FROM t;
SELECT * FROM t ORDER BY b;
DROP TABLE t;
--source include/restore_strict_mode.inc

--echo #
--echo # Bug#21541481: MEMORY LEAK OF ALLOCATIONS MADE IN
--echo #               VAL_JSON_FUNC_FIELD_SUBSELECT
--echo #
--echo # This comparison gave valgrind leakage before the fix
create table t(a json not null) engine=innodb;
insert into t values('{}');
select row(uuid(), a) < row(a, str_to_date(1,1)) from t;
drop table t;
--echo # Bug#21547877: UPDATE/INSERT JSON COLUMN CRASHES IF EXPRESSION
--echo #               REFERS TO SELF
--echo #
SET NAMES latin1;
CREATE TABLE t (j JSON);
INSERT INTO t VALUES ('{}');
--error ER_INVALID_JSON_TEXT
UPDATE t SET j='1', j='1111-11-11', j=('1' NOT BETWEEN j AND '1');
SELECT * FROM t;
DROP TABLE t;
SET NAMES DEFAULT;

--echo #
--echo # Bug#21602361: ASSERTION FAILED: (VECTOR_IDX == -1) || (VECTOR_IDX >= 0)
--echo #
create table t(b int, key(b))engine=innodb;
insert into t values(1),(2);

# more than one row
--error ER_TOO_MANY_ROWS
select json_length('{}',@uninitialized_21602361)
from t group by b into @uninitialized_21602361;

# second time around the path is invalid
set @initialized_21602361 = '$';
--error ER_INVALID_JSON_PATH
select json_length('{}',@initialized_21602361)
from t group by b into @initialized_21602361;

# invalid path
set @error_value_21602361 = '$[';
--error ER_INVALID_JSON_PATH
select json_length('{}',@error_value_21602361)
from t group by b into @error_value_21602361;

# more than one row
set @valid_path_21602361 = '$';
--error ER_TOO_MANY_ROWS
select concat( '$[', json_length('{}',@valid_path_21602361), ']' )
from t group by b into @valid_path_21602361;

# used to trip the assertion
set @null_value_21602361 = null;
--error ER_TOO_MANY_ROWS
select json_length('{}',@null_value_21602361)
from t group by b into @null_value_21602361;

drop table t;

--echo #
--echo # Bug#21649073: JSON_TYPE RETURNS OPAQUE FOR SOME BINARY VALUES
--echo #
SELECT JSON_TYPE(CAST(CAST('abcd' AS BINARY) AS JSON));
CREATE TABLE t (bn BINARY(5), vb VARBINARY(5),
                tb TINYBLOB, mb MEDIUMBLOB, bb BLOB, lb LONGBLOB);
INSERT INTO t (bn) VALUES (x'cafe');
UPDATE t SET vb = bn, tb = bn, mb = bn, bb = bn, lb = bn;
SELECT JSON_TYPE(CAST(bn AS JSON)) AS bn, JSON_TYPE(CAST(vb AS JSON)) AS vb,
       JSON_TYPE(CAST(tb AS JSON)) AS tb, JSON_TYPE(CAST(mb AS JSON)) AS mb,
       JSON_TYPE(CAST(bb AS JSON)) AS bb, JSON_TYPE(CAST(lb AS JSON)) AS lb
FROM t;
DROP TABLE t;

--echo #
--echo # Basic tests for inlined JSON path
--echo #
CREATE TABLE t1(f1 JSON);
INSERT INTO t1 VALUES('{"a":1}'),('{"a":3}'),('{"a":2}');
INSERT INTO t1 VALUES('{"a":11, "b":3}'),('{"a":33, "b":1}'),('{"a":22,"b":2}');
ANALYZE TABLE t1;

SELECT f1->"$.a" FROM t1;
EXPLAIN SELECT f1->"$.a" FROM t1;

SELECT f1->"$.a" FROM t1 WHERE f1->"$.b" > 1;
EXPLAIN SELECT f1->"$.a" FROM t1 WHERE f1->"$.b" > 1;

SELECT f1->"$.a", f1->"$.b" FROM t1 ORDER BY f1->"$.b";
EXPLAIN SELECT f1->"$.a", f1->"$.b" FROM t1 ORDER BY f1->"$.b";

SELECT MAX(f1->"$.a"), f1->"$.b" FROM t1 GROUP BY f1->"$.b";
EXPLAIN SELECT MAX(f1->"$.a"), f1->"$.b" FROM t1 GROUP BY f1->"$.b";

SELECT JSON_OBJECT("c",f1->"$.b") AS f2
  FROM t1 HAVING JSON_TYPE(f2->"$.c") <> 'NULL';
EXPLAIN
  SELECT JSON_OBJECT("c",f1->"$.b") AS f2 FROM t1 HAVING JSON_TYPE(f2->"$.c") <> 'NULL';

DROP TABLE t1;

--echo #
--echo # Bug#21828321: JSON FUNCS CALL DBUG_ABORT OR EXIT() ON WINDOWS!
--echo #
# LEAST and GREATEST treat JSON arguments as strings for now. They used to hit
# an assertion if used in a JSON context and all arguments were JSON values, or
# a mix of NULLs and JSON values.
SELECT JSON_ARRAY(LEAST(NULL, NULL), GREATEST(NULL, NULL), LEAST(j1, NULL),
                  GREATEST(NULL, j2), LEAST(j1, j2), GREATEST(j1, j2)) AS j
FROM (SELECT CAST('1' AS JSON) AS j1, CAST('2' AS JSON) AS j2) t;

--echo #
--echo #  Bug#22278524: ALTER TABLE SOMETIMES CONVERTS TEXT TO JSON WITHOUT
--echo #                SYNTAX CHECKING
--echo #
CREATE TABLE t1(txt TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_bin);
INSERT INTO t1 VALUES ('not JSON');
--replace_regex /#sql-[0-9a-f_]*/#sql-temporary/
--error ER_INVALID_JSON_TEXT
ALTER TABLE t1 MODIFY COLUMN txt JSON;
SELECT * FROM t1;
CREATE TABLE t2(j JSON);
INSERT INTO t2 VALUES (JSON_OBJECT('a', 'b'));
ALTER TABLE t2 MODIFY COLUMN j TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_bin;
SELECT * FROM t2;
CREATE TABLE t3 (j JSON);
INSERT INTO t3 VALUES (JSON_OBJECT('a', 'b'));
CREATE TABLE t4 AS SELECT UPPER(j) AS jj FROM t3;
INSERT INTO t4 VALUES ('not JSON');
--replace_regex /#sql-[0-9a-f_]*/#sql-temporary/
--error ER_INVALID_JSON_TEXT
ALTER TABLE t4 MODIFY COLUMN jj JSON;
SELECT * FROM t4;
DROP TABLE t1, t2, t3, t4;


# Local Variables:
# mode: sql
# sql-product: mysql
# comment-column: 48
# comment-start: "# "
# fill-column: 80
# End:
