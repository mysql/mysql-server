include/master-slave.inc [rpl_server_count=3]
Warnings:
Note	####	Sending passwords in plain text without SSL/TLS is extremely insecure.
Note	####	Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the 'START SLAVE Syntax' in the MySQL Manual for more information.
[connection master]
SET @debug_save= @@GLOBAL.DEBUG;
#
# 1) Setup a new single node cluster
#
server1
# Set the debug flag to block recovery
SET @@GLOBAL.DEBUG='d,recovery_thread_wait';
# Node should be offline before start
include/assert.inc [On the new stopped node, the status is OFFLINE]
include/start_group_replication.inc
# Node should be online as the node is alone and doesn't need recovery
include/rpl_group_replication_wait_for_member_state.inc
#
# 2) A new node enters the existing cluster
#
server2
# Set the debug flag to block recovery
SET @@GLOBAL.DEBUG='d,recovery_thread_wait';
SET GLOBAL group_replication_group_name= "89ab83b0-9f17-11e3-a5e2-0800200c9a66";
START GROUP_REPLICATION;
# Node should be marked as on recovery on both nodes
include/rpl_group_replication_wait_for_member_state.inc
server1
include/rpl_group_replication_wait_for_member_state.inc
server2
SET DEBUG_SYNC= "now SIGNAL signal.recovery_continue";
# Node should be marked as online on both nodes
include/rpl_group_replication_wait_for_member_state.inc
server1
include/rpl_group_replication_wait_for_member_state.inc
#
# 3) A node leaves the existing cluster
#
# Add a new node to hold quorum
server3
include/start_group_replication.inc
# Node should be marked as online
include/rpl_group_replication_wait_for_member_state.inc
include/stop_group_replication.inc
# Node should be marked as offline after stop
include/rpl_group_replication_wait_for_member_state.inc
#
# Cleaning up
#
server3
SET @@GLOBAL.DEBUG= @debug_save;
server2
SET @@GLOBAL.DEBUG= @debug_save;
server1
SET @@GLOBAL.DEBUG= @debug_save;
include/rpl_end.inc
