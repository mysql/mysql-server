include/master-slave.inc [rpl_server_count=3]
Warnings:
Note	####	Sending passwords in plain text without SSL/TLS is extremely insecure.
Note	####	Storing MySQL user name or password information in the master info repository is not secure and is therefore not recommended. Please consider using the USER and PASSWORD connection options for START SLAVE; see the 'START SLAVE Syntax' in the MySQL Manual for more information.
[connection master]
#
# Setup a new cluster
#
server1
include/start_group_replication.inc
# Add some data for recovery
CREATE TABLE t1 (c1 INT NOT NULL PRIMARY KEY) ENGINE=InnoDB;
BEGIN;
INSERT INTO t1 VALUES (1);
INSERT INTO t1 VALUES (2);
COMMIT;
INSERT INTO t1 VALUES (3);
#
# 1) Create a 3 node cluster and verify that it is functional
#
#Add 2 more nodes
server2
include/start_group_replication.inc
server3
include/start_group_replication.inc
#After recovery all nodes must see 3 other nodes
include/rpl_group_replication_wait_for_number_of_members.inc
include/rpl_group_replication_wait_for_number_of_members.inc
include/rpl_group_replication_wait_for_number_of_members.inc
#After recovery all nodes must have the data present in the donor.
include/assert.inc [On all nodes, the table should exist and have 3 elements]
include/assert.inc [On all nodes, the table should exist and have 3 elements]
include/assert.inc [On all nodes, the table should exist and have 3 elements]
#
# 2) Shut down all nodes until 0. Add some data in the process.
#
#Stop the node 3
server3
include/stop_group_replication.inc
#Add some data to future recoveries and ensure that every node has it
server2
INSERT INTO t1 VALUES (4);
INSERT INTO t1 VALUES (5);
include/rpl_sync.inc
#Stop the node 2
include/stop_group_replication.inc
#Add some data to future recoveries
server1
INSERT INTO t1 VALUES (6);
INSERT INTO t1 VALUES (7);
#Stop the node 1
include/stop_group_replication.inc
#
# 3) Bring up the cluster back to life. At the end, all data must be in
#    all three nodes.
#
server1
include/start_group_replication.inc
server2
include/start_group_replication.inc
server3
include/start_group_replication.inc
#After recovery all nodes must see 3 other nodes
include/rpl_group_replication_wait_for_number_of_members.inc
include/rpl_group_replication_wait_for_number_of_members.inc
include/rpl_group_replication_wait_for_number_of_members.inc
#After recovery all nodes must have the data present in the donor.
include/assert.inc [On all nodes, the table should exist and have 7 elements]
include/assert.inc [On all nodes, the table should exist and have 7 elements]
include/assert.inc [On all nodes, the table should exist and have 7 elements]
#
# Cleaning up
#
DROP TABLE t1;
include/rpl_end.inc
